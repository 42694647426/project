{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/hanyings/conda_envs/tnmt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-05 13:36:39.791748: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-05 13:36:45.154517: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
      "2023-03-05 13:36:45.155208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
      "2023-03-05 13:36:45.155239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "#words = unmasker(\"Hello I'm a [MASK] boy.\")\n",
    "from nltk.corpus import wordnet as wn\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def find_tokenidx(token, sentence_lst):\n",
    "  for i in range(len(sentence_lst)):\n",
    "    if token in sentence_lst[i]:\n",
    "      return i\n",
    "def mutant_pairs(examples):\n",
    "  pairs=[]\n",
    "  inputs = [ex[\"translation\"][source_lang] for ex in examples]\n",
    "  for i, input in enumerate(inputs):\n",
    "    input_list = input.split()\n",
    "    tokens = examples[i]['top_tokens']\n",
    "    # sys = wn.synsets(input_list[tokens])\n",
    "    # idx = 0\n",
    "    # while 1:\n",
    "    #   if len(wn.synset(sys[idx].name()).lemmas()) >= 2:\n",
    "    #     break\n",
    "    #   idx += 1\n",
    "    # candidate = [str(lemma.name()) for lemma in wn.synset(sys[idx].name()).lemmas()]\n",
    "    print(tokens)\n",
    "    token_idx = find_tokenidx(tokens[0], input_list)\n",
    "    print(input_list[token_idx])\n",
    "    input_list[token_idx] = \"[MASK]\"\n",
    "    candidate = unmasker(\" \".join(input_list))\n",
    "    #print(candidate)\n",
    "    mutants = []\n",
    "    for c in candidate[:3]:\n",
    "      mutant = input_list.copy()\n",
    "      mutant[token_idx] = c[\"token_str\"]\n",
    "      mutants.append(\" \".join(mutant))\n",
    "    pairs.append({'en': input, 'mutants': mutants})\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mutant_pairs(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "mutant_pairs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cozy', 'cosy', 'snug']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sys = wn.synsets(\"comfortable\", pos = 'a')\n",
    "\n",
    "#[str(lemma.name()) for lemma in wn.synset(sys[0].name()).lemmas()]\n",
    "sys[0].similar_tos()[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('eurasia.n.01'), Synset('west.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sys = wn.synsets(\"Europe\")\n",
    "\n",
    "#[str(lemma.name()) for lemma in wn.synset(sys[0].name()).lemmas()]\n",
    "\n",
    "print(sys[0].part_holonyms())\n",
    "# sys[0].pos()\n",
    "# sys[0].wup_similarity(sys[0].hyponyms()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acyclic_tree',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'in_region_domains',\n",
       " 'in_topic_domains',\n",
       " 'in_usage_domains',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'mst',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions in synset\n",
    "[func for func in dir(wn.synsets(\"run\")[0]) if func[0] != \"_\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ('avoiding', [])\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def tag(sentence):\n",
    " words = word_tokenize(sentence)\n",
    " words = pos_tag(words)\n",
    " return words\n",
    "\n",
    "def paraphraseable(tag):\n",
    " return tag.startswith('N') or tag.startswith('V') or tag.startswith('J') or tag.startswith('R')\n",
    "\n",
    "def pos(tag):\n",
    " if tag.startswith('N'):\n",
    "  return wn.NOUN\n",
    " elif tag.startswith('V'):\n",
    "  return wn.VERB\n",
    " elif tag.startswith('J'):\n",
    "  return wn.ADJ\n",
    " elif pos.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "\n",
    "def synonyms(word, tag):\n",
    "  word_synset = wn.synsets(word, pos(tag))[0]\n",
    "  def check_same(lst):\n",
    "    return not all(x.lemma_names()[0] == word for x in lst)\n",
    "  if tag.startswith('J'):\n",
    "    lemma_lists = [ss for ss in word_synset.similar_tos()]\n",
    "\n",
    "  else:\n",
    "    if word_synset.hyponyms():\n",
    "      lemma_lists = word_synset.hyponyms()\n",
    "    elif word_synset.hypernyms():\n",
    "      lemma_lists = word_synset.hypernyms()\n",
    "    elif word_synset.part_holonyms():\n",
    "      lemma_lists = word_synset.part_holonyms()\n",
    "    else:\n",
    "      for func in dir(word_synset):\n",
    "        if func[0] != \"_\":\n",
    "          item = getattr(word_synset, func)\n",
    "          try:\n",
    "            temp_lemma_lst = item()\n",
    "          except Exception as e: \n",
    "            print(e)\n",
    "            continue\n",
    "          if isinstance(temp_lemma_lst, list) and temp_lemma_lst !=[]  and not isinstance(temp_lemma_lst[0], list)  and check_same(temp_lemma_lst):\n",
    "            lemma_lists = temp_lemma_lst\n",
    "            break\n",
    "  \n",
    "  # for lemma in lemma_lists:\n",
    "  #   if pos_tag(lemma) == tag:\n",
    "  #     print(lemma.lemma_names())\n",
    "  lemmas =[]\n",
    "  for lemma in lemma_lists:\n",
    "    sim = lemma.wup_similarity(wn.synsets(word, pos=pos(tag))[0])\n",
    "    if sim >= 0.5:\n",
    "      lemmas.append((lemma.lemma_names()[0], sim))\n",
    "    #print(lemma.lemma_names())\n",
    "  #lemmas = [(lemma.lemma_names(), lemma.wup_similarity(wn.synsets(word)[0])) for lemma in lemma_lists ]\n",
    "  return lemmas\n",
    "\n",
    "def synonymIfExists(sentence, index):\n",
    "   tags = tag(sentence)\n",
    "   word, t = tags[index]\n",
    "   if paraphraseable(t):\n",
    "    syns = synonyms(word, t)\n",
    "    if syns:\n",
    "      syns_list = [s for s in syns if s[0].lower() != word.lower()]\n",
    "      return word, syns_list\n",
    "    return word, []\n",
    "\n",
    "def paraphrase(sentence):\n",
    " return [x for x in synonymIfExists(sentence)]\n",
    "get=[]\n",
    "get=synonymIfExists(\"Europe is being cautious in the name of avoiding debt and defending the euro\", 8)\n",
    "print(\"synonyms\",get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "WordNetError",
     "evalue": "Computing the lch similarity requires Synset('fabian.s.02') and Synset('cautious.a.01') to have the same part of speech.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wn\u001b[39m.\u001b[39;49msynset(\u001b[39m'\u001b[39;49m\u001b[39mfabian.s.02\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mlch_similarity(wn\u001b[39m.\u001b[39;49msynsets(\u001b[39m'\u001b[39;49m\u001b[39mcautious\u001b[39;49m\u001b[39m'\u001b[39;49m, pos\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m wn\u001b[39m.\u001b[39msynsets(\u001b[39m'\u001b[39m\u001b[39mcautious\u001b[39m\u001b[39m'\u001b[39m, pos\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/data2/hanyings/conda_envs/tnmt/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:900\u001b[0m, in \u001b[0;36mSynset.lch_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[39mLeacock Chodorow Similarity:\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[39mReturn a score denoting how similar two word senses are, based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[39m    depth.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m!=\u001b[39m other\u001b[39m.\u001b[39m_pos:\n\u001b[0;32m--> 900\u001b[0m     \u001b[39mraise\u001b[39;00m WordNetError(\n\u001b[1;32m    901\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComputing the lch similarity requires \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to have the same part of speech.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    905\u001b[0m need_root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_root()\n\u001b[1;32m    907\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wordnet_corpus_reader\u001b[39m.\u001b[39m_max_depth:\n",
      "\u001b[0;31mWordNetError\u001b[0m: Computing the lch similarity requires Synset('fabian.s.02') and Synset('cautious.a.01') to have the same part of speech."
     ]
    }
   ],
   "source": [
    "wn.synset('fabian.s.02').lch_similarity(wn.synsets('cautious', pos=\"a\")[0])\n",
    "wn.synsets('cautious', pos=\"a\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def find_tokenidx(token, sentence_lst):\n",
    "  for i in range(len(sentence_lst)):\n",
    "    if token.lower() in sentence_lst[i].lower():\n",
    "      return i\n",
    "def token_vec_sum(text, word):\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = bert_tokenizer.tokenize(marked_text)\n",
    "  index = find_tokenidx(word, tokenized_text)\n",
    "  print(index)\n",
    "  print(tokenized_text)\n",
    "  indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                    )\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      outputs = model(tokens_tensor, segments_tensors)\n",
    "      hidden_states = outputs[2]\n",
    "      token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "      token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "      token_embeddings = token_embeddings.permute(1,0,2)\n",
    "      token_vecs_sum = []\n",
    "      for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "  return token_vecs_sum[index]\n",
    "def similarity(origin, mutant, word, mutant_word):\n",
    "  \n",
    "  origin_sum, mutant_sum =  token_vec_sum(origin, word), token_vec_sum(mutant, mutant_word)\n",
    "  return 1 - cosine(origin_sum, mutant_sum)\n",
    "      \n",
    "  \n",
    "def mutant_pairs(examples):\n",
    "  pairs=[]\n",
    "  inputs = [ex[\"en\"] for ex in examples]\n",
    "  for i, input in enumerate(inputs):\n",
    "    input_list = input.split()\n",
    "    tokens = examples[i]['top_tokens']\n",
    "    #print(tokens)\n",
    "    token_idx = find_tokenidx(tokens[0], input_list)\n",
    "    #print(input_list[token_idx])\n",
    "    word = input_list[token_idx]\n",
    "\n",
    "    input_list[token_idx] = \"[MASK]\"\n",
    "    candidate = unmasker(\" \".join(input_list))\n",
    "\n",
    "    mutants = []\n",
    "    for c in candidate[:3]:\n",
    "      mutant = input_list.copy()\n",
    "      mutant[token_idx] = c[\"token_str\"]\n",
    "      \n",
    "      mutant_sent = \" \".join(mutant)\n",
    "      # if similarity(input, mutant_sent, word, c[\"token_str\"]) >=0.9:\n",
    "      mutants.append(\" \".join(mutant))\n",
    "    pairs.append({'en': input, 'mutants': mutants})\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutant_pairs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "def sentence_sim(text1, text2):\n",
    "    origin = model.encode(text1)\n",
    "    mutant = model.encode(text2)\n",
    "    return cosine_similarity(\n",
    "    [origin],\n",
    "    [mutant])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, BertForMaskedLM\n",
    "import os\n",
    "import torch\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "def bertInit():\n",
    "    #config = Ber\n",
    "    berttokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "    bertmodel = BertForMaskedLM.from_pretrained(\"bert-large-cased\")#'/data/szy/bertlarge')\n",
    "    bertori = BertModel.from_pretrained(\"bert-large-cased\")#'/data/szy/bertlarge')\n",
    "    #berttokenizer = RobertaTokenizer.from_pretrained('bert-large-uncased')\n",
    "    #bertmodel = RoBertaForMaskedLM.from_pretrained('/data/szy/bertlarge')\n",
    "    bertmodel.eval().cuda().to(torch.device(\"cuda:0\"))\n",
    "    bertori.eval().cuda()#.to(torch.device(\"cuda:1\"))\n",
    "    \n",
    "    return bertmodel, berttokenizer, bertori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m101\u001b[39m, \u001b[39m103\u001b[39m, \u001b[39m24563\u001b[39m, \u001b[39m4102\u001b[39m, \u001b[39m1106\u001b[39m, \u001b[39m1321\u001b[39m, \u001b[39m170\u001b[39m, \u001b[39m12020\u001b[39m, \u001b[39m1107\u001b[39m, \u001b[39m4507\u001b[39m, \u001b[39m119\u001b[39m, \u001b[39m1188\u001b[39m, \u001b[39m1110\u001b[39m, \u001b[39m1103\u001b[39m, \u001b[39m1148\u001b[39m, \u001b[39m1159\u001b[39m, \u001b[39m1115\u001b[39m, \u001b[39m170\u001b[39m, \u001b[39m1244\u001b[39m, \u001b[39m2325\u001b[39m, \u001b[39m5748\u001b[39m, \u001b[39m3907\u001b[39m, \u001b[39m4102\u001b[39m, \u001b[39m1106\u001b[39m, \u001b[39m1301\u001b[39m, \u001b[39m12020\u001b[39m, \u001b[39m1107\u001b[39m, \u001b[39m4507\u001b[39m, \u001b[39m1170\u001b[39m, \u001b[39m4811\u001b[39m, \u001b[39m23300\u001b[39m, \u001b[39m117\u001b[39m, \u001b[39m2452\u001b[39m, \u001b[39m1106\u001b[39m, \u001b[39m1103\u001b[39m, \u001b[39m1244\u001b[39m, \u001b[39m2325\u001b[39m, \u001b[39m3054\u001b[39m, \u001b[39m5732\u001b[39m, \u001b[39m12627\u001b[39m, \u001b[39m119\u001b[39m, \u001b[39m102\u001b[39m]])\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> 2\u001b[0m F\u001b[39m.\u001b[39msoftmax(bertmodel(tensor)[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdata\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[101, 103, 24563, 4102, 1106, 1321, 170, 12020, 1107, 4507, 119, 1188, 1110, 1103, 1148, 1159, 1115, 170, 1244, 2325, 5748, 3907, 4102, 1106, 1301, 12020, 1107, 4507, 1170, 4811, 23300, 117, 2452, 1106, 1103, 1244, 2325, 3054, 5732, 12627, 119, 102]]).cuda()\n",
    "F.softmax(bertmodel(tensor)[0], dim=-1).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "K_Number = 100\n",
    "Max_Mutants = 5\n",
    "\n",
    "ft = time.time()\n",
    "\n",
    "lcache = []\n",
    "def BertM(bert, berttoken, inpori, bertori, mutate_idx):\n",
    "    #global lcache\n",
    "    # for k in lcache:\n",
    "    #     if inpori == k[0]:\n",
    "    #         return k[1], k[2]\n",
    "    sentence = inpori\n",
    "\n",
    "    tokens = berttoken.tokenize(sentence)\n",
    "    batchsize = 1000 // len(tokens)\n",
    "\n",
    "    gen = []\n",
    "    ltokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "    try:\n",
    "        encoding = [berttoken.convert_tokens_to_ids(ltokens[0:mutate_idx] + [\"[MASK]\"] + ltokens[mutate_idx + 1:])]#.cuda()\n",
    "    except:\n",
    "        return \" \".join(tokens), gen\n",
    "    p = []\n",
    "    #print(\"encoding :\", len(encoding))\n",
    "    for i in range(0, len(encoding)):\n",
    "        tensor = torch.tensor(encoding[i: min(len(encoding), i + batchsize)]).cuda()\n",
    "        #print(\"tensor size\", tensor.size())\n",
    "        pre = F.softmax(bert(tensor)[0], dim=-1).data.cpu() # logits of the bertmodel, (1,42,28996)\n",
    "        p.append(pre)\n",
    "\n",
    "    pre = torch.cat(p, 0) # concate on 0 dimension \n",
    "    #print(\"pre size\", pre.size()) #(40, 42, 28996), (token, layer, dimension in bert)\n",
    "\n",
    "    tarl = [[tokens, -1]]\n",
    "\n",
    "\n",
    "    # delete for loop \n",
    "    topk = torch.topk(pre[0][mutate_idx +1], K_Number)#.tolist()\n",
    "    \n",
    "    value = topk[0].numpy() #top values\n",
    "    topk = topk[1].numpy().tolist() # index of the top values\n",
    "    \n",
    "    #print (topk)\n",
    "    topkTokens = berttoken.convert_ids_to_tokens(topk)\n",
    "    # tarl = []\n",
    "    #print(topkTokens)\n",
    "    for index in range(len(topkTokens)):\n",
    "        #print(topkTokens[index], value[index])\n",
    "        if value[index] < 0.00001:\n",
    "            break\n",
    "        tt = topkTokens[index]\n",
    "        #print (tt)\n",
    "        if tt in string.punctuation:\n",
    "            continue\n",
    "        if tt.strip() == tokens[mutate_idx].strip():\n",
    "            continue\n",
    "        l = deepcopy(tokens)\n",
    "        l[mutate_idx] = tt\n",
    "        tarl.append([l, mutate_idx, value[index]])\n",
    "    #print(\"tarl:\", tarl)\n",
    "        \n",
    "    if len(tarl) == 0:\n",
    "        return \" \".join(tokens), gen\n",
    "    \n",
    "    lDB = []\n",
    " \n",
    "    for i in range(0, len(tarl), batchsize):\n",
    "        lDB.append(bertori(torch.tensor([berttoken.convert_tokens_to_ids([\"[CLS]\"] + l[0] + [\"[SEP]\"]) for l in tarl[i: min(i + batchsize, len(tarl))]]).cuda())[0].data.cpu().numpy())\n",
    "    lDB = np.concatenate(lDB, axis=0)\n",
    "            \n",
    "\n",
    "    lDA = lDB[0]\n",
    "    assert len(lDB) == len(tarl)\n",
    "    tarl = tarl[1:]\n",
    "    lDB = lDB[1:]\n",
    "    for t in range(len(lDB)):\n",
    "        DB = lDB[t][tarl[t][1]]\n",
    "        DA = lDA[tarl[t][1]]\n",
    "\n",
    "        cossim = np.sum(DA * DB) / (np.sqrt(np.sum(DA * DA)) * np.sqrt(np.sum(DB * DB)))\n",
    "\n",
    "        if cossim < 0.85:\n",
    "            continue\n",
    "\n",
    "        sen = \" \".join(tarl[t][0])# + \"\\t!@#$%^& \" + str(math.exp(value[index]))#.replace(\" ##\", \"\")\n",
    "        \n",
    "          \n",
    "        gen.append([cossim, sen])\n",
    "    # if len(lcache) > 4:\n",
    "    #     lcache = lcache[1:]    \n",
    "\n",
    "    # lcache.append([inpori, \" \".join(tokens), gen])\n",
    "    gen.sort(key=lambda x: x[0], reverse=True)\n",
    "    return \" \".join(tokens), gen#.replace(\" ##\", \"\"), gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertmodel, berttoken, bertori = bertInit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar, gen = BertM(bertmodel, berttoken, \"Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .\",  bertori, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.98236746,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.97620964,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdoms prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9493239,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Britain prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.8952423,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United UK prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.86621743,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United England prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.8549576,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United GB prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_index(bertmodel, berttoken, line, bertori, token_index, top_tokens):\n",
    "    result = []\n",
    "    tokens = berttoken.tokenize(line)\n",
    "    # print(tokens)\n",
    "    tar=\"\"\n",
    "    for i in range(len(token_index)):\n",
    "        idx = max(token_index[i]-1, 0)\n",
    "        try: \n",
    "            tokens[idx].lower() == top_tokens[i].lower()\n",
    "        except:\n",
    "            print(idx)\n",
    "            print(token_index)\n",
    "            print(tokens)\n",
    "            print(top_tokens[i])\n",
    "        if tokens[idx].lower() == top_tokens[i].lower():\n",
    "            tar, gen = BertM(bertmodel, berttoken, line, bertori, idx)\n",
    "            # print(tokens[idx])\n",
    "            # print(gen)\n",
    "            result.append(gen[0])\n",
    "        else:\n",
    "            idx = max(token_index[i]-3, 0)\n",
    "            while idx<min(token_index[i]+4, len(tokens)):\n",
    "                if tokens[idx].lower() == top_tokens[i].lower():\n",
    "                    tar, gen = BertM(bertmodel, berttoken, line, bertori, idx)\n",
    "                    result.append(gen[0])\n",
    "                    break\n",
    "                idx +=1\n",
    "    return tar, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('28 - Year - Old Chef Found Dead at San Francisco Mall',\n",
       " [[0.968499, '28 - Year - Old Chef Found Missing at San Francisco Mall'],\n",
       "  [0.9966399, '28 - Year - Old Chef Found Dead at New Francisco Mall'],\n",
       "  [0.9750549, '28 - Year - Old Chef Shot Dead at San Francisco Mall'],\n",
       "  [0.9335128, '28 - Year - Old Chef Found Dead at San Diego Mall'],\n",
       "  [0.96899986, '28 - Year - Old Assistant Found Dead at San Francisco Mall']])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line=\"28-Year-Old Chef Found Dead at San Francisco Mall\"\n",
    "token_index=[\n",
    "            8,\n",
    "            10,\n",
    "            7,\n",
    "            11,\n",
    "            6\n",
    "        ]\n",
    "top_tokens=[\n",
    "            \"Dead\",\n",
    "            \"San\",\n",
    "            \"Found\",\n",
    "            \"Francisco\",\n",
    "            \"Chef\"\n",
    "        ]\n",
    "align_index(bertmodel, berttoken, line, bertori, token_index, top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  1%|          | 15/2001 [00:09<21:13,  1.56it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m top_tokens \u001b[39m=\u001b[39m sent_data[i][\u001b[39m\"\u001b[39m\u001b[39mtop_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[39m#tag = nlp.pos_tag(line)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tar, gen \u001b[39m=\u001b[39m align_index(bertmodel, berttoken, line, bertori, index, top_tokens)\n\u001b[1;32m     16\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(gen)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m, in \u001b[0;36malign_index\u001b[0;34m(bertmodel, berttoken, line, bertori, token_index, top_tokens)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(token_index)):\n\u001b[1;32m      7\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(token_index[i]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mif\u001b[39;00m tokens[idx]\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m top_tokens[i]\u001b[39m.\u001b[39mlower():\n\u001b[1;32m      9\u001b[0m         tar, gen \u001b[39m=\u001b[39m BertM(bertmodel, berttoken, line, bertori, idx)\n\u001b[1;32m     10\u001b[0m         \u001b[39m# print(tokens[idx])\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[39m# print(gen)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# with open(sys.argv[1]) as f:\n",
    "with open(\"../CAT/NewThres/TestGenerator-NMT/en_token.json\") as f:\n",
    "    sent_data = json.load(f)\n",
    "f = open(\"../CAT/NewThres/TestGenerator-NMT/f_en_mu_grad.txt\", \"w\")\n",
    "fline = open(\"../CAT/NewThres/TestGenerator-NMT/f_en_mu_grad.index\", \"w\")\n",
    "bertmodel, berttoken, bertori = bertInit()\n",
    "\n",
    "for i in tqdm(range(len(sent_data))):\n",
    "    line = sent_data[i][\"en\"]\n",
    "    index= sent_data[i][\"token_index\"]\n",
    "    top_tokens = sent_data[i][\"top_tokens\"]\n",
    "    #tag = nlp.pos_tag(line)\n",
    "\n",
    "    tar, gen = align_index(bertmodel, berttoken, line, bertori, index, top_tokens)\n",
    "    gen = sorted(gen)[::-1]\n",
    "    count = 0\n",
    "    for sen in gen:\n",
    "        f.write(tar.strip() + \"\\n\")\n",
    "        f.write(sen[1].strip() + \"\\n\")\n",
    "        fline.write(str(i) + \"\\n\")\n",
    "        count += 1\n",
    "        if count >= Max_Mutants:\n",
    "            break\n",
    "f.close()\n",
    "fline.close()\n",
    "print (time.time() - ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "K_Number = 100\n",
    "Max_Mutants = 5\n",
    "\n",
    "ft = time.time()\n",
    "\n",
    "lcache = []\n",
    "def BertM2 (bert, berttoken, inpori, bertori):\n",
    "    global lcache\n",
    "    # for k in lcache:\n",
    "    #     if inpori == k[0]:\n",
    "    #         return k[1], k[2]\n",
    "    sentence = inpori\n",
    "\n",
    "    tokens = berttoken.tokenize(sentence)\n",
    "    batchsize = 1000 // len(tokens)\n",
    "\n",
    "    gen = []\n",
    "    ltokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "    try:\n",
    "        encoding = [berttoken.convert_tokens_to_ids(ltokens[0:i] + [\"[MASK]\"] + ltokens[i + 1:]) for i in range(1, len(ltokens) - 1)]#.cuda()\n",
    "    except:\n",
    "        return \" \".join(tokens), gen\n",
    "    p = []\n",
    "    print(\"encoding :\", len(encoding))\n",
    "    for i in range(0, len(encoding), batchsize):\n",
    "        tensor = torch.tensor(encoding[i: min(len(encoding), i + batchsize)]).cuda()\n",
    "        print(\"tensor size\", tensor.size())\n",
    "        pre = F.softmax(bert(tensor)[0], dim=-1).data.cpu() # logits of the bertmodel, (1,42,28996)\n",
    "        p.append(pre)\n",
    "\n",
    "    pre = torch.cat(p, 0) # concate on 0 dimension \n",
    "    print(\"pre size\", pre.size()) #(40, 42, 28996), (token, layer, dimension in bert)\n",
    "\n",
    "    tarl = [[tokens, -1]]\n",
    "    for i in range(len(tokens)):\n",
    "\n",
    "        if tokens[i] in string.punctuation:\n",
    "            continue\n",
    "        topk = torch.topk(pre[i][i + 1], K_Number)#.tolist()\n",
    "        \n",
    "        value = topk[0].numpy() #top values\n",
    "        topk = topk[1].numpy().tolist() # index of the top values\n",
    "        \n",
    "        #print (topk)\n",
    "        topkTokens = berttoken.convert_ids_to_tokens(topk)\n",
    "       # tarl = []\n",
    "        #print(topkTokens)\n",
    "        for index in range(len(topkTokens)):\n",
    "            if value[index] < 0.05:\n",
    "                break\n",
    "            tt = topkTokens[index]\n",
    "            #print (tt)\n",
    "            if tt in string.punctuation:\n",
    "                continue\n",
    "            if tt.strip() == tokens[i].strip():\n",
    "                continue\n",
    "            l = deepcopy(tokens)\n",
    "            l[i] = tt\n",
    "            tarl.append([l, i, value[index]])\n",
    "        #print(\"tarl:\", tarl)\n",
    "        \n",
    "    if len(tarl) == 0:\n",
    "        return \" \".join(tokens), gen\n",
    "        \n",
    "    \n",
    "    \n",
    "    lDB = []\n",
    " \n",
    "    for i in range(0, len(tarl), batchsize):\n",
    "        lDB.append(bertori(torch.tensor([berttoken.convert_tokens_to_ids([\"[CLS]\"] + l[0] + [\"[SEP]\"]) for l in tarl[i: min(i + batchsize, len(tarl))]]).cuda())[0].data.cpu().numpy())\n",
    "    lDB = np.concatenate(lDB, axis=0)\n",
    "            \n",
    "\n",
    "    lDA = lDB[0]\n",
    "    assert len(lDB) == len(tarl)\n",
    "    tarl = tarl[1:]\n",
    "    lDB = lDB[1:]\n",
    "    for t in range(len(lDB)):\n",
    "        DB = lDB[t][tarl[t][1]]\n",
    "        DA = lDA[tarl[t][1]]\n",
    "\n",
    "        cossim = np.sum(DA * DB) / (np.sqrt(np.sum(DA * DA)) * np.sqrt(np.sum(DB * DB)))\n",
    "\n",
    "        if cossim < 0.85:\n",
    "            continue\n",
    "\n",
    "        sen = \" \".join(tarl[t][0])# + \"\\t!@#$%^& \" + str(math.exp(value[index]))#.replace(\" ##\", \"\")\n",
    "        \n",
    "          \n",
    "        gen.append([cossim, sen])\n",
    "    if len(lcache) > 4:\n",
    "        lcache = lcache[1:]    \n",
    "\n",
    "    lcache.append([inpori, \" \".join(tokens), gen])\n",
    "    gen.sort(key=lambda x: x[0], inverse=True)\n",
    "    return \" \".join(tokens), gen#.replace(\" ##\", \"\"), gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding : 40\n",
      "tensor size torch.Size([25, 42])\n",
      "tensor size torch.Size([15, 42])\n",
      "pre size torch.Size([40, 42, 28996])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.99286634,\n",
       "  'Li Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.98540044,\n",
       "  'Chen Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9864119,\n",
       "  'Teresa also chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.98795044,\n",
       "  'Teresa May chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.98682374,\n",
       "  'Teresa later chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9869999,\n",
       "  'Teresa then chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9906484,\n",
       "  'Teresa Mei decided to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9952511,\n",
       "  'Teresa Mei chose to spend a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9299741,\n",
       "  'Teresa Mei chose to take her vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9884781,\n",
       "  'Teresa Mei chose to take a holiday in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.990673,\n",
       "  'Teresa Mei chose to take a vacation to Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.99882156,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . It is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.990628,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This was the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9790232,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the second time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9621241,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that the United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9774061,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that any United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9978993,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister decided to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9892731,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to take vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9897937,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go holiday in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.98764527,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation to Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9702994,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Europe after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9984566,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland since Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .'],\n",
       " [0.9346081,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom The Daily Telegraph .'],\n",
       " [0.9863517,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper The Telegraph .'],\n",
       " [0.9575202,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Mail .'],\n",
       " [0.9366686,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Mirror .'],\n",
       " [0.9335289,\n",
       "  'Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Express .']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar2, gen2 = BertM2(bertmodel, berttoken, \"Teresa Mei chose to take a vacation in Switzerland . This is the first time that a United Kingdom prime minister chose to go vacation in Switzerland after Margaret Thatcher , according to the United Kingdom newspaper Daily Telegraph .\",  bertori)\n",
    "gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c818903a972b15f948093850adeb30c84eba2d54a31a843f1ceb959d2841b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
