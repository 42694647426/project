{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/hanyings/conda_envs/tnmt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset wmt17 (/data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:36<00:00, 12.11s/it]\n",
      "/tmp/ipykernel_3385/2303173043.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"wmt17\", \"zh-en\", cache_dir=\"/data2/hanyings/.cache\")\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 18:44:07.488185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 18:44:12.927504: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
      "2022-12-12 18:44:12.927881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
      "2022-12-12 18:44:12.927890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline, MarianTokenizer, MarianTokenizer, TFMarianMTModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 25134743\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2002\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = raw_datasets[\"train\"].train_test_split(test_size=1)\n",
    "raw_datasets[\"train\"] = split[\"test\"]\n",
    "raw_datasets[\"validation\"] = split[\"test\"]\n",
    "raw_datasets[\"test\"] = split[\"test\"]\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = \"\"\n",
    "# max_input_length = 128\n",
    "# max_target_length = 128\n",
    "# source_lang = \"en\"\n",
    "# target_lang = \"zh\"\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "#     targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "#     # Setup the tokenizer for targets\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data2/hanyings/hf_transformer/tranformer_gradient.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m preprocess \u001b[39m=\u001b[39m preprocess_function(raw_datasets[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m preprocess\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_function' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess = preprocess_function(raw_datasets['train'][:1])\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 171.79ba/s]\n",
      "Loading cached processed dataset at /data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953/cache-7ffeacd63e3c9838.arrow\n",
      "Loading cached processed dataset at /data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953/cache-7ffeacd63e3c9838.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': '(c) Encourage and support participatory research with street-connected children and families to inform policy-making and design of specialized interventions.',\n",
       "  'zh': 'é¼“åŠ±å’Œæ”¯åŠ©å›´ç»•ä¸Žè¡—å¤´æœ‰è”ç³»çš„å„¿ç«¥åŠå…¶å®¶äººå¼€å±•å‚ä¸Žåž‹ç ”ç©¶ï¼Œä¸ºåˆ¶å®šæ”¿ç­–å’Œè®¾è®¡ä¸“é—¨å¹²é¢„æŽªæ–½æä¾›ä¿¡æ¯ã€‚'},\n",
       " 'input_ids': [22,\n",
       "  149,\n",
       "  17,\n",
       "  20327,\n",
       "  7,\n",
       "  125,\n",
       "  10252,\n",
       "  1304,\n",
       "  29,\n",
       "  8979,\n",
       "  16,\n",
       "  56308,\n",
       "  238,\n",
       "  7,\n",
       "  2106,\n",
       "  9,\n",
       "  3842,\n",
       "  419,\n",
       "  16,\n",
       "  2516,\n",
       "  7,\n",
       "  3634,\n",
       "  4,\n",
       "  2372,\n",
       "  6529,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [5078,\n",
       "  12103,\n",
       "  13169,\n",
       "  67,\n",
       "  21047,\n",
       "  25153,\n",
       "  568,\n",
       "  28152,\n",
       "  589,\n",
       "  467,\n",
       "  3517,\n",
       "  751,\n",
       "  2,\n",
       "  76,\n",
       "  25892,\n",
       "  18,\n",
       "  3808,\n",
       "  2728,\n",
       "  15031,\n",
       "  12224,\n",
       "  10,\n",
       "  0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenized_datasets[\"train\"][0][\"input_ids\"][1]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0029,  0.0093, -0.0038,  ..., -0.0296, -0.0405, -0.0319],\n",
       "        [ 0.0101,  0.0050,  0.0023,  ..., -0.0351, -0.0636, -0.0068],\n",
       "        [ 0.0057,  0.0075,  0.0078,  ..., -0.0462, -0.0066, -0.0438],\n",
       "        ...,\n",
       "        [ 0.0026,  0.0119, -0.0073,  ..., -0.0304, -0.0043, -0.0533],\n",
       "        [-0.0153,  0.0381, -0.0026,  ..., -0.0481, -0.0228, -0.0146],\n",
       "        [ 0.0114,  0.0352, -0.0117,  ..., -0.0167, -0.0142, -0.0327]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "old_embed = model.get_encoder().embed_tokens(torch.tensor(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
    "old_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data2/hanyings/hf_transformer/tranformer_gradient.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     result \u001b[39m=\u001b[39m {k: \u001b[39mround\u001b[39m(v, \u001b[39m4\u001b[39m) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     args,\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"./tmp_trainer\",\n",
    "    evaluation_strategy = \"epoch\", # evaluate on valid dataset at emd pf each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    metric_for_best_model = \"bleu\",  #Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "    # bleu in the compute metric ?   \n",
    "    save_strategy=\"no\"\n",
    "    \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import numpy as np\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, tokenize=\"zh\",smooth_method=\"add-k\")\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0144e-02,  5.0447e-03,  2.2518e-03, -1.7374e-02,  2.7206e-02,\n",
       "          1.6035e-02, -1.3437e-02, -1.8341e-02,  3.6228e-02, -2.5258e-02,\n",
       "         -1.5547e-02, -8.9262e-04, -2.0592e-02,  1.1161e-02,  7.9412e-03,\n",
       "         -4.9970e-03, -2.5273e-02, -1.9579e-02, -3.4272e-02, -1.3408e-02,\n",
       "         -2.3376e-03, -2.8470e-02, -1.8016e-02,  5.9842e-04,  2.8821e-03,\n",
       "          2.2967e-03, -1.5486e-02,  1.0980e-01,  7.5325e-03,  3.2127e-03,\n",
       "         -8.1589e-02, -1.0854e-02, -1.1434e-02,  1.8830e-02, -6.8241e-03,\n",
       "          6.0114e-03, -3.5065e-02, -7.3030e-02,  2.9641e-02, -1.3873e-02,\n",
       "          2.6968e-02, -3.1924e-02, -2.0414e-05, -2.8745e-02, -1.6473e-02,\n",
       "         -1.7155e-02,  1.2791e-02,  3.6385e-02, -4.6023e-02, -7.3509e-02,\n",
       "          3.4076e-02, -2.7510e-02, -7.1822e-02,  5.6939e-03,  1.2389e-02,\n",
       "         -1.3336e-02, -1.4755e-02,  1.6470e-02, -2.7795e-02, -3.4384e-02,\n",
       "         -3.8219e-02, -2.6174e-02, -3.0673e-02, -1.9236e-02, -2.7159e-03,\n",
       "          2.3940e-03,  1.1802e-01, -2.5492e-02, -9.2612e-03, -7.1148e-03,\n",
       "          1.4022e-01, -2.8818e-02, -1.4391e-02, -3.4423e-02, -6.0696e-03,\n",
       "         -1.2687e-02, -3.5247e-02, -3.0287e-02, -7.9636e-03,  2.6026e-03,\n",
       "         -4.4743e-02, -1.1297e-02,  2.2631e-02, -2.1105e-02, -1.5501e-02,\n",
       "          1.1369e-03, -1.7882e-02,  1.6888e-02, -3.1032e-02, -3.6135e-02,\n",
       "         -1.7765e-02, -7.8464e-03, -1.4416e-02, -8.0215e-02,  1.2121e-02,\n",
       "         -7.2677e-02,  1.0158e-02, -3.9664e-02,  2.6785e-02, -6.7036e-03,\n",
       "         -9.9276e-03, -2.8247e-02,  2.5079e-02,  4.1802e-02, -4.1905e-02,\n",
       "         -1.9546e-02, -1.5264e-02,  2.3602e-02, -1.4398e-02, -3.5282e-02,\n",
       "         -4.6212e-02,  1.8887e-02,  5.7713e-03, -4.7070e-02,  4.4385e-02,\n",
       "          1.5856e-02,  4.6985e-03, -3.9593e-02, -5.7036e-02, -1.8022e-02,\n",
       "          1.4588e-02,  1.9499e-02,  2.6011e-02, -3.7851e-02,  1.9428e-03,\n",
       "         -1.0478e-02, -1.3784e-01,  2.4836e-03, -1.2924e-02,  4.4306e-03,\n",
       "          4.8342e-03, -1.9531e-02,  1.0859e-02,  4.2091e-02, -8.9936e-03,\n",
       "         -7.5945e-03, -3.6594e-02, -1.1918e-02,  2.7858e-02,  6.5688e-03,\n",
       "         -3.9132e-03, -5.2792e-02,  8.0137e-03, -3.8845e-02, -3.7652e-02,\n",
       "         -2.0016e-02, -1.5774e-02,  1.0347e-02, -1.0737e-02, -8.2504e-03,\n",
       "         -9.9088e-02, -1.3425e-02,  2.1717e-02,  1.9104e-02, -8.1028e-02,\n",
       "         -1.2665e-02,  3.0977e-02,  4.2822e-03, -2.8734e-03, -4.1488e-03,\n",
       "         -1.2540e-02,  4.7409e-02, -2.1923e-02, -3.0786e-02, -1.1249e-02,\n",
       "         -3.3386e-02,  1.3177e-02, -1.5061e-02, -5.2403e-02,  7.2520e-04,\n",
       "         -4.0260e-03, -8.4852e-03,  6.0007e-04,  7.7135e-03,  7.4816e-03,\n",
       "          5.7486e-04, -4.2414e-03, -9.0401e-05, -4.2420e-02,  3.8312e-02,\n",
       "          5.9253e-03,  5.9632e-03, -1.3232e-02, -1.0571e-02, -3.9798e-02,\n",
       "         -1.4132e-02,  2.0193e-02, -1.4147e-03, -7.2251e-03, -3.8267e-02,\n",
       "         -3.5801e-02, -3.7000e-03,  7.5324e-03, -1.1646e-02, -2.4298e-02,\n",
       "         -3.4766e-02,  2.9306e-03, -2.3712e-02,  1.7135e-02, -1.8196e-02,\n",
       "          3.4036e-02, -1.2833e-02, -6.5631e-04, -1.9171e-02,  3.5188e-03,\n",
       "          8.9714e-04, -2.1791e-02,  2.8418e-02,  1.7828e-03,  2.4052e-02,\n",
       "         -1.6332e-02, -1.9221e-02, -3.2166e-02, -9.3802e-03,  2.5117e-03,\n",
       "         -2.5607e-02,  1.5074e-03, -2.0983e-02, -1.0713e-02,  2.3681e-02,\n",
       "          3.7187e-03,  5.1689e-02,  1.7011e-02, -4.6576e-02, -2.1775e-02,\n",
       "          3.7017e-02, -2.7272e-02,  2.8393e-02, -1.3220e-02, -1.9342e-02,\n",
       "         -1.2248e-04,  1.2896e-02,  4.9169e-02, -2.3413e-02, -5.7153e-03,\n",
       "         -2.1099e-02, -8.5127e-05, -3.8823e-03,  1.4314e-02, -7.5083e-03,\n",
       "         -2.2582e-02,  1.2176e-02, -2.3239e-02, -2.5366e-02, -1.3950e-02,\n",
       "         -2.1838e-02,  5.4910e-03,  4.1100e-02, -2.0033e-02, -6.2552e-03,\n",
       "         -5.8284e-03, -7.9814e-04,  9.5206e-03, -2.0854e-02, -3.9550e-02,\n",
       "         -1.7424e-02, -2.5270e-02, -1.4128e-02,  2.0236e-03,  1.3963e-03,\n",
       "         -4.6856e-03,  8.2189e-03, -3.2945e-03,  2.8958e-03,  2.1670e-03,\n",
       "         -1.1171e-01, -2.2781e-02,  6.7681e-03,  1.1717e-02, -8.7468e-03,\n",
       "         -2.1943e-03,  2.4086e-02, -1.8727e-02, -2.2616e-02,  2.0182e-02,\n",
       "         -2.2768e-02, -9.4120e-03, -1.8339e-02,  3.0388e-02, -2.9767e-02,\n",
       "          4.0143e-02, -1.5286e-02,  5.6731e-02, -3.1053e-03, -1.2375e-02,\n",
       "         -4.3095e-02,  2.8043e-02, -1.6439e-02,  2.0403e-02, -1.0477e-02,\n",
       "         -2.8225e-02,  1.1060e-02,  2.5165e-03,  4.8313e-02,  2.1227e-02,\n",
       "         -1.6243e-02,  3.4914e-02, -1.3750e-02, -9.0781e-03, -4.9718e-02,\n",
       "         -3.7223e-02,  2.3713e-02,  1.1352e-03,  3.2946e-03, -4.7943e-02,\n",
       "         -2.4482e-03, -1.0544e-02,  1.0671e-02,  1.2282e-02,  1.1301e-02,\n",
       "          3.9140e-02,  9.5448e-03, -3.2689e-02, -1.7156e-02, -4.3522e-04,\n",
       "         -1.8948e-02,  1.5231e-02,  1.2044e-02, -2.0130e-02, -3.7036e-02,\n",
       "          6.9039e-03,  4.6164e-03, -1.3590e-02,  3.4360e-02, -4.1272e-03,\n",
       "         -5.1465e-02,  3.3865e-02, -3.5306e-02,  3.1607e-02, -1.2925e-02,\n",
       "         -4.4976e-02, -3.9552e-02,  4.1883e-02, -3.4554e-02, -2.5847e-02,\n",
       "         -2.5894e-02,  4.7220e-03,  1.7151e-02,  4.2185e-02,  6.4257e-02,\n",
       "         -2.7060e-02, -4.1588e-02, -3.7941e-02, -2.2800e-02, -7.4876e-02,\n",
       "         -5.2146e-03, -2.0583e-02, -5.5267e-03, -3.7849e-02, -3.9694e-02,\n",
       "         -3.1545e-02, -2.2449e-02,  1.9640e-02, -1.3358e-02, -5.3818e-02,\n",
       "         -1.7025e-02, -3.0966e-02, -3.6936e-02, -3.1892e-02,  5.8818e-03,\n",
       "         -3.5266e-02, -1.5223e-02, -1.5426e-02, -4.6621e-02,  6.7564e-06,\n",
       "         -6.3605e-02, -2.0153e-02, -1.4933e-02, -3.5448e-02, -4.9301e-02,\n",
       "         -1.0240e-01, -7.2155e-02, -4.5576e-02, -5.2396e-02, -1.8503e-02,\n",
       "         -3.4641e-02, -5.0109e-02, -1.5780e-02, -1.0548e-02,  2.2084e-02,\n",
       "         -3.9116e-02, -5.8432e-02,  3.6269e-02,  8.6668e-02,  6.9612e-03,\n",
       "         -5.3569e-02, -1.4835e-02,  2.6466e-02, -4.8096e-02,  1.8791e-03,\n",
       "         -8.2218e-02,  1.3346e-02, -4.7724e-02, -3.4380e-02, -7.7694e-03,\n",
       "         -5.1997e-02, -6.9080e-02, -3.4150e-02, -3.4739e-02, -3.4111e-02,\n",
       "         -7.8168e-02, -1.0128e-02, -3.2493e-02, -7.4757e-02, -5.7266e-03,\n",
       "         -5.3008e-02, -3.6866e-03, -3.2419e-02, -4.8439e-02, -2.3148e-02,\n",
       "         -4.1254e-02, -3.2640e-02, -4.7205e-02, -7.7067e-02, -4.0622e-02,\n",
       "         -4.0189e-02, -2.7122e-02, -2.8383e-02, -6.5369e-02, -1.7008e-02,\n",
       "         -3.3952e-02, -2.8662e-02, -2.1175e-02, -7.7075e-02, -2.3717e-02,\n",
       "         -3.8173e-02, -6.3327e-02,  1.1757e-02, -2.2832e-02, -3.3648e-02,\n",
       "         -1.9979e-03,  1.1446e-02,  1.9439e-02,  1.7276e-02,  3.9171e-03,\n",
       "          9.0313e-03, -3.4779e-02, -1.6778e-02, -5.7317e-02, -5.6195e-02,\n",
       "          6.0527e-03, -3.1159e-02,  2.3826e-02, -4.2355e-02, -1.9771e-02,\n",
       "         -4.8552e-02, -4.6355e-02, -1.6896e-02, -1.5037e-02, -5.2866e-02,\n",
       "         -4.2994e-02, -1.7111e-02, -4.1910e-02, -4.7766e-02, -3.2400e-02,\n",
       "         -8.0628e-02, -2.5397e-02, -4.4414e-02, -1.0951e-02, -9.3462e-03,\n",
       "         -9.2441e-03, -3.9443e-02, -3.2005e-02, -3.1103e-02, -5.5955e-02,\n",
       "         -1.2078e-02, -5.1119e-03,  1.2339e-02, -4.6944e-02, -2.7991e-02,\n",
       "         -7.8758e-03, -5.6421e-02, -4.5338e-03, -4.1237e-02,  4.8806e-03,\n",
       "          4.9462e-03, -3.7349e-02, -3.5336e-02, -3.3513e-02, -3.6708e-02,\n",
       "         -1.2737e-02, -5.8968e-02, -2.3300e-02, -5.7048e-02, -8.6473e-02,\n",
       "         -5.1932e-02, -5.9022e-02,  1.6152e-02, -2.5908e-02, -2.6014e-02,\n",
       "         -2.9393e-02, -5.2143e-02, -2.8282e-02, -6.7060e-02, -5.7476e-02,\n",
       "         -4.8307e-02, -4.6125e-02, -4.5338e-02, -2.6244e-02,  3.0352e-02,\n",
       "         -5.4137e-02, -6.0205e-02, -2.4088e-02, -3.1778e-02,  4.9711e-02,\n",
       "         -4.8104e-02,  4.0377e-02, -3.8231e-02, -2.0145e-02, -3.5142e-02,\n",
       "         -6.3647e-02, -6.8191e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "new_embed = model.get_encoder().embed_tokens(torch.tensor([token]))\n",
    "new_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999964237213135"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "1 - cosine(old_embed[0].detach().numpy(), new_embed[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.arrow_dataset import Dataset\n",
    "Dataset.from_dict(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions wrapup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "Found cached dataset wmt17 (/data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 69.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline, MarianTokenizer, MarianTokenizer, TFMarianMTModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name, return_tensors=\"pt\")\n",
    "from datasets import load_dataset, load_metric\n",
    "raw_datasets = load_dataset(\"wmt17\", \"zh-en\", cache_dir=\"/data2/hanyings/.cache\")\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "from datasets.arrow_dataset import Dataset\n",
    "prefix = \"\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"translation\"] = examples[\"translation\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "def preprocess_dataset(num_ex):\n",
    "    raw_datasets = load_dataset(\"wmt17\", \"zh-en\", cache_dir=\"/data2/hanyings/.cache\")   \n",
    "    preprocess = preprocess_function(raw_datasets['train'][num_ex:num_ex+1])\n",
    "    return Dataset.from_dict(preprocess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt17 (/data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 78.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'translation'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = preprocess_dataset(7)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2666,\n",
       "  1306,\n",
       "  2,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  2793,\n",
       "  4,\n",
       "  6035,\n",
       "  1116,\n",
       "  2229,\n",
       "  49763,\n",
       "  66,\n",
       "  2535,\n",
       "  9,\n",
       "  146,\n",
       "  29,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  23396,\n",
       "  17878,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3952,\n",
       "  2,\n",
       "  2905,\n",
       "  3649,\n",
       "  7370,\n",
       "  11801,\n",
       "  3921,\n",
       "  16562,\n",
       "  18,\n",
       "  18332,\n",
       "  11360,\n",
       "  12,\n",
       "  3921,\n",
       "  34390,\n",
       "  5145,\n",
       "  1720,\n",
       "  10,\n",
       "  0],\n",
       " 'translation': {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'zh': 'å½“ç„¶ï¼Œé›·æ›¼å…„å¼Ÿå…¬å¸çš„å€’é—­å’ŒæŸæž—å¢™çš„å€’å¡Œæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline, MarianTokenizer, MarianTokenizer, TFMarianMTModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "def get_old_embed(dataset):\n",
    "    mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "    model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "    return model.get_encoder().embed_tokens(torch.tensor(dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed = get_old_embed(dataset)\n",
    "old_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0153, -0.0038,  0.0085,  ..., -0.0577, -0.0065, -0.0415],\n",
       "        [-0.0123, -0.0079, -0.0223,  ..., -0.0177, -0.0312, -0.0419],\n",
       "        [ 0.0114,  0.0256,  0.0116,  ..., -0.0563, -0.0548, -0.0277],\n",
       "        ...,\n",
       "        [ 0.0466, -0.0057,  0.0314,  ..., -0.0157, -0.0185, -0.0190],\n",
       "        [-0.0154,  0.0380, -0.0025,  ..., -0.0482, -0.0226, -0.0147],\n",
       "        [ 0.0114,  0.0351, -0.0119,  ..., -0.0167, -0.0144, -0.0326]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "def retrain(dataset):\n",
    "    mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "    model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mode_name, return_tensors=\"pt\")\n",
    "    \n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        \"./tmp_trainer\",\n",
    "        evaluation_strategy = \"epoch\", # evaluate on valid dataset at emd pf each epoch\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=10,\n",
    "        predict_with_generate=True,\n",
    "        metric_for_best_model = \"bleu\",  #Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "        # bleu in the compute metric ?   \n",
    "        save_strategy=\"no\"\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    import numpy as np\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        return preds, labels\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, tokenize=\"zh\",smooth_method=\"add-k\")\n",
    "        result = {\"bleu\": result[\"score\"]}\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        return result\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353989</td>\n",
       "      <td>51.321600</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227323</td>\n",
       "      <td>67.208600</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.150327</td>\n",
       "      <td>89.820700</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.103348</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.071431</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.052304</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.040667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033638</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.029591</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.027723</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = retrain(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_embed(model, dataset):\n",
    "    model.to(\"cpu\")\n",
    "    return model.get_encoder().embed_tokens(torch.tensor(dataset[0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0153, -0.0037,  0.0086,  ..., -0.0578, -0.0064, -0.0415],\n",
       "        [-0.0124, -0.0079, -0.0223,  ..., -0.0178, -0.0311, -0.0419],\n",
       "        [ 0.0115,  0.0256,  0.0115,  ..., -0.0563, -0.0549, -0.0276],\n",
       "        ...,\n",
       "        [ 0.0467, -0.0057,  0.0314,  ..., -0.0156, -0.0185, -0.0190],\n",
       "        [-0.0153,  0.0381, -0.0025,  ..., -0.0482, -0.0226, -0.0147],\n",
       "        [ 0.0113,  0.0350, -0.0119,  ..., -0.0167, -0.0144, -0.0325]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embed = get_new_embed(model, dataset)\n",
    "new_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_sim(dataset, old_embed, new_embed):\n",
    "    output = dataset[0]\n",
    "    output[\"cosine_sim\"] = []\n",
    "    for i in range(len(old_embed)):\n",
    "        cos = 1 - cosine(old_embed[i].detach().numpy(), new_embed[i].detach().numpy())\n",
    "        output[\"cosine_sim\"].append(cos)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2666,\n",
       "  1306,\n",
       "  2,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  2793,\n",
       "  4,\n",
       "  6035,\n",
       "  1116,\n",
       "  2229,\n",
       "  49763,\n",
       "  66,\n",
       "  2535,\n",
       "  9,\n",
       "  146,\n",
       "  29,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  23396,\n",
       "  17878,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3952,\n",
       "  2,\n",
       "  2905,\n",
       "  3649,\n",
       "  7370,\n",
       "  11801,\n",
       "  3921,\n",
       "  16562,\n",
       "  18,\n",
       "  18332,\n",
       "  11360,\n",
       "  12,\n",
       "  3921,\n",
       "  34390,\n",
       "  5145,\n",
       "  1720,\n",
       "  10,\n",
       "  0],\n",
       " 'translation': {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'zh': 'å½“ç„¶ï¼Œé›·æ›¼å…„å¼Ÿå…¬å¸çš„å€’é—­å’ŒæŸæž—å¢™çš„å€’å¡Œæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚'},\n",
       " 'cosine_sim': [0.9999985694885254,\n",
       "  0.9999991059303284,\n",
       "  0.9999980926513672,\n",
       "  0.9999982714653015,\n",
       "  0.9999990463256836,\n",
       "  0.9999983310699463,\n",
       "  0.9999982714653015,\n",
       "  0.9999990463256836,\n",
       "  0.9999983310699463,\n",
       "  0.9999989867210388,\n",
       "  0.9999985098838806,\n",
       "  0.9999988675117493,\n",
       "  0.9999985098838806,\n",
       "  0.9999980926513672,\n",
       "  0.9999982714653015,\n",
       "  0.9999979138374329,\n",
       "  0.9999983906745911,\n",
       "  0.9999982714653015,\n",
       "  0.9999982714653015,\n",
       "  0.9999990463256836,\n",
       "  0.9999983310699463,\n",
       "  0.9999982714653015,\n",
       "  0.9999987483024597,\n",
       "  0.9999991655349731,\n",
       "  0.9999985694885254,\n",
       "  0.9999986886978149]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_data = cosine_sim(dataset, old_embed, new_embed)\n",
    "cosine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â–1929'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(51091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "def top_k_token(dataset_cos, tokenizer, k=3):\n",
    "    idx = np.argsort(dataset_cos[\"cosine_sim\"])[:k]\n",
    "    input_tokens = [dataset_cos['input_ids'][i] for i in idx ]\n",
    "    return [tokenizer.decode(t) for t in input_tokens if tokenizer.decode(t) not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'has']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_token(cosine_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "punc = string.punctuation\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74, 2]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['â–been', ','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â–been'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â–nothing'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string('â–nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "def is_stopword(token_id, tokenizer):\n",
    "    word = tokenizer.decode(token_id)\n",
    "    if word in string.punctuation:\n",
    "        return True\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    stopword = stopwords.words('english') + [\"</s>\", \"<unk>\", \">>cmn_Hans<<\", \"<pad>\"]\n",
    "    if word in stopword or not word:\n",
    "        return True\n",
    "    return False  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_stopword(243, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def wrap(num_ex, k=3):\n",
    "    output = []\n",
    "    for i in range(num_ex):\n",
    "        id = random.randint(0, 2000)\n",
    "        dataset = preprocess_dataset(i)\n",
    "        old_embed = get_old_embed(dataset)\n",
    "        model, tokenizer = retrain(dataset)\n",
    "        new_embed = get_new_embed(model, dataset)\n",
    "        cosine_data = cosine_sim(dataset, old_embed, new_embed)\n",
    "        output.append({\"translation\": cosine_data[\"translation\"], \"top_tokens\": top_k_token(cosine_data, tokenizer, k=k)})\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_sim_nostop(dataset, old_embed, new_embed, tokenizer):\n",
    "    output = dataset[0]\n",
    "    output[\"cosine_sim\"] = []\n",
    "    for i in range(len(old_embed)):\n",
    "        current_token = output[\"input_ids\"][i]\n",
    "        if is_stopword(current_token, tokenizer):\n",
    "            output[\"cosine_sim\"].append(100)\n",
    "            continue\n",
    "        cos = 1 - cosine(old_embed[i].detach().numpy(), new_embed[i].detach().numpy())\n",
    "        output[\"cosine_sim\"].append(cos)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2666,\n",
       "  1306,\n",
       "  2,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  2793,\n",
       "  4,\n",
       "  6035,\n",
       "  1116,\n",
       "  2229,\n",
       "  49763,\n",
       "  66,\n",
       "  2535,\n",
       "  9,\n",
       "  146,\n",
       "  29,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  23396,\n",
       "  17878,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3952,\n",
       "  2,\n",
       "  2905,\n",
       "  3649,\n",
       "  7370,\n",
       "  11801,\n",
       "  3921,\n",
       "  16562,\n",
       "  18,\n",
       "  18332,\n",
       "  11360,\n",
       "  12,\n",
       "  3921,\n",
       "  34390,\n",
       "  5145,\n",
       "  1720,\n",
       "  10,\n",
       "  0],\n",
       " 'translation': {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'zh': 'å½“ç„¶ï¼Œé›·æ›¼å…„å¼Ÿå…¬å¸çš„å€’é—­å’ŒæŸæž—å¢™çš„å€’å¡Œæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚'},\n",
       " 'cosine_sim': [100,\n",
       "  0.9999991059303284,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999990463256836,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999990463256836,\n",
       "  100,\n",
       "  0.9999989867210388,\n",
       "  0.9999985098838806,\n",
       "  0.9999988675117493,\n",
       "  0.9999985098838806,\n",
       "  100,\n",
       "  0.9999982714653015,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999990463256836,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999987483024597,\n",
       "  0.9999991655349731,\n",
       "  100,\n",
       "  100]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_data = cosine_sim_nostop(dataset, old_embed, new_embed, tokenizer)\n",
    "cosine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nothing', 'Brothers']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_token(cosine_data, tokenizer, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def wrap_nostop(num_ex, k=3):\n",
    "    output = []\n",
    "    for i in range(num_ex):\n",
    "        id = random.randint(0, 2000)\n",
    "        dataset = preprocess_dataset(id)\n",
    "        old_embed = get_old_embed(dataset)\n",
    "        model, tokenizer = retrain(dataset)\n",
    "        new_embed = get_new_embed(model, dataset)\n",
    "        cosine_data = cosine_sim_nostop(dataset, old_embed, new_embed, tokenizer)\n",
    "        \n",
    "        output.append({\"translation\": cosine_data[\"translation\"], \"top_tokens\": top_k_token(cosine_data, tokenizer, k=k)})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "result = wrap_nostop(100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation': {'en': 'Moreover, the CAPâ€™s role as a motor of political and social integration in Europe could be restored once renewed policies are in place.',\n",
       "   'zh': 'è€Œä¸”åªè¦æ›´æ–°åŽçš„æ”¿ç­–å°±ä½ï¼Œå…±åŒå†œä¸šæ”¿ç­–åœ¨æ¬§æ´²æ”¿æ²»ä¸Žç¤¾ä¼šæ•´åˆè¿‡ç¨‹ä¸­æ‰€æ‰®æ¼”çš„å‘åŠ¨æœºè§’è‰²å°±èƒ½æ¢å¤ã€‚'},\n",
       "  'top_tokens': ['Moreover', 'motor', 'renewed', 'place', 'social']},\n",
       " {'translation': {'en': 'In the end, the Addis Ababa Action Agenda provides that the current Committee of Experts will continue to function according to its 2004 mandate, with three additional meeting days per year, all funded through voluntary contributions.',\n",
       "   'zh': 'æœ€ç»ˆï¼Œäºšçš„æ–¯äºšè´å·´è¡ŒåŠ¨æ—¥ç¨‹ï¼ˆAddis Ababa Action Agendaï¼‰è§„å®šï¼Œç›®å‰çš„ä¸“å®¶å§”å‘˜ä¼šå°†ç»§ç»­æ ¹æ®å…¶2004å¹´çš„æƒé™è¡Œä½¿åŠŸèƒ½ï¼Œæ¯å¹´å¢žåŠ ä¸‰æ¬¡ä¼šè®®ï¼Œå®Œå…¨ä»¥è‡ªæ„¿ææ¬¾çš„æ–¹å¼ç­¹é›†èµ„é‡‘ã€‚'},\n",
       "  'top_tokens': ['function', 'three', '2004', 'continue', 'funded']},\n",
       " {'translation': {'en': 'Now, with household debt sustained on a knife-edge after feverish government intervention, the fiscal position has deteriorated dramatically and the current-account balance has worsened again.',\n",
       "   'zh': 'å¦‚ä»Šæ”¿åºœå¤§åŠ›å¹²é¢„åŽçš„å®¶åº­è´Ÿå€ºçŠ¶å†µä¾ç„¶å¤„äºŽå±é™©è¾¹ç¼˜ï¼ŒåŒæ—¶è´¢æ”¿çŠ¶å†µæ€¥å‰§æ¶åŒ–ï¼Œç»å¸¸è´¦æˆ·å¹³è¡¡çŠ¶å†µä¹Ÿå†åº¦å˜å·®ã€‚'},\n",
       "  'top_tokens': ['dramatically',\n",
       "   'worsened',\n",
       "   'deteriorated',\n",
       "   'intervention',\n",
       "   'fever']},\n",
       " {'translation': {'en': 'If donor governments really want results, they should take the money out of the hands of thirty or more separate aid bureaucracies and pool it in one or two places, the most logical being the World Bank in Washington and the International Fund for Agricultural Development (IFAD) in Rome.',\n",
       "   'zh': 'å¦‚æžœæåŠ©å›½æ”¿åºœçœŸçš„çœ‹é‡ç»“æžœï¼Œå°±åº”è¯¥ä»Ž30å®¶æˆ–è€…æ›´å¤šçš„ç‹¬ç«‹å®˜åƒšæ´åŠ©æœºæž„ä¸­æŠŠæ¬¾é¡¹æ”¶å›žï¼Œé›†ä¸­äº¤ç»™ä¸€ä¸¤å®¶æœºæž„ç®¡ç†ã€‚ æœ€é¡ºç†æˆç« çš„ç®¡ç†æœºæž„æ˜¯åŽç››é¡¿ä¸–ç•Œé“¶è¡Œå’Œç½—é©¬è”åˆå›½å›½é™…å†œä¸šå‘å±•åŸºé‡‘ã€‚'},\n",
       "  'top_tokens': ['want', 'logical', 'Agricultural', 'one', 'two']},\n",
       " {'translation': {'en': 'Developing nations are grasping just how outrageous the current distribution of greenhouse-gas emissions really is.',\n",
       "   'zh': 'å‘å±•ä¸­å›½å®¶éƒ½æ„è¯†åˆ°äº†å½“å‰æ¸©å®¤æ°”ä½“æŽ’æ”¾é‡åˆ†é…æ–¹æ¡ˆçš„ä¸åˆç†ä¹‹å¤„ã€‚'},\n",
       "  'top_tokens': ['outrageous',\n",
       "   'grasp',\n",
       "   'distribution',\n",
       "   'Developing',\n",
       "   'greenhouse']},\n",
       " {'translation': {'en': 'For the sake of all Japanese â€“ not to mention a world economy in need of a new source of dynamism â€“ that promise deserves to be met.',\n",
       "   'zh': 'ä¸ºäº†æ—¥æœ¬çš„æ•´ä½“åˆ©ç›Šâ€”â€”ä¸è¦è¯´ä¸€ä¸ªéœ€è¦æ–°åŠ¨åŠ›æ¥æºçš„ä¸–ç•Œç»æµŽâ€”â€”è¿™ä¸ªæ‰¿è¯ºå€¼å¾—åŽ»å…‘çŽ°ã€‚'},\n",
       "  'top_tokens': ['dynamism', 'deserves', 'mention', 'Japanese', 'economy']},\n",
       " {'translation': {'en': 'Tetapi, sama mendesaknya, lebih dari sepertiga anak usia sekolah dasar â€“ 250 juta anak â€“ tidak mendapatkan pembelajaran yang sifatnya mendasar, menurut temuan dalam UNESCO Education for All Global Monitoring Report.',\n",
       "   'zh': 'ä½†æ˜¯ï¼ŒåŒæ ·ç´§è¿«çš„æ˜¯ï¼Œæ®è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡å…¨æ°‘æ•™è‚²å…¨çƒæ£€æµ‹æŠ¥å‘Šï¼Œæœ‰è¶…è¿‡ä¸‰åˆ†ä¹‹ä¸€çš„å°å­¦é€‚é¾„å„¿ç«¥â€”â€”2. 5äº¿äººâ€”â€”æ²¡æœ‰å­¦åˆ°åŸºç¡€çŸ¥è¯†ã€‚'},\n",
       "  'top_tokens': ['Te', 'Report', 'Monitoring', 'sama', 'Education']},\n",
       " {'translation': {'en': 'Or do you want universities that regard the idea of a â€œsafe spaceâ€ â€“ in terms of closing down debate in case it offends someone â€“ as an oxymoron in an academic setting?',\n",
       "   'zh': 'ä½ å¿ƒç›®ä¸­çš„å¤§å­¦æ˜¯å¦å°†â€œå®‰å…¨ç©ºé—´â€ç†å¿µâ€”â€”å³åœ¨å†’çŠ¯ä»–äººçš„æƒ…å†µä¸‹åœæ­¢è®¨è®ºâ€”â€”è§†ä¸ºå­¦æœ¯çŽ¯å¢ƒä¸­çš„çŸ›ç›¾ä¿®é¥°æ³•ï¼Ÿ'},\n",
       "  'top_tokens': ['offend', 'closing', 'setting', 'oxy', 'want']},\n",
       " {'translation': {'en': 'Thus, scientists working in developing countries face a dilemma: either work on rich-world problems for which there is abundant data, or risk career advancement by conducting qualitative work that will not make it into A-level journals.',\n",
       "   'zh': 'å› æ­¤ï¼Œåœ¨å‘å±•ä¸­å›½å®¶å·¥ä½œçš„ç§‘å­¦å®¶ä»¬é¢ä¸´ç€å›šå¾’å›°å¢ƒï¼šæˆ–è€…ç ”ç©¶æœ‰ç€ä¸°å¯Œæ•°æ®çš„å¯Œè£•å›½å®¶é—®é¢˜ï¼Œæˆ–è€…ç”˜å†’èŒä¸šç”Ÿæ¶¯ä¹‹é™©è€Œè¿›è¡Œæ— ç¼˜è¿›å…¥Açº§æœŸåˆŠçš„å®šæ€§ç ”ç©¶ã€‚'},\n",
       "  'top_tokens': ['abundant', 'work', 'work', 'conducting', 'journals']},\n",
       " {'translation': {'en': 'The fact is that the challenges inherent in completing the TTIP are no more intractable than those that EU leaders have faced in the last few years of crisis.',\n",
       "   'zh': 'äº‹å®žä¸Šï¼Œå®ŒæˆTTIPæ‰€åŒ…å«çš„æŒ‘æˆ˜å¹¶ä¸æ¯”æ¬§ç›Ÿé¢†å¯¼äººåœ¨è¿‡åŽ»å‡ å¹´çš„å±æœºä¸­æ‰€é‡åˆ°çš„æŒ‘æˆ˜æ›´æ£˜æ‰‹ã€‚'},\n",
       "  'top_tokens': ['intractable',\n",
       "   'inherent',\n",
       "   'challenges',\n",
       "   'completing',\n",
       "   'fact']},\n",
       " {'translation': {'en': 'Global trade as a share of GDP may therefore decline, but without adverse consequences for global economic growth.',\n",
       "   'zh': 'å…¨çƒè´¸æ˜“ä½œä¸ºå…¨çƒGDPçš„ç»„æˆéƒ¨åˆ†ï¼Œå…¶æ¯”ä¾‹å¯èƒ½å› æ­¤ä¸‹é™ï¼Œä½†ä¸ä¼šå¯¹å…¨çƒç»æµŽå¢žé•¿äº§ç”Ÿä¸åˆ©å½±å“ã€‚'},\n",
       "  'top_tokens': ['adverse', 'consequences', 'share', 'may', 'therefore']},\n",
       " {'translation': {'en': 'Importantly, the report, â€œResearch and Development to Meet Health Needs in Developing Countries,â€ recommends a comprehensive approach, including mandatory funding contributions from governments for research on developing countriesâ€™ health needs; international coordination of health-care priorities and implementation; and a global observatory that would monitor where needs are greatest.',\n",
       "   'zh': 'æœ‰ä¸€ç‚¹å¾ˆé‡è¦ï¼Œè¿™ä»½é¢˜ä¸ºã€Šç”¨ç ”å‘æ»¡è¶³å‘å±•ä¸­å›½å®¶çš„å«ç”Ÿéœ€è¦ã€‹ï¼ˆâ€œResearch and Development to Meet Health Needs in Developing Countriesï¼‰ç»™å‡ºäº†ä¸€ä¸ªå®Œæ•´çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¥è‡ªæ”¿åºœçš„æŒ‡ä»¤æ€§å‘å±•ä¸­å›½å®¶å«ç”Ÿç ”ç©¶èµ„é‡‘åˆ†é…ï¼› ä¼˜å…ˆå«ç”Ÿé¡¹ç›®å’Œå®žæ–½çš„å›½é™…åˆä½œï¼›'},\n",
       "  'top_tokens': ['Importantly',\n",
       "   'mandatory',\n",
       "   'Meet',\n",
       "   'comprehensive',\n",
       "   'observatory']},\n",
       " {'translation': {'en': 'This intractability is, in some cases, understandable; the pain of survivors and their descendants remains acute.',\n",
       "   'zh': 'ä»ŽæŸç§ç¨‹åº¦ä¸Šï¼Œè¿™ä¸ªæ£˜æ‰‹é—®é¢˜ä¸éš¾ç†è§£ï¼› å¹¸å­˜è€…åŠå…¶åŽä»£çš„ç—›è‹¦ä¾ç„¶å‰§çƒˆã€‚'},\n",
       "  'top_tokens': ['understandable', 'tract', 'acute', 'cases', 'ability']},\n",
       " {'translation': {'en': 'Nor is inclusion of the renminbi in the SDR basket likely to provide as big a boost to the currencyâ€™s internationalization as many believe.',\n",
       "   'zh': 'åŠ å…¥SDRç¯®å­æ˜¯å¦èƒ½ç»™äººæ°‘å¸å›½é™…åŒ–å¸¦æ¥å·¨å¤§çš„ææŒ¯ä¹Ÿä¸åƒè®¸å¤šäººè®¤ä¸ºçš„é‚£æ ·ç¡®å®šã€‚'},\n",
       "  'top_tokens': ['renminbi', 'boost', 'inclusion', 'ization', 'currency']},\n",
       " {'translation': {'en': 'Thus, 9/11 has meant, directly or indirectly, a great shock, both psychologically and to our political systems.',\n",
       "   'zh': 'è¿™æ ·ï¼Œ9/11å·²ç»ç›´æŽ¥æˆ–é—´æŽ¥åœ°æ„å‘³ç€ä¸€ä¸ªå·¨å¤§çš„éœ‡æƒŠäº‹ä»¶ï¼Œåœ¨å¿ƒç†ä¸Šä»¥åŠå¯¹äºŽæ”¿æ²»ä½“ç³»è€Œè¨€éƒ½æ˜¯å¦‚æ­¤ã€‚'},\n",
       "  'top_tokens': ['Thus', '9/11', 'shock', 'systems', 'psychological']},\n",
       " {'translation': {'en': 'The WHOâ€™s efforts to encourage broad reforms at the international level are crucial.',\n",
       "   'zh': 'ä¸–ç•Œå«ç”Ÿç»„ç»‡æ—¨åœ¨ä¿ƒè¿›å›½é™…å±‚é¢çš„å¹¿æ³›æ”¹é©çš„åŠªåŠ›éžå¸¸å…³é”®ã€‚'},\n",
       "  'top_tokens': ['encourage', 'crucial', 'international', 'level', 'broad']},\n",
       " {'translation': {'en': 'Until now, Russia has cared less about a new PCA than the EU, because two-thirds of Russiaâ€™s exports to the Union comprise natural resources, which bring in cash even without the strong rules that a PCA provides.',\n",
       "   'zh': '\\xa0\\xa0\\xa0 ç›®å‰ä¸ºæ­¢ï¼Œä¿„å›½å¯¹åè®®çš„å…³å¿ƒå¹¶ä¸åƒæ¬§ç›Ÿé‚£æ ·å¼ºçƒˆï¼Œå› ä¸ºä¿„å›½å‘æ¬§ç›Ÿå‡ºå£çš„ä¸‰åˆ†ä¹‹äºŒæ˜¯è‡ªç„¶èµ„æºã€‚ å› æ­¤ï¼Œå³ä½¿æ²¡æœ‰åè®®è§„å®šçš„ç¡¬æ€§æ¡æ¬¾ä¹Ÿèƒ½å¤Ÿå¸¦æ¥çŽ°é‡‘æ”¶å…¥ã€‚'},\n",
       "  'top_tokens': ['PCA', 'PCA', 'Russia', 'Russia', 'comprise']},\n",
       " {'translation': {'en': 'When demand begins to exceed supply, demand-side stimulus policies will become increasingly ineffective, and it will be time to launch the third arrow of Abenomics: growth-enhancing structural reforms.',\n",
       "   'zh': 'å½“éœ€æ±‚å¼€å§‹è¶…è¿‡ä¾›ç»™çš„æ—¶å€™ï¼Œéœ€æ±‚ç«¯çš„åˆºæ¿€æ”¿ç­–å°†é€æ¸å˜å¾—æ— æ•ˆï¼Œæ˜¯æ—¶å€™å°„å‡ºå®‰å€ç»æµŽå­¦çš„ç¬¬ä¸‰æ”¯ç®­ï¼šå¼ºåŒ–å¢žé•¿çš„ç»“æž„æ€§æ”¹é©ã€‚'},\n",
       "  'top_tokens': ['enhancing',\n",
       "   'ineffective',\n",
       "   'arrow',\n",
       "   'launch',\n",
       "   'increasingly']},\n",
       " {'translation': {'en': 'Because rapid fiscal deterioration now has investors worrying about capital losses on US government securities, devaluation would make foreigners more hesitant to finance Americaâ€™s budget deficit.',\n",
       "   'zh': 'å› ä¸ºè´¢æ”¿çŠ¶å†µçš„è¿…é€Ÿæ¶åŒ–å·²ç»ä»¤æŠ•èµ„è€…ä»¬å¯¹è‡ªå·±ç¾Žå›½æ”¿åºœå€ºåˆ¸ä¸Šçš„èµ„æœ¬æŸå¤±å¿§è™‘ä¸å·²ï¼Œæ­¤å¤–è´¬å€¼ä¹Ÿå°†ä½¿å¤–å›½äººæ›´ä¸æ„¿æ„èµ„åŠ©ç¾Žå›½çš„é¢„ç®—èµ¤å­—ã€‚'},\n",
       "  'top_tokens': ['hesitant', 'worrying', 'devaluation', 'rapid', 'deficit']},\n",
       " {'translation': {'en': 'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic controversies that the entire project may well be scuttled.',\n",
       "   'zh': 'åœ¨TPPè°ˆåˆ¤ä¸æ–­æŽ¨è¿›çš„åŒæ—¶ï¼Œæ¬§ç›Ÿ-ç¾Žå›½è·¨å¤§è¥¿æ´‹è´¸æ˜“å’ŒæŠ•èµ„ä¼™ä¼´å…³ç³»ï¼ˆTransatlantic Trade and Investment Partnershipï¼ŒTTIPï¼‰è°ˆåˆ¤å´æ·±é™·å›½å†…çŸ›ç›¾çš„æŽ£è‚˜ï¼Œæ•´ä¸ªå·¥ç¨‹éƒ½æœ‰å¯èƒ½å¤­æŠ˜ã€‚'},\n",
       "  'top_tokens': ['controversies', 'problem', 'project', 'well', 'led']},\n",
       " {'translation': {'en': 'Such depictions are breathtaking in their audacity, given Japanâ€™s seven-decade record as a peaceful and constructive member of the international community.',\n",
       "   'zh': 'è¿™æ ·çš„æè¿°ä¸å¯ä¸è°“åŽšé¢œæ— è€»ï¼Œå› ä¸ºä¸ƒåå¹´æ¥æ—¥æœ¬ä¸€ç›´æ˜¯å›½é™…ç¤¾ä¼šå’Œå¹³è€Œå…·æœ‰å»ºè®¾æ€§çš„æˆå‘˜ã€‚'},\n",
       "  'top_tokens': ['decade', 'constructive', 'depict', 'seven', 'uda']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning â€œhello,â€ â€œthanks,â€ or â€œwell,â€ and sumimasen, which can carry any of the meanings of domo, as well as â€œsorryâ€ or â€œexcuse me.â€',\n",
       "   'zh': 'å¦‚ä»Šï¼Œæ‰€æœ‰æ¥æ—¥æ—…å®¢éƒ½ä¼šå­¦ä¼šä¸¤ä¸ªå…³é”®è¯ï¼šâ€œå¤šæœ«â€ï¼Œæ„æ€æ˜¯â€œä½ å¥½â€ã€â€œè°¢è°¢â€æˆ–â€œå¾ˆå¥½â€ï¼› ä»¥åŠâ€œæ–¯ç±³é©¬èµ›â€ï¼Œå®ƒåŒ…æ‹¬äº†domoçš„å…¨éƒ¨å«ä¹‰ï¼Œè¿˜å¯ä»¥è¡¨è¾¾â€œå¯¹ä¸èµ·â€å’Œâ€œåŠ³é©¾â€ã€‚'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': 'è‹±å›½å®‰å…¨éƒ¨é—¨éžå¸¸æ€€ç–‘ä»–ã€‚'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': 'åœ¨å›½äº‹æ´»åŠ¨ä¸­ï¼Œå¤–è¡¨åº”å½“æ˜¯é®äººè€³ç›®çš„ã€‚'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': 'è¿™äº›æ•™è®­å¯¹å‘è¾¾ç»æµŽä½“åŒæ ·æœ‰æ•ˆï¼Œå› ä¸ºå‘è¾¾ç»æµŽä½“ä¹Ÿåœ¨é­å—ä¸å¹³ç­‰æ€§æ¶åŒ–å’Œè…è´¥æ½œè§„åˆ™çš„é—®é¢˜ã€‚'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-Ã -vis European and American companies would diminish substantially.',\n",
       "   'zh': 'ä½†æ˜¯ï¼Œå¦‚æžœTTIPå°†ç¬¬ä¸‰å›½ä¼ä¸šæŽ’é™¤åœ¨äº’ç›¸æ‰¿è®¤æ”¿ç­–ä¹‹å¤–ï¼Œé‚£ä¹ˆå®ƒä»¬ç›¸å¯¹æ¬§æ´²å’Œç¾Žè‚¡å…¬å¸çš„ç«žäº‰åŠ›å°†å—åˆ°æžå¤§çš„å‰Šå¼±ã€‚'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': 'æ¯ä¸ªäººä¼¼ä¹Žéƒ½æ˜¯è¾“å®¶ï¼Œå³ä½¿æœ‰äº›å›½å®¶æ¯”å…¶å®ƒå›½å®¶å—åˆ°çš„å½±å“æ›´å¤§ã€‚'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': 'å°½ç®¡æ‹¥æœ‰è¶…å‡¡çš„ä¸ªäººé­…åŠ›ï¼Œä½†æœ´æ—¢ä¸åŒäºŽè¨æ‹‰Â·ä½©æž—ä¹Ÿä¸åŒäºŽä¼Šå¨ƒÂ·è£´éš†ã€‚'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happening', 'widen', 'analog', 'deepen', 'understand']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][\"top_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#pos = nltk.pos_tag(result[0][\"top_tokens\"])\n",
    "def count_pos(result_dict):\n",
    "    all_tokens = []\n",
    "    for sent in result_dict:\n",
    "        all_tokens += sent[\"top_tokens\"]\n",
    "    pos = nltk.pos_tag(all_tokens)\n",
    "    the_count = Counter(tag for _, tag in pos)\n",
    "    \n",
    "    labels, values = zip(*the_count.items())\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "\n",
    "    plt.title(\"Distribution of Tokens POS\")\n",
    "    plt.bar(indexes, values, width, color=(0.3, 0.4, 0.7, 0.6))\n",
    "    plt.xticks(indexes + width * 0.5, labels, rotation=90)\n",
    "    plt.show()\n",
    "    return the_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = count_pos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'RB': 50,\n",
       "         'NN': 122,\n",
       "         'VBD': 2,\n",
       "         'JJ': 79,\n",
       "         'CD': 6,\n",
       "         'VBN': 3,\n",
       "         'VBP': 15,\n",
       "         'NNP': 59,\n",
       "         'NNS': 30,\n",
       "         'VBZ': 13,\n",
       "         'IN': 1,\n",
       "         'VBG': 28,\n",
       "         'MD': 23,\n",
       "         'VB': 23,\n",
       "         'JJR': 1,\n",
       "         'DT': 12,\n",
       "         'FW': 33})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/users/as/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/users/as/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/users/as/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/users/as/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "#words = unmasker(\"Hello I'm a [MASK] boy.\")\n",
    "from nltk.corpus import wordnet as wn\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def find_tokenidx(token, sentence_lst):\n",
    "  for i in range(len(sentence_lst)):\n",
    "    if token in sentence_lst[i]:\n",
    "      return i\n",
    "def mutant_pairs(examples):\n",
    "  pairs=[]\n",
    "  inputs = [ex[\"translation\"][source_lang] for ex in examples]\n",
    "  for i, input in enumerate(inputs):\n",
    "    input_list = input.split()\n",
    "    tokens = examples[i]['top_tokens']\n",
    "    # sys = wn.synsets(input_list[tokens])\n",
    "    # idx = 0\n",
    "    # while 1:\n",
    "    #   if len(wn.synset(sys[idx].name()).lemmas()) >= 2:\n",
    "    #     break\n",
    "    #   idx += 1\n",
    "    # candidate = [str(lemma.name()) for lemma in wn.synset(sys[idx].name()).lemmas()]\n",
    "    print(tokens)\n",
    "    token_idx = find_tokenidx(tokens[0], input_list)\n",
    "    print(input_list[token_idx])\n",
    "    input_list[token_idx] = \"[MASK]\"\n",
    "    candidate = unmasker(\" \".join(input_list))\n",
    "    #print(candidate)\n",
    "    mutants = []\n",
    "    for c in candidate[:3]:\n",
    "      mutant = input_list.copy()\n",
    "      mutant[token_idx] = c[\"token_str\"]\n",
    "      mutants.append(\" \".join(mutant))\n",
    "    pairs.append({'en': input, 'mutants': mutants})\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929', '1989', 'or', '</s>']\n",
      "1929\n",
      "['happening', 'widen', 'analog', 'deepen', 'understand']\n",
      "happening.\n",
      "['classical', 'reassuring', 'refer', 'cyclical', 'many']\n",
      "classical\n",
      "['grim', 'mood', 'classical', 'behave', 'governments']\n",
      "grimmer,\n",
      "['diffusion', 'tendency', 'restraint', 'States', 'Europe']\n",
      "diffusion\n",
      "['moved', 'badly', 'avoiding', 'implement', 'fronts']\n",
      "moved\n",
      "['geo', 'naturally', 'comes', 'year', 'economic']\n",
      "geo-strategists,\n",
      "['nothing', 'Brothers', 'h', 'Berlin', 'man']\n",
      "nothing\n",
      "['reassuring', 'destruct', 'oppression', 'versus', 'perfect']\n",
      "reassuring\n",
      "['unfolding', 'correspond', '2008-2009,', 'felt', 'whose']\n",
      "unfolding\n",
      "['end', 'end', 'ideological', 'divide', 'absolute']\n",
      "end\n",
      "['jeopardize', '2009', '1989,', 'tendencies', 'principles']\n",
      "jeopardize\n",
      "['Blo', 'promoted', 'c', 'ideology', 'triumph']\n",
      "Bloc.\n",
      "['deliberate', 'brink', 'Ronald', 'pushed', 'supporters']\n",
      "deliberate\n",
      "['differences', 'obvious', '1989', 'course', 'Of']\n",
      "differences\n",
      "['collapse', 'bi', 'First', 'arity', '1989']\n",
      "collapse\n",
      "['substituting', '2009', 'pave', 'contrast', 'way']\n",
      "substituting\n",
      "['losers', 'Second', 'democracy', 'winners', 'winners']\n",
      "losers.\n",
      "['loser', 'Everyone', 'seems', 'even', 'others']\n",
      "loser,\n",
      "['Yet', 'unfair', 'greater', 'may', 'better']\n",
      "Yet,\n",
      "['shape', 'better', 'alone', 'In']\n",
      "shape,\n",
      "['preview', 'Harvard', 'getting', 'finally', 'passes']\n",
      "preview\n",
      "['One', 'dominated', 'universe', 'Asian', 'making']\n",
      "One\n",
      "['incredible', 'Indians', 'Athens', 'admiration', 'Harvard']\n",
      "incredible\n",
      "['spreading', 'outright', 'disorder', 'new', 'may']\n",
      "spreading\n",
      "['central', 'producing', 'crisis', 'Egyptian', 'Gulf']\n",
      "central\n",
      "['poorer', 'get', 'less', 'get', 'rich']\n",
      "poorer.\n",
      "['supposedly', 'explosions', 'foreign', 'xenophobia', 'potential']\n",
      "supposedly\n",
      "['enduring', 'would', 'less', 'many', 'consequences']\n",
      "enduring\n",
      "['reflex', '2009', 'dramatic', 'hope', 'historical']\n",
      "reflexes\n",
      "['2008', 'Failed', 'What', 'in']\n",
      "2008?\n",
      "['solve', 'know', 'problem', 'E', 'RK']\n",
      "solve\n",
      "['thought', 'willing', 'turns', 'implement', 'quite']\n",
      "thought.\n",
      "['regulated', 'result', 'easier', 'perceived', 'de']\n",
      "deregulated,\n",
      "['proliferate', 'wild', 'imagination', 'risk', 'result']\n",
      "proliferated\n",
      "['incorrect', 'tested', 'ultimately', 'created', 'Un']\n",
      "incorrect\n",
      "['underestimated', 'tail', 'Officials', 'risks']\n",
      "underestimated\n",
      "['maneuver', '2', 'set', 'chop', 'leaving']\n",
      "maneuver\n",
      "['currency', 'common', 'euro', 'uda', 'cious']\n",
      "currency.\n",
      "['headed', 'began', 'continued', 'long', 'Indeed']\n",
      "wrongheaded\n",
      "['worsening', 'Depression', 'Politi', 'address', 'Great']\n",
      "worsening\n",
      "['smart', 'ail', 'un', 'simple', 'countering']\n",
      "smart,\n",
      "['suggests', 'especially', 'short', 'issue', 'reserve']\n",
      "suggests\n",
      "['argues', '4', 'raise', 'even', 'targets']\n",
      "argues,\n",
      "['discourage', 'Wolf', 'lower', 'put', 'place']\n",
      "discourage\n",
      "['contradictions', 'eurozone', 'institutions', 'properly', 'must']\n",
      "contradictions,\n",
      "['tackling', 'craft', 'inequality', 'Wolf', 'countries']\n",
      "tackling\n",
      "['proposals', 'implement', 'Wolf', 'little', 'may']\n",
      "proposals\n",
      "['Mirror', 'found', 'patron', 'E', 'Barry']\n",
      "Mirrors,\n",
      "['E', 'Fried', 'Milton', 'Minsk', 'chen']\n",
      "Eichengreen\n",
      "['2008', 'Fried', 'erupted', 'tried', 'apply']\n",
      "2008\n",
      "['radically', 'incomplete', 'significant', 'turned', 'Depression']\n",
      "radically\n",
      "['prevent', 'depression', 'c', 'politicians', 'embrace']\n",
      "prevent\n",
      "['stagnant', 'become', 'threatens', 'normal', 'today']\n",
      "stagnant\n",
      "['exposed', 'strengthen', 'failure', 'major', 'another']\n",
      "exposed\n",
      "['E', 'Wolf', 'agree', 'underpin', 'shortcomings']\n",
      "Eichengreen\n",
      "['never', 'learned', 'truly', 'true', 'Indeed']\n",
      "never\n",
      "['Come', 'back', 'Strategy', 'Europe', 'A']\n",
      "Comeback\n",
      "['pleasant', 'vitality', 'lacking', 'grandmother', 'OCK']\n",
      "pleasant\n",
      "['forge', 'recognize', 'tackling', 'argued', 'high']\n",
      "forge\n",
      "['Admittedly', 'alarming', 'characterization', 'accurate', 'e']\n",
      "Admittedly,\n",
      "['strengths', 'significant', 'seem', 'ing', 'retain']\n",
      "strengths.\n",
      "['encompassing', 'impressive', 'built', 'billion', 'competitive']\n",
      "encompassing\n",
      "['influencing', 'changing', 'increasingly', 'world', 'economic']\n",
      "influencing\n",
      "['shift', 'mega', 'join', 'accelerate', 'eventually']\n",
      "shift\n",
      "['faces', 'hurdle', 'underestimated', 'shortage', 'augment']\n",
      "faces\n",
      "['enhancing', 'work', 'ties', 'beginning', 'secure']\n",
      "enhancing\n",
      "['controversies', 'problem', 'project', 'well', 'led']\n",
      "controversies\n",
      "['substantial', 'convinced', 'Business', 'perception', 'benefits']\n",
      "substantial\n",
      "['trivial', 'dominate', 'Yet', 'issues', 'chlorinat']\n",
      "trivial\n",
      "['TT', 'unleash', 'half', 'trade', 'wealth']\n",
      "TTIPâ€™s\n",
      "['greater', 'even', 'benefits', 'would', 'TT']\n",
      "greater.)\n",
      "['catastrophic', 'failure', 'compelling', 'benefits', 'agreement']\n",
      "catastrophic\n",
      "['implemented', 'ammunition', 'give', 'unlikely', 'withdrawal']\n",
      "implemented,\n",
      "['squander', 'perception', 'disengagement', 'EU', 'drive']\n",
      "squander\n",
      "['invariably', 'exert', 'Putin', 'would', 'Vladimir']\n",
      "invariably\n",
      "['regain', 'stark', 'collapse', 'tip', 'moves']\n",
      "regaining\n",
      "['seemed', 'TT', 'recognize', 'first', 'proposed']\n",
      "seemed\n",
      "['EU', 'doubted', 'Indeed', 'process', 'initially']\n",
      "EU\n",
      "['ambition', 'tank', 'gas', 'complete', 'negotiations']\n",
      "ambition\n",
      "['one', 'protracted', 'associated', 'endure', 'wanted']\n",
      "one\n",
      "['seemingly', 'fears', 'essentially', 'abandoned', 'confirming']\n",
      "seemingly\n",
      "['headway', 'presenting', 'negotiators', 'discourse', 'public']\n",
      "headway,\n",
      "['inaccurate', 'tract', 'talk', 'gaining', 'agreement']\n",
      "inaccurate\n",
      "['revive', 'conclude', 'successfully', 'commitment', '2015.']\n",
      "revive\n",
      "['simple', 'remaining', 'resolving', 'issues', 'say']\n",
      "simple.\n",
      "['entails', 'establishing', 'always', 'difficult', 'many']\n",
      "entails\n",
      "['intractable', 'inherent', 'challenges', 'completing', 'fact']\n",
      "intractable\n",
      "['push', 'completing', 'TT', 'genuine', 'must']\n",
      "push\n",
      "['improved', 'chances', 'might', 'US', 'good']\n",
      "improved\n",
      "['Barack', 'get', 'Obama', 'might', 'negotiating']\n",
      "Barack\n",
      "['picking', 'would', 'apart', 'negotiated', 'simply']\n",
      "picking\n",
      "['easily', 'year', 'could', 'agenda', 'starting']\n",
      "easily\n",
      "['leaders', 'waste', 'time', 'Europe', 'That']\n",
      "leaders\n",
      "['must', 'seize', 'strategic', 'economic', 'avert']\n",
      "must\n",
      "['Ended', 'E', 'Year', 'ch', 'po']\n",
      "Ended\n",
      "['shroud', '2016', 'outlook', 'uncertainty', '2017']\n",
      "shrouded\n",
      "['rising', 'Tension', 'East', 'States', 'movements']\n",
      "rising,\n",
      "['rapprochement', 'disagreement', 'Bashar', 'despite', 'fundamental']\n",
      "rapprochement,\n",
      "['Meanwhile', 'devastated', 'utterly', 'backed', 'war']\n",
      "Meanwhile,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'en': '1929 or 1989?',\n",
       "  'mutants': ['1988 or 1989?', '1989 or 1989?', '1987 or 1989?']},\n",
       " {'en': 'PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       "  'mutants': ['PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been .',\n",
       "   'PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been ;',\n",
       "   'PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been ?']},\n",
       " {'en': 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       "  'mutants': ['At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to the cyclical downturns.',\n",
       "   'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to recent cyclical downturns.',\n",
       "   'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to similar cyclical downturns.']},\n",
       " {'en': 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "  'mutants': ['Today, the mood is much lighter with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "   'Today, the mood is much darker with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "   'Today, the mood is much improved with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.']},\n",
       " {'en': 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       "  'mutants': ['The tendency is either excessive restraint (Europe) or a failure of the effort (the United States).',\n",
       "   'The tendency is either excessive restraint (Europe) or a lack of the effort (the United States).',\n",
       "   'The tendency is either excessive restraint (Europe) or a waste of the effort (the United States).']},\n",
       " {'en': 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "  'mutants': ['Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has intervened on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "   'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has acted on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "   'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has invested on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.']},\n",
       " {'en': 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "  'mutants': ['For me however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "   'For us however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "   'For him however, the year that naturally comes to mind, in both politics and economics, is 1989.']},\n",
       " {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'mutants': ['Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "   'Of course, the fall of the house of Lehman Brothers has something to do with the fall of the Berlin Wall.',\n",
       "   'Of course, the fall of the house of Lehman Brothers has everything to do with the fall of the Berlin Wall.']},\n",
       " {'en': 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       "  'mutants': ['Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and corrupt institution of financial capitalism.',\n",
       "   'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and unstable institution of financial capitalism.',\n",
       "   'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and fragile institution of financial capitalism.']},\n",
       " {'en': 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.',\n",
       "  'mutants': ['Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose immediate consequences will be felt for decades.',\n",
       "   'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose negative consequences will be felt for decades.',\n",
       "   'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose lasting consequences will be felt for decades.']},\n",
       " {'en': 'The end of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       "  'mutants': ['The end of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       "   'The beginning of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       "   'The introduction of the East-West ideological divide and the end of absolute faith in markets are historical turning points.']},\n",
       " {'en': 'And what happens in 2009 may jeopardize some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       "  'mutants': ['And what happens in 2009 may reflect some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       "   'And what happens in 2009 may explain some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       "   'And what happens in 2009 may be some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.']},\n",
       " {'en': 'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet Bloc.',\n",
       "  'mutants': ['In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet .',\n",
       "   'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet ;',\n",
       "   'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet !']},\n",
       " {'en': 'For many of his supporters, it was President Ronald Reagan who, with his deliberate escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       "  'mutants': ['For many of his supporters, it was President Ronald Reagan who, with his dramatic escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       "   'For many of his supporters, it was President Ronald Reagan who, with his rapid escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       "   'For many of his supporters, it was President Ronald Reagan who, with his subsequent escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.']},\n",
       " {'en': 'Of course, there are obvious differences between 1989 and now.',\n",
       "  'mutants': ['Of course, there are obvious differences between 1989 and now.',\n",
       "   'Of course, there are obvious gaps between 1989 and now.',\n",
       "   'Of course, there are obvious changes between 1989 and now.']},\n",
       " {'en': 'First, and perhaps above all, the revolutions of 1989 and the subsequent collapse of the Soviet Union put an end to global bipolarity.',\n",
       "  'mutants': ['First, and perhaps above all, the revolutions of 1989 and the subsequent fall of the Soviet Union put an end to global bipolarity.',\n",
       "   'First, and perhaps above all, the revolutions of 1989 and the subsequent collapse of the Soviet Union put an end to global bipolarity.',\n",
       "   'First, and perhaps above all, the revolutions of 1989 and the subsequent dissolution of the Soviet Union put an end to global bipolarity.']},\n",
       " {'en': 'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China substituting for the Soviet Union.',\n",
       "  'mutants': ['By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China falling for the Soviet Union.',\n",
       "   'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China voting for the Soviet Union.',\n",
       "   'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China leaving for the Soviet Union.']},\n",
       " {'en': 'Second, whereas democracy and market capitalism appeared as clear â€“ if more fragile than expected â€“ winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from losers.',\n",
       "  'mutants': ['Second, whereas democracy and market capitalism appeared as clear â€“ if more fragile than expected â€“ winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from .',\n",
       "   'Second, whereas democracy and market capitalism appeared as clear â€“ if more fragile than expected â€“ winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from ;',\n",
       "   'Second, whereas democracy and market capitalism appeared as clear â€“ if more fragile than expected â€“ winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from ?']},\n",
       " {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "  'mutants': ['Everyone seems to be a mess even if some are more affected than others.',\n",
       "   'Everyone seems to be a failure even if some are more affected than others.',\n",
       "   'Everyone seems to be a wreck even if some are more affected than others.']},\n",
       " {'en': 'Yet, history is unfair, and the US, despite its greater responsibility for todayâ€™s global crisis, may emerge in better shape than most countries from the morass.',\n",
       "  'mutants': ['this history is unfair, and the US, despite its greater responsibility for todayâ€™s global crisis, may emerge in better shape than most countries from the morass.',\n",
       "   'the history is unfair, and the US, despite its greater responsibility for todayâ€™s global crisis, may emerge in better shape than most countries from the morass.',\n",
       "   'its history is unfair, and the US, despite its greater responsibility for todayâ€™s global crisis, may emerge in better shape than most countries from the morass.']},\n",
       " {'en': 'In better shape, but not alone.',\n",
       "  'mutants': ['In better shape but not alone.',\n",
       "   'In better health but not alone.',\n",
       "   'In better form but not alone.']},\n",
       " {'en': 'As a visiting professor at Harvard and MIT, I am getting a good preview of what the world could look like when the crisis finally passes.',\n",
       "  'mutants': ['As a visiting professor at Harvard and MIT, I am getting a good idea of what the world could look like when the crisis finally passes.',\n",
       "   'As a visiting professor at Harvard and MIT, I am getting a good view of what the world could look like when the crisis finally passes.',\n",
       "   'As a visiting professor at Harvard and MIT, I am getting a good understanding of what the world could look like when the crisis finally passes.']},\n",
       " {'en': 'One senses something like the making of an American-Asian dominated universe.',\n",
       "  'mutants': ['he senses something like the making of an American-Asian dominated universe.',\n",
       "   'she senses something like the making of an American-Asian dominated universe.',\n",
       "   'it senses something like the making of an American-Asian dominated universe.']},\n",
       " {'en': 'From the incredible media lab at MIT to the mathematics and economics departments at Harvard, Asians â€“ Chinese and Indians, in particular â€“ are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       "  'mutants': ['From the mit media lab at MIT to the mathematics and economics departments at Harvard, Asians â€“ Chinese and Indians, in particular â€“ are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       "   'From the new media lab at MIT to the mathematics and economics departments at Harvard, Asians â€“ Chinese and Indians, in particular â€“ are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       "   'From the digital media lab at MIT to the mathematics and economics departments at Harvard, Asians â€“ Chinese and Indians, in particular â€“ are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.']},\n",
       " {'en': 'But before this new order appears, the world may be faced with spreading disorder, if not outright chaos.',\n",
       "  'mutants': ['But before this new order appears, the world may be faced with complete disorder, if not outright chaos.',\n",
       "   'But before this new order appears, the world may be faced with total disorder, if not outright chaos.',\n",
       "   'But before this new order appears, the world may be faced with utter disorder, if not outright chaos.']},\n",
       " {'en': 'What, for example, will happen to a country as central and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       "  'mutants': ['What, for example, will happen to a country as poor and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       "   'What, for example, will happen to a country as isolated and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       "   'What, for example, will happen to a country as impoverished and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?']},\n",
       " {'en': 'When the rich get less rich, the poor get poorer.',\n",
       "  'mutants': ['When the rich get less rich, the poor get .',\n",
       "   'When the rich get less rich, the poor get ;',\n",
       "   'When the rich get less rich, the poor get !']},\n",
       " {'en': 'And what about the foreign workers who have reached for the â€œEuropean dreamâ€ and are now faced with potential explosions of xenophobia in Europeâ€™s supposedly open countries?',\n",
       "  'mutants': ['And what about the foreign workers who have reached for the â€œEuropean dreamâ€ and are now faced with potential explosions of xenophobia in Europeâ€™s most open countries?',\n",
       "   'And what about the foreign workers who have reached for the â€œEuropean dreamâ€ and are now faced with potential explosions of xenophobia in Europeâ€™s more open countries?',\n",
       "   'And what about the foreign workers who have reached for the â€œEuropean dreamâ€ and are now faced with potential explosions of xenophobia in Europeâ€™s least open countries?']},\n",
       " {'en': 'The consequences of 1989 ended up being less enduring than many observers, including me, would have assumed.',\n",
       "  'mutants': ['The consequences of 1989 ended up being less severe than many observers, including me, would have assumed.',\n",
       "   'The consequences of 1989 ended up being less serious than many observers, including me, would have assumed.',\n",
       "   'The consequences of 1989 ended up being less significant than many observers, including me, would have assumed.']},\n",
       " {'en': 'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now â€“ intuitively and in our historical reflexes â€“ feel them to be.',\n",
       "  'mutants': ['We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now â€“ intuitively and in our historical context â€“ feel them to be.',\n",
       "   'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now â€“ intuitively and in our historical experience â€“ feel them to be.',\n",
       "   'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now â€“ intuitively and in our historical perspective â€“ feel them to be.']},\n",
       " {'en': 'What Failed in 2008?',\n",
       "  'mutants': ['What Failed in ?', 'What Failed in .', 'What Failed in ;']},\n",
       " {'en': 'BERKELEY â€“ To solve a problem, it is not enough to know what to do.',\n",
       "  'mutants': ['BERKELEY â€“ To be a problem, it is not enough to know what to do.',\n",
       "   'BERKELEY â€“ To solve a problem, it is not enough to know what to do.',\n",
       "   'BERKELEY â€“ To have a problem, it is not enough to know what to do.']},\n",
       " {'en': 'You actually have to implement the solution â€“ and be willing to change course if it turns out that you did not know quite as much as you thought.',\n",
       "  'mutants': ['You actually have to implement the solution â€“ and be willing to change course if it turns out that you did not know quite as much as you .',\n",
       "   'You actually have to implement the solution â€“ and be willing to change course if it turns out that you did not know quite as much as you ;',\n",
       "   'You actually have to implement the solution â€“ and be willing to change course if it turns out that you did not know quite as much as you !']},\n",
       " {'en': 'As a result, markets were deregulated, making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       "  'mutants': ['As a result, markets were increasingly making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       "   'As a result, markets were changing making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       "   'As a result, markets were growing making it easier to trade assets that were perceived to be safe, but were in fact not.']},\n",
       " {'en': 'As a result, systemic risk proliferated beyond central bankersâ€™ wildest imagination.',\n",
       "  'mutants': ['As a result, systemic risk is beyond central bankersâ€™ wildest imagination.',\n",
       "   'As a result, systemic risk goes beyond central bankersâ€™ wildest imagination.',\n",
       "   'As a result, systemic risk extends beyond central bankersâ€™ wildest imagination.']},\n",
       " {'en': 'Untested â€“ and ultimately incorrect â€“ assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       "  'mutants': ['Untested â€“ and ultimately incorrect â€“ assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       "   'Untested â€“ and ultimately false â€“ assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       "   'Untested â€“ and ultimately inaccurate â€“ assumptions created a policymaking environment defined by what can only be called hubris.']},\n",
       " {'en': 'Officials underestimated tail risks.',\n",
       "  'mutants': ['Officials report tail risks.',\n",
       "   'Officials reported tail risks.',\n",
       "   'Officials avoid tail risks.']},\n",
       " {'en': 'They set inflation targets at around 2% â€“ leaving little room for maneuver when the water got choppy.',\n",
       "  'mutants': ['They set inflation targets at around 2% â€“ leaving little room for inflation when the water got choppy.',\n",
       "   'They set inflation targets at around 2% â€“ leaving little room for improvement when the water got choppy.',\n",
       "   'They set inflation targets at around 2% â€“ leaving little room for change when the water got choppy.']},\n",
       " {'en': 'And, most audaciously of all, the European Union introduced the euro as a common currency.',\n",
       "  'mutants': ['And, most audaciously of all, the European Union introduced the euro as a common .',\n",
       "   'And, most audaciously of all, the European Union introduced the euro as a common ;',\n",
       "   'And, most audaciously of all, the European Union introduced the euro as a common !']},\n",
       " {'en': 'Indeed, wrongheaded policymaking continued long after the crisis began.',\n",
       "  'mutants': ['Indeed, foreign policymaking continued long after the crisis began.',\n",
       "   'Indeed, soviet policymaking continued long after the crisis began.',\n",
       "   'Indeed, american policymaking continued long after the crisis began.']},\n",
       " {'en': 'Politicians responded to worsening economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       "  'mutants': ['Politicians responded to the economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       "   'Politicians responded to poor economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       "   'Politicians responded to difficult economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.']},\n",
       " {'en': 'Wolfâ€™s prescription for countering the crisis is simple, smart, and unassailable.',\n",
       "  'mutants': ['Wolfâ€™s prescription for countering the crisis is simple, effective and unassailable.',\n",
       "   'Wolfâ€™s prescription for countering the crisis is simple, straightforward and unassailable.',\n",
       "   'Wolfâ€™s prescription for countering the crisis is simple, simple and unassailable.']},\n",
       " {'en': 'In the short term, he suggests that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       "  'mutants': ['In the short term, he argued that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       "   'In the short term, he suggested that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       "   'In the short term, he proposed that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.']},\n",
       " {'en': 'Their central banks, he argues, should raise inflation targets to 3% or even 4% per year.',\n",
       "  'mutants': ['Their central banks, he said should raise inflation targets to 3% or even 4% per year.',\n",
       "   'Their central banks, he argued should raise inflation targets to 3% or even 4% per year.',\n",
       "   'Their central banks, he suggested should raise inflation targets to 3% or even 4% per year.']},\n",
       " {'en': 'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and discourage overleveraging.',\n",
       "  'mutants': ['Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and prevent overleveraging.',\n",
       "   'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and reduce overleveraging.',\n",
       "   'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and avoid overleveraging.']},\n",
       " {'en': 'The eurozone, too, must resolve its internal contradictions, either by disbanding or by introducing â€œa minimum set of institutions and policiesâ€ that allow the monetary union to function properly.',\n",
       "  'mutants': ['The eurozone, too, must resolve its internal problems either by disbanding or by introducing â€œa minimum set of institutions and policiesâ€ that allow the monetary union to function properly.',\n",
       "   'The eurozone, too, must resolve its internal conflicts either by disbanding or by introducing â€œa minimum set of institutions and policiesâ€ that allow the monetary union to function properly.',\n",
       "   'The eurozone, too, must resolve its internal issues either by disbanding or by introducing â€œa minimum set of institutions and policiesâ€ that allow the monetary union to function properly.']},\n",
       " {'en': 'Wolfâ€™s long-term solutions include tackling inequality, â€œmore global regulation,â€ a greater degree of â€œfreedom for individual countries to craft their own responses,â€ and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       "  'mutants': ['Wolfâ€™s long-term solutions include greater inequality, â€œmore global regulation,â€ a greater degree of â€œfreedom for individual countries to craft their own responses,â€ and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       "   'Wolfâ€™s long-term solutions include lower inequality, â€œmore global regulation,â€ a greater degree of â€œfreedom for individual countries to craft their own responses,â€ and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       "   'Wolfâ€™s long-term solutions include increasing inequality, â€œmore global regulation,â€ a greater degree of â€œfreedom for individual countries to craft their own responses,â€ and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.']},\n",
       " {'en': 'And yet, as recommendable as Wolfâ€™s proposals may be, little has been done to implement them.',\n",
       "  'mutants': ['And yet, as recommendable as Wolfâ€™s rules may be, little has been done to implement them.',\n",
       "   'And yet, as recommendable as Wolfâ€™s laws may be, little has been done to implement them.',\n",
       "   'And yet, as recommendable as Wolfâ€™s suggestions may be, little has been done to implement them.']},\n",
       " {'en': 'The reasons why are found in the second book: Hall of Mirrors, by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       "  'mutants': ['The reasons why are found in the second book: Hall of fame by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       "   'The reasons why are found in the second book: Hall of mirrors by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       "   'The reasons why are found in the second book: Hall of honor by&nbsp;my friend, teacher, and patron, Barry Eichengreen.']},\n",
       " {'en': 'Eichengreen traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers â€“ at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       "  'mutants': ['it traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers â€“ at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       "   'he traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers â€“ at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       "   'she traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers â€“ at least when it comes to interpretations of the causes and consequences of the Great Depression.']},\n",
       " {'en': 'When the 2008 financial crisis erupted, policymakers tried to apply Friedmanâ€™s proposed solutions to the Great Depression.',\n",
       "  'mutants': ['When the 1929 financial crisis erupted, policymakers tried to apply Friedmanâ€™s proposed solutions to the Great Depression.',\n",
       "   'When the great financial crisis erupted, policymakers tried to apply Friedmanâ€™s proposed solutions to the Great Depression.',\n",
       "   'When the world financial crisis erupted, policymakers tried to apply Friedmanâ€™s proposed solutions to the Great Depression.']},\n",
       " {'en': 'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and radically incomplete.',\n",
       "  'mutants': ['Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and ultimately incomplete.',\n",
       "   'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and therefore incomplete.',\n",
       "   'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and thus incomplete.']},\n",
       " {'en': 'The resulting policies were enough to prevent the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       "  'mutants': ['The resulting policies were enough to prevent the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       "   'The resulting policies were enough to stop the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       "   'The resulting policies were enough to keep the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.']},\n",
       " {'en': 'The result is todayâ€™s stagnant economy, marked by anemic growth that threatens to become the new normal.',\n",
       "  'mutants': ['The result is todayâ€™s mixed economy, marked by anemic growth that threatens to become the new normal.',\n",
       "   'The result is todayâ€™s global economy, marked by anemic growth that threatens to become the new normal.',\n",
       "   'The result is todayâ€™s market economy, marked by anemic growth that threatens to become the new normal.']},\n",
       " {'en': 'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy exposed to the risk of another major crisis.',\n",
       "  'mutants': ['The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy exposed to the risk of another major crisis.',\n",
       "   'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy vulnerable to the risk of another major crisis.',\n",
       "   'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy open to the risk of another major crisis.']},\n",
       " {'en': 'Wolf and Eichengreen would agree that the main shortcomings that led to the 2008 financial crisis â€“ and that continue to underpin our inadequate response to it â€“ are intellectual.',\n",
       "  'mutants': ['Wolf and colleagues would agree that the main shortcomings that led to the 2008 financial crisis â€“ and that continue to underpin our inadequate response to it â€“ are intellectual.',\n",
       "   'Wolf and others would agree that the main shortcomings that led to the 2008 financial crisis â€“ and that continue to underpin our inadequate response to it â€“ are intellectual.',\n",
       "   'Wolf and wilson would agree that the main shortcomings that led to the 2008 financial crisis â€“ and that continue to underpin our inadequate response to it â€“ are intellectual.']},\n",
       " {'en': 'Indeed, the only true lesson of the crisis so far seems to be that its lessons will never truly be learned.',\n",
       "  'mutants': ['Indeed, the only true lesson of the crisis so far seems to be that its lessons will never truly be learned.',\n",
       "   'Indeed, the only true lesson of the crisis so far seems to be that its lessons will not truly be learned.',\n",
       "   'Indeed, the only true lesson of the crisis so far seems to be that its lessons will ever truly be learned.']},\n",
       " {'en': 'A Comeback Strategy for Europe',\n",
       "  'mutants': ['A national Strategy for Europe',\n",
       "   'A new Strategy for Europe',\n",
       "   'A strategic Strategy for Europe']},\n",
       " {'en': 'STOCKHOLM/MADRID â€“ When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother â€“ pleasant and rich with experience, but lacking the vitality and energy of the past.',\n",
       "  'mutants': ['STOCKHOLM/MADRID â€“ When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother â€“ young and rich with experience, but lacking the vitality and energy of the past.',\n",
       "   'STOCKHOLM/MADRID â€“ When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother â€“ old and rich with experience, but lacking the vitality and energy of the past.',\n",
       "   'STOCKHOLM/MADRID â€“ When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother â€“ strong and rich with experience, but lacking the vitality and energy of the past.']},\n",
       " {'en': 'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and forge a clear policy for tackling them.',\n",
       "  'mutants': ['It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and develop a clear policy for tackling them.',\n",
       "   'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and establish a clear policy for tackling them.',\n",
       "   'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and set a clear policy for tackling them.']},\n",
       " {'en': 'Admittedly, the popeâ€™s characterization was alarmingly accurate in some respects.',\n",
       "  'mutants': ['but the popeâ€™s characterization was alarmingly accurate in some respects.',\n",
       "   'yet the popeâ€™s characterization was alarmingly accurate in some respects.',\n",
       "   'and the popeâ€™s characterization was alarmingly accurate in some respects.']},\n",
       " {'en': 'But, despite its seeming lassitude, Europe retains significant strengths.',\n",
       "  'mutants': ['But, despite its seeming lassitude, Europe retains significant .',\n",
       "   'But, despite its seeming lassitude, Europe retains significant ;',\n",
       "   'But, despite its seeming lassitude, Europe retains significant !']},\n",
       " {'en': 'It is a hub of high-level thought and innovation; it is home to some of the worldâ€™s most competitive regions and industries; and, perhaps most impressive, it has built a community and market encompassing a half-billion people.',\n",
       "  'mutants': ['It is a hub of high-level thought and innovation; it is home to some of the worldâ€™s most competitive regions and industries; and, perhaps most impressive, it has built a community and market for a half-billion people.',\n",
       "   'It is a hub of high-level thought and innovation; it is home to some of the worldâ€™s most competitive regions and industries; and, perhaps most impressive, it has built a community and market of a half-billion people.',\n",
       "   'It is a hub of high-level thought and innovation; it is home to some of the worldâ€™s most competitive regions and industries; and, perhaps most impressive, it has built a community and market with a half-billion people.']},\n",
       " {'en': 'But the world is changing: the Asia-Pacific region is increasingly influencing global developments, economic and otherwise.',\n",
       "  'mutants': ['But the world is changing: the Asia-Pacific region is increasingly experiencing global developments, economic and otherwise.',\n",
       "   'But the world is changing: the Asia-Pacific region is increasingly undergoing global developments, economic and otherwise.',\n",
       "   'But the world is changing: the Asia-Pacific region is increasingly seeing global developments, economic and otherwise.']},\n",
       " {'en': 'The Trans-Pacific Partnership â€“ by which the United States and 11 other countries would create a mega-regional free-trade zone â€“ would most likely accelerate this shift (all the more so if China eventually joins).',\n",
       "  'mutants': ['The Trans-Pacific Partnership â€“ by which the United States and 11 other countries would create a mega-regional free-trade zone â€“ would most likely accelerate this process (all the more so if China eventually joins).',\n",
       "   'The Trans-Pacific Partnership â€“ by which the United States and 11 other countries would create a mega-regional free-trade zone â€“ would most likely accelerate this development (all the more so if China eventually joins).',\n",
       "   'The Trans-Pacific Partnership â€“ by which the United States and 11 other countries would create a mega-regional free-trade zone â€“ would most likely accelerate this growth (all the more so if China eventually joins).']},\n",
       " {'en': 'Though the TPP faces no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asiaâ€™s economic power cannot be underestimated.',\n",
       "  'mutants': ['Though the TPP has no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asiaâ€™s economic power cannot be underestimated.',\n",
       "   'Though the TPP faces no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asiaâ€™s economic power cannot be underestimated.',\n",
       "   'Though the TPP poses no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asiaâ€™s economic power cannot be underestimated.']},\n",
       " {'en': 'Europe must work to secure its position in the new world order â€“ beginning by enhancing its own trade and investment ties with the US.',\n",
       "  'mutants': ['Europe must work to secure its position in the new world order â€“ beginning by establishing its own trade and investment ties with the US.',\n",
       "   'Europe must work to secure its position in the new world order â€“ beginning by building its own trade and investment ties with the US.',\n",
       "   'Europe must work to secure its position in the new world order â€“ beginning by strengthening its own trade and investment ties with the US.']},\n",
       " {'en': 'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic controversies that the entire project may well be scuttled.',\n",
       "  'mutants': ['The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic politics that the entire project may well be scuttled.',\n",
       "   'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic problems that the entire project may well be scuttled.',\n",
       "   'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic issues that the entire project may well be scuttled.']},\n",
       " {'en': 'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring substantial economic benefits â€“ a perception that many studies reinforce.',\n",
       "  'mutants': ['Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring significant economic benefits â€“ a perception that many studies reinforce.',\n",
       "   'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring substantial economic benefits â€“ a perception that many studies reinforce.',\n",
       "   'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring many economic benefits â€“ a perception that many studies reinforce.']},\n",
       " {'en': 'Yet trivial issues â€“ for example, the use of chlorinated chicken and settlements of investor disputes â€“ continue to dominate the debate.',\n",
       "  'mutants': ['Yet other issues â€“ for example, the use of chlorinated chicken and settlements of investor disputes â€“ continue to dominate the debate.',\n",
       "   'Yet environmental issues â€“ for example, the use of chlorinated chicken and settlements of investor disputes â€“ continue to dominate the debate.',\n",
       "   'Yet legal issues â€“ for example, the use of chlorinated chicken and settlements of investor disputes â€“ continue to dominate the debate.']},\n",
       " {'en': 'The TTIPâ€™s goal is to unleash the power of the transatlantic economy, which remains by far the worldâ€™s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       "  'mutants': ['The ultimate goal is to unleash the power of the transatlantic economy, which remains by far the worldâ€™s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       "   'The main goal is to unleash the power of the transatlantic economy, which remains by far the worldâ€™s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       "   'The stated goal is to unleash the power of the transatlantic economy, which remains by far the worldâ€™s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.']},\n",
       " {'en': '(If the TTIP was opened to other economies â€“ such as Turkey, Mexico, and Canada â€“ the benefits would be even greater.)',\n",
       "  'mutants': ['(If the TTIP was opened to other economies â€“ such as Turkey, Mexico, and Canada â€“ the benefits would be even .',\n",
       "   '(If the TTIP was opened to other economies â€“ such as Turkey, Mexico, and Canada â€“ the benefits would be even ;',\n",
       "   '(If the TTIP was opened to other economies â€“ such as Turkey, Mexico, and Canada â€“ the benefits would be even )']},\n",
       " {'en': 'Even more compelling than the benefits of achieving an agreement, though, are the potentially catastrophic consequences of failure.',\n",
       "  'mutants': ['Even more compelling than the benefits of achieving an agreement, though, are the potentially devastating consequences of failure.',\n",
       "   'Even more compelling than the benefits of achieving an agreement, though, are the potentially disastrous consequences of failure.',\n",
       "   'Even more compelling than the benefits of achieving an agreement, though, are the potentially fatal consequences of failure.']},\n",
       " {'en': 'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were implemented, the UK would be unwise â€“ and thus unlikely â€“ to leave.',\n",
       "  'mutants': ['For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were dissolved the UK would be unwise â€“ and thus unlikely â€“ to leave.',\n",
       "   'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were successful the UK would be unwise â€“ and thus unlikely â€“ to leave.',\n",
       "   'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were established the UK would be unwise â€“ and thus unlikely â€“ to leave.']},\n",
       " {'en': 'Moreover, the perception that the EUâ€™s internal squabbles had led it to squander a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       "  'mutants': ['Moreover, the perception that the EUâ€™s internal squabbles had led it to seize a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       "   'Moreover, the perception that the EUâ€™s internal squabbles had led it to seek a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       "   'Moreover, the perception that the EUâ€™s internal squabbles had led it to see a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.']},\n",
       " {'en': 'And Russian President Vladimir Putin would invariably regard the EUâ€™s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       "  'mutants': ['And Russian President Vladimir Putin would later regard the EUâ€™s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       "   'And Russian President Vladimir Putin would also regard the EUâ€™s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       "   'And Russian President Vladimir Putin would likely regard the EUâ€™s failure as a major opportunity to exert more influence over parts of Europe.']},\n",
       " {'en': 'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asiaâ€™s favor â€“ and Europe will have few options, if any, for regaining its economic and geopolitical influence.',\n",
       "  'mutants': ['All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asiaâ€™s favor â€“ and Europe will have few options, if any, for increasing its economic and geopolitical influence.',\n",
       "   'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asiaâ€™s favor â€“ and Europe will have few options, if any, for reducing its economic and geopolitical influence.',\n",
       "   'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asiaâ€™s favor â€“ and Europe will have few options, if any, for extending its economic and geopolitical influence.']},\n",
       " {'en': 'When the TTIP was first proposed, Europe seemed to recognize its value.',\n",
       "  'mutants': ['When the TTIP was first proposed, Europe failed to recognize its value.',\n",
       "   'When the TTIP was first proposed, Europe refused to recognize its value.',\n",
       "   'When the TTIP was first proposed, Europe began to recognize its value.']},\n",
       " {'en': 'Indeed, it was the EU that pushed the US, which initially doubted Europeâ€™s commitment, to launch the negotiation process in June 2013.',\n",
       "  'mutants': ['Indeed, it was the negotiations that pushed the US, which initially doubted Europeâ€™s commitment, to launch the negotiation process in June 2013.',\n",
       "   'Indeed, it was the us that pushed the US, which initially doubted Europeâ€™s commitment, to launch the negotiation process in June 2013.',\n",
       "   'Indeed, it was the vote that pushed the US, which initially doubted Europeâ€™s commitment, to launch the negotiation process in June 2013.']},\n",
       " {'en': 'The ambition was to complete the negotiations on â€œone tank of gas.â€',\n",
       "  'mutants': ['The goal was to complete the negotiations on â€œone tank of gas.â€',\n",
       "   'The plan was to complete the negotiations on â€œone tank of gas.â€',\n",
       "   'The aim was to complete the negotiations on â€œone tank of gas.â€']},\n",
       " {'en': 'No one wanted to endure protracted talks â€“ or the associated political pain.',\n",
       "  'mutants': ['No one wanted to endure protracted talks â€“ or the associated political pain.',\n",
       "   'No ones wanted to endure protracted talks â€“ or the associated political pain.',\n",
       "   'No man wanted to endure protracted talks â€“ or the associated political pain.']},\n",
       " {'en': 'But EU leaders essentially abandoned the project, seemingly confirming American fears.',\n",
       "  'mutants': ['But EU leaders essentially abandoned the project, further confirming American fears.',\n",
       "   'But EU leaders essentially abandoned the project, thus confirming American fears.',\n",
       "   'But EU leaders essentially abandoned the project, thereby confirming American fears.']},\n",
       " {'en': 'Trade negotiators struggled to make headway, while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europeâ€™s democracy to its health.',\n",
       "  'mutants': ['Trade negotiators struggled to make concessions while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europeâ€™s democracy to its health.',\n",
       "   'Trade negotiators struggled to make money while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europeâ€™s democracy to its health.',\n",
       "   'Trade negotiators struggled to make gains while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europeâ€™s democracy to its health.']},\n",
       " {'en': 'This is dangerously inaccurate talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       "  'mutants': ['This is dangerously small talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       "   'This is dangerously slow talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       "   'This is dangerously dangerous talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.']},\n",
       " {'en': 'And they must revive their commitment to conclude the talks successfully in 2015.',\n",
       "  'mutants': ['And they must show their commitment to conclude the talks successfully in 2015.',\n",
       "   'And they must maintain their commitment to conclude the talks successfully in 2015.',\n",
       "   'And they must demonstrate their commitment to conclude the talks successfully in 2015.']},\n",
       " {'en': 'This is not to say that resolving the remaining issues in the TTIP negotiations will be simple.',\n",
       "  'mutants': ['This is not to say that resolving the remaining issues in the TTIP negotiations will be .',\n",
       "   'This is not to say that resolving the remaining issues in the TTIP negotiations will be ;',\n",
       "   'This is not to say that resolving the remaining issues in the TTIP negotiations will be ?']},\n",
       " {'en': 'But establishing a trade agreement, especially one that entails so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       "  'mutants': ['But establishing a trade agreement, especially one that addresses so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       "   'But establishing a trade agreement, especially one that has so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       "   'But establishing a trade agreement, especially one that covers so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.']},\n",
       " {'en': 'The fact is that the challenges inherent in completing the TTIP are no more intractable than those that EU leaders have faced in the last few years of crisis.',\n",
       "  'mutants': ['The fact is that the challenges inherent in completing the TTIP are no more serious than those that EU leaders have faced in the last few years of crisis.',\n",
       "   'The fact is that the challenges inherent in completing the TTIP are no more severe than those that EU leaders have faced in the last few years of crisis.',\n",
       "   'The fact is that the challenges inherent in completing the TTIP are no more pressing than those that EU leaders have faced in the last few years of crisis.']},\n",
       " {'en': 'When the TTIP negotiations resume next month, EU leaders must push for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       "  'mutants': ['When the TTIP negotiations resume next month, EU leaders must look for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       "   'When the TTIP negotiations resume next month, EU leaders must hope for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       "   'When the TTIP negotiations resume next month, EU leaders must strive for genuine progress, with the goal of completing a deal by the end of the year.']},\n",
       " {'en': 'The good news is that the recent midterm elections in the US might have improved their chances.',\n",
       "  'mutants': ['The good news is that the recent midterm elections in the US might have improved their chances.',\n",
       "   'The good news is that the recent midterm elections in the US might have ruined their chances.',\n",
       "   'The good news is that the recent midterm elections in the US might have reduced their chances.']},\n",
       " {'en': 'President Barack Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       "  'mutants': ['President barack Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       "   'President joe Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       "   'President president Obama now might get so-called fast-track negotiating authority from Congress.']},\n",
       " {'en': 'If he does, Congress would simply approve or reject any negotiated agreement, rather than picking it apart.',\n",
       "  'mutants': ['If he does, Congress would simply approve or reject any negotiated agreement, rather than tear it apart.',\n",
       "   'If he does, Congress would simply approve or reject any negotiated agreement, rather than tearing it apart.',\n",
       "   'If he does, Congress would simply approve or reject any negotiated agreement, rather than take it apart.']},\n",
       " {'en': 'The US presidential election season is starting, and other issues in the new year could easily take over the EU agenda.',\n",
       "  'mutants': ['The US presidential election season is starting, and other issues in the new year could easily take over the EU agenda.',\n",
       "   'The US presidential election season is starting, and other issues in the new year could possibly take over the EU agenda.',\n",
       "   'The US presidential election season is starting, and other issues in the new year could soon take over the EU agenda.']},\n",
       " {'en': 'That is why Europeâ€™s leaders have no time to waste.',\n",
       "  'mutants': ['That is why Europeâ€™s leaders have no time to waste.',\n",
       "   'That is why Europeâ€™s governments have no time to waste.',\n",
       "   'That is why Europeâ€™s nations have no time to waste.']},\n",
       " {'en': 'They must seize economic opportunity â€“ and avert strategic disaster.',\n",
       "  'mutants': ['They can seize economic opportunity â€“ and avert strategic disaster.',\n",
       "   'They must seize economic opportunity â€“ and avert strategic disaster.',\n",
       "   'They will seize economic opportunity â€“ and avert strategic disaster.']},\n",
       " {'en': 'The Year That Ended an Epoch?',\n",
       "  'mutants': ['The Year That was an Epoch?',\n",
       "   'The Year That became an Epoch?',\n",
       "   'The Year That is an Epoch?']},\n",
       " {'en': 'MADRID â€“ As 2016 comes to an end, the outlook for 2017 is shrouded in uncertainty.',\n",
       "  'mutants': ['MADRID â€“ As 2016 comes to an end, the outlook for 2017 is still in uncertainty.',\n",
       "   'MADRID â€“ As 2016 comes to an end, the outlook for 2017 is already in uncertainty.',\n",
       "   'MADRID â€“ As 2016 comes to an end, the outlook for 2017 is also in uncertainty.']},\n",
       " {'en': 'Tensions in the Middle East are rising, and populist movements have appeared in Europe and the United States.',\n",
       "  'mutants': ['Tensions in the Middle East are rising and populist movements have appeared in Europe and the United States.',\n",
       "   'Tensions in the Middle East are high and populist movements have appeared in Europe and the United States.',\n",
       "   'Tensions in the Middle East are increasing and populist movements have appeared in Europe and the United States.']},\n",
       " {'en': 'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at rapprochement, which were marred by the fundamental disagreement about Syrian President Bashar al-Assadâ€™s future role in any peace process or political transition.',\n",
       "  'mutants': ['In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at reconciliation which were marred by the fundamental disagreement about Syrian President Bashar al-Assadâ€™s future role in any peace process or political transition.',\n",
       "   'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at resolution which were marred by the fundamental disagreement about Syrian President Bashar al-Assadâ€™s future role in any peace process or political transition.',\n",
       "   'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at peace which were marred by the fundamental disagreement about Syrian President Bashar al-Assadâ€™s future role in any peace process or political transition.']},\n",
       " {'en': 'Meanwhile, over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo â€“ once Syriaâ€™s largest city, now utterly devastated by the war.',\n",
       "  'mutants': ['but over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo â€“ once Syriaâ€™s largest city, now utterly devastated by the war.',\n",
       "   'and over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo â€“ once Syriaâ€™s largest city, now utterly devastated by the war.',\n",
       "   'also over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo â€“ once Syriaâ€™s largest city, now utterly devastated by the war.']}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutant_pairs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sys = wn.synsets(\"book\", pos = wn.VERB)\n",
    "\n",
    "#[str(lemma.name()) for lemma in wn.synset(sys[0].name()).lemmas()]\n",
    "sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['however',\n",
       " 'nevertheless',\n",
       " 'withal',\n",
       " 'still',\n",
       " 'yet',\n",
       " 'all_the_same',\n",
       " 'even_so',\n",
       " 'nonetheless',\n",
       " 'notwithstanding']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('however', wn.ADV)[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrase ('brother', ['buddy', 'blood_brother', 'chum', 'sidekick', 'pal', 'comrade', 'crony'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def tag(sentence):\n",
    " words = word_tokenize(sentence)\n",
    " words = pos_tag(words)\n",
    " return words\n",
    "\n",
    "def paraphraseable(tag):\n",
    " return tag.startswith('N') or tag.startswith('V') or tag.startswith('J') or tag.startswith('R')\n",
    "\n",
    "def pos(tag):\n",
    " if tag.startswith('N'):\n",
    "  return wn.NOUN\n",
    " elif tag.startswith('V'):\n",
    "  return wn.VERB\n",
    " elif tag.startswith('J'):\n",
    "  return wn.ADJ\n",
    " elif pos.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "\n",
    "def synonyms(word, tag):\n",
    "    lemma_lists = [ss.lemmas() for ss in wn.synsets(word, pos(tag))]\n",
    "    lemmas = [lemma.name() for lemma in sum(lemma_lists, [])]\n",
    "    return set(lemmas)\n",
    "\n",
    "def synonymIfExists(sentence, index):\n",
    "   tags = tag(sentence)\n",
    "   word, t = tags[index]\n",
    "   if paraphraseable(t):\n",
    "    syns = synonyms(word, t)\n",
    "    if syns:\n",
    "      syns_list = [s for s in syns if s.lower() != word.lower()]\n",
    "      return word, syns_list\n",
    "    return word, []\n",
    "\n",
    "def paraphrase(sentence):\n",
    " return [x for x in synonymIfExists(sentence)]\n",
    "get=[]\n",
    "get=synonymIfExists(\"I am his old brother\", 4)\n",
    "print(\"paraphrase\",get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æˆ‘æ˜¯ä»–çš„è€å…„å¼Ÿ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer([\"I am his old blood brother\"], return_tensors=\"pt\")\n",
    "outputs = model.generate(input.input_ids, output_scores=True, return_dict_in_generate=True, output_hidden_states=True)\n",
    "tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeamSearchEncoderDecoderOutput(sequences=tensor([[65000,  2672,  2320,  2554,  7370,     0]]), sequences_scores=tensor([-0.6191]), scores=(tensor([[-8.0706e+00, -1.3176e+01, -1.8279e+01,  ..., -2.0530e+01,\n",
       "         -2.0005e+01,        -inf],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09,        -inf],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09,        -inf],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09,        -inf]]), tensor([[ -9.0575,  -9.1186,  -9.8088,  ..., -19.7308, -25.6526,     -inf],\n",
       "        [-12.3644, -10.8325,  -6.5922,  ..., -20.7825, -25.4118,     -inf],\n",
       "        [-12.9211, -14.8706, -18.3882,  ..., -26.8826, -28.8784,     -inf],\n",
       "        [-10.2162,  -7.9428,  -7.1700,  ..., -21.1276, -23.4055,     -inf]]), tensor([[ -9.6594, -10.5615, -10.2936,  ..., -21.6635, -25.1355,     -inf],\n",
       "        [ -9.1250,  -9.4463,  -8.5660,  ..., -21.3557, -23.9969,     -inf],\n",
       "        [ -8.5816,  -9.3940, -10.3983,  ..., -21.2435, -24.6133,     -inf],\n",
       "        [-12.1354, -14.8067, -16.3441,  ..., -26.1704, -29.6275,     -inf]]), tensor([[ -6.6145,  -8.1378, -10.1964,  ..., -20.3942, -22.8144,     -inf],\n",
       "        [ -7.7982,  -8.6290, -10.3179,  ..., -21.1071, -23.7560,     -inf],\n",
       "        [-12.4997, -12.6737, -13.5503,  ..., -23.9351, -26.8833,     -inf],\n",
       "        [ -9.7994, -13.1096, -13.0252,  ..., -25.3591, -27.4971,     -inf]]), tensor([[ -3.0957, -13.0173,  -6.6229,  ..., -22.6768, -26.0252,     -inf],\n",
       "        [ -3.4096, -13.8650,  -7.8142,  ..., -23.7624, -26.8911,     -inf],\n",
       "        [ -3.5840, -13.2102,  -7.5418,  ..., -23.0925, -26.4841,     -inf],\n",
       "        [ -3.7401, -12.9096,  -6.9442,  ..., -22.5029, -26.3529,     -inf]]), tensor([[ -5.6081, -17.7817, -18.3308,  ..., -26.5614, -25.6657,     -inf],\n",
       "        [ -6.1124, -18.6245, -19.7648,  ..., -27.0204, -25.8851,     -inf],\n",
       "        [ -6.3872, -18.3671, -19.2258,  ..., -26.9236, -25.9835,     -inf],\n",
       "        [ -6.5506, -18.2036, -19.2827,  ..., -26.8354, -25.9512,     -inf]])), encoder_attentions=None, encoder_hidden_states=(tensor([[[ 0.3468, -0.0593,  0.3446,  ..., -0.3040, -0.1900,  0.4216],\n",
       "         [ 1.8861,  0.8580,  0.6001,  ..., -0.2460,  0.3780,  0.0996],\n",
       "         [ 0.9865,  1.0367,  0.8521,  ...,  0.8540, -0.8031,  0.8809],\n",
       "         ...,\n",
       "         [-1.1959, -0.8270,  0.0870,  ..., -0.8830, -0.9582,  1.8733],\n",
       "         [-1.7496, -0.6921, -0.7966,  ...,  1.2136,  0.2179,  0.6713],\n",
       "         [-0.0220,  0.3182, -0.9137,  ...,  0.6230,  0.6750,  0.2616]]]), tensor([[[ 0.4042, -0.2027, -0.6349,  ..., -0.1734, -0.7449, -0.6875],\n",
       "         [ 1.0035,  0.9140,  0.4521,  ..., -0.8240, -0.1325, -0.3888],\n",
       "         [ 0.3238,  1.4224,  0.3470,  ...,  0.5625, -0.8859,  1.0611],\n",
       "         ...,\n",
       "         [-0.3398, -1.1085,  0.0534,  ..., -1.0491, -0.2823,  1.3319],\n",
       "         [-0.9108, -0.2572, -0.9972,  ...,  1.1047,  0.0889,  0.8161],\n",
       "         [-0.0602, -0.0428, -0.1500,  ..., -0.1825,  1.9735, -0.0350]]]), tensor([[[ 0.5426,  0.5221, -0.4740,  ..., -0.0795, -0.9348, -0.4579],\n",
       "         [ 0.0755,  1.0037,  0.8955,  ..., -0.7115,  0.4295, -0.0625],\n",
       "         [ 0.3175,  0.6033,  1.1177,  ..., -0.1558, -1.3818,  0.7759],\n",
       "         ...,\n",
       "         [ 0.0419, -0.5635, -0.2700,  ..., -1.2772, -0.1498,  1.4286],\n",
       "         [-0.1115, -0.6901, -0.6778,  ...,  0.8612,  0.1510,  0.7598],\n",
       "         [ 0.0522, -0.1320,  0.0451,  ..., -0.1582,  0.4066,  0.0842]]]), tensor([[[ 0.5946,  0.6063, -0.5754,  ..., -0.1866, -0.8190, -0.6433],\n",
       "         [ 0.2618,  0.5792,  0.2477,  ..., -0.5050,  0.0084,  0.0324],\n",
       "         [ 0.2178, -0.3474,  0.0389,  ...,  0.1920, -1.3821,  0.8041],\n",
       "         ...,\n",
       "         [-0.1788, -0.9146, -0.0559,  ..., -1.6132,  0.2544,  0.7030],\n",
       "         [ 0.7344, -0.2794,  0.1558,  ...,  0.4995, -0.0865,  0.1123],\n",
       "         [-0.0957, -0.1167, -0.1632,  ..., -0.2206,  0.0261,  0.1475]]]), tensor([[[ 0.3016,  0.1708, -0.1303,  ...,  0.3287, -0.3307,  0.3492],\n",
       "         [-0.2168,  0.0154,  0.6645,  ...,  0.1963,  0.0291,  0.0807],\n",
       "         [ 0.6471, -0.7146, -0.3265,  ...,  0.1714, -1.2405,  0.7126],\n",
       "         ...,\n",
       "         [ 0.2570, -0.3550, -0.7931,  ..., -1.5357,  0.4861,  0.1961],\n",
       "         [ 1.1568,  0.1955, -0.6755,  ...,  0.5843,  0.1092,  0.3390],\n",
       "         [-0.1551, -0.2164, -0.0707,  ..., -0.0876, -0.1131,  0.4097]]]), tensor([[[-0.0600,  0.5779, -0.6818,  ...,  0.7986, -0.9508,  0.4424],\n",
       "         [ 0.3136, -0.1603,  0.1079,  ...,  0.5084, -0.0677,  0.0628],\n",
       "         [ 0.9403, -0.8329,  0.4227,  ..., -0.3357, -0.9872,  0.1242],\n",
       "         ...,\n",
       "         [ 0.1932, -0.2600, -0.5308,  ..., -1.7123,  0.6601, -0.3139],\n",
       "         [ 0.7223, -0.0659, -0.1924,  ...,  0.1370,  0.0269,  0.3414],\n",
       "         [-0.1491, -0.1682, -0.1788,  ..., -0.0258, -0.2394,  0.2459]]]), tensor([[[ 0.1719, -0.0067, -0.4274,  ...,  0.7945, -0.4561,  0.5338],\n",
       "         [ 0.1281, -0.5929,  0.2467,  ...,  0.3022,  0.2956,  0.1809],\n",
       "         [ 0.2407, -0.4569,  0.5994,  ..., -0.5339, -0.3741,  0.0461],\n",
       "         ...,\n",
       "         [ 0.2606, -0.2287, -0.3382,  ..., -1.1979,  0.5374, -0.2935],\n",
       "         [ 0.3392, -0.0884, -0.1897,  ..., -0.2339, -0.1555, -0.1211],\n",
       "         [-0.0370, -0.0594, -0.0266,  ..., -0.1394, -0.2151,  0.0423]]])), decoder_attentions=None, cross_attentions=None, decoder_hidden_states=((tensor([[[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]],\n",
       "\n",
       "        [[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]],\n",
       "\n",
       "        [[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]],\n",
       "\n",
       "        [[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]]]), tensor([[[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]],\n",
       "\n",
       "        [[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]],\n",
       "\n",
       "        [[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]],\n",
       "\n",
       "        [[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]]]), tensor([[[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]],\n",
       "\n",
       "        [[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]],\n",
       "\n",
       "        [[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]],\n",
       "\n",
       "        [[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]]]), tensor([[[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]],\n",
       "\n",
       "        [[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]],\n",
       "\n",
       "        [[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]],\n",
       "\n",
       "        [[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]]]), tensor([[[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]],\n",
       "\n",
       "        [[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]],\n",
       "\n",
       "        [[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]],\n",
       "\n",
       "        [[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]]]), tensor([[[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]],\n",
       "\n",
       "        [[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]],\n",
       "\n",
       "        [[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]],\n",
       "\n",
       "        [[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]]]), tensor([[[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]],\n",
       "\n",
       "        [[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]],\n",
       "\n",
       "        [[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]],\n",
       "\n",
       "        [[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]]])), (tensor([[[ 1.3752,  0.7252,  1.7303,  ...,  0.1608,  0.7895,  0.4669]],\n",
       "\n",
       "        [[ 0.8758,  0.6905,  1.7954,  ...,  0.1587,  0.0176,  0.1273]],\n",
       "\n",
       "        [[ 0.9412,  0.9064,  1.8858,  ...,  0.6706,  0.8403,  0.5027]],\n",
       "\n",
       "        [[ 0.9093,  0.7428,  0.7795,  ..., -0.1225, -0.4390,  0.1416]]]), tensor([[[-0.6338,  0.1900, -0.0081,  ..., -0.8440,  0.6493, -0.0964]],\n",
       "\n",
       "        [[-0.0084, -0.3259, -0.8432,  ..., -0.0562, -0.1005, -0.3610]],\n",
       "\n",
       "        [[-0.7341, -0.0110,  0.1218,  ...,  0.0981,  1.4101, -0.1092]],\n",
       "\n",
       "        [[ 0.8696,  0.7684, -0.4003,  ...,  0.3899,  0.1323, -0.2550]]]), tensor([[[-0.1881,  0.9668,  0.2671,  ..., -1.4565,  0.5286,  0.1009]],\n",
       "\n",
       "        [[ 0.0839, -0.2131, -0.8653,  ..., -0.3846,  0.4270, -0.2407]],\n",
       "\n",
       "        [[-0.4180, -1.4671, -0.0554,  ...,  0.6997,  1.7747, -0.4086]],\n",
       "\n",
       "        [[ 0.6240, -0.0718, -0.1535,  ...,  0.5612,  0.4912, -0.6020]]]), tensor([[[-0.7800,  1.1222,  0.3322,  ..., -1.2844,  0.4637,  0.3102]],\n",
       "\n",
       "        [[-0.0977, -0.0200, -0.7190,  ..., -0.5539,  0.7813,  0.2757]],\n",
       "\n",
       "        [[-1.2829, -1.2407,  0.3688,  ...,  0.8264,  2.3903,  0.0200]],\n",
       "\n",
       "        [[ 0.1942, -0.1899,  0.3262,  ...,  0.2960,  0.4458, -0.2559]]]), tensor([[[-1.0633,  0.9297,  0.4906,  ..., -0.3730,  0.9208,  0.5365]],\n",
       "\n",
       "        [[-0.1933,  0.1830, -0.2552,  ..., -0.0853,  0.8656,  0.2406]],\n",
       "\n",
       "        [[-0.4999, -1.3624,  0.6060,  ...,  0.3297,  0.6816, -0.1871]],\n",
       "\n",
       "        [[ 0.3026, -0.1067,  1.1289,  ..., -0.0716,  0.0493,  0.4905]]]), tensor([[[-0.8745,  0.0561, -0.9117,  ..., -0.1073,  1.6905,  0.1678]],\n",
       "\n",
       "        [[ 0.1465,  0.6778, -0.5368,  ..., -0.0250,  1.0447,  0.3079]],\n",
       "\n",
       "        [[-0.1163, -2.0070,  0.3803,  ..., -0.8711,  2.0000, -0.3430]],\n",
       "\n",
       "        [[-0.6518,  0.1179,  1.7705,  ..., -0.6410, -0.5425,  0.3525]]]), tensor([[[ 2.3627, -0.5658, -4.2257,  ...,  0.3660,  5.0970, -1.1923]],\n",
       "\n",
       "        [[ 8.7663,  6.7017, -0.5519,  ...,  6.4116,  6.8683,  2.3494]],\n",
       "\n",
       "        [[ 3.7553, -5.1326,  3.2195,  ..., -1.2554, 10.1023, -1.0150]],\n",
       "\n",
       "        [[-0.7372,  0.9929,  5.2282,  ..., -2.4213,  0.5672,  1.8538]]])), (tensor([[[ 1.3077,  1.6365,  1.2665,  ...,  0.2984,  1.1858,  0.9258]],\n",
       "\n",
       "        [[ 1.3910,  1.2257,  1.0497,  ...,  0.6650,  1.0174,  0.3155]],\n",
       "\n",
       "        [[ 1.9161,  1.4189,  1.4074,  ...,  0.5770, -0.2562,  0.6835]],\n",
       "\n",
       "        [[ 0.9015,  1.2208,  1.1346,  ...,  0.2146,  0.9037,  0.0937]]]), tensor([[[ 0.3431, -0.5637,  0.3787,  ..., -0.3443,  0.8691,  0.0596]],\n",
       "\n",
       "        [[-0.2141, -0.4640, -0.1602,  ...,  0.3578,  0.3771, -0.3066]],\n",
       "\n",
       "        [[ 1.8768,  0.0594,  0.1546,  ..., -0.1605, -0.1369,  0.6260]],\n",
       "\n",
       "        [[-1.0154, -0.0353, -0.3643,  ..., -0.5874,  0.8222, -0.3101]]]), tensor([[[ 0.1165, -0.7664,  0.6281,  ..., -0.4668,  1.1748, -0.6576]],\n",
       "\n",
       "        [[-0.0373, -0.4419,  0.2965,  ...,  0.3401,  0.7440, -0.9097]],\n",
       "\n",
       "        [[ 1.2355, -0.3778, -0.1583,  ..., -0.3043,  0.3302, -0.0568]],\n",
       "\n",
       "        [[-0.2951,  0.6815,  0.0999,  ..., -1.1500,  0.4647, -0.2123]]]), tensor([[[-0.2989, -0.6664,  0.9231,  ..., -0.3308,  0.3051, -0.3776]],\n",
       "\n",
       "        [[-0.1281, -0.2125,  0.3366,  ...,  0.2509,  0.6282, -0.3133]],\n",
       "\n",
       "        [[ 1.0821,  0.0740, -0.4274,  ..., -0.4342,  0.3453, -0.5712]],\n",
       "\n",
       "        [[-1.1076,  1.0402,  0.2219,  ..., -1.1132,  0.4350,  0.0209]]]), tensor([[[-0.0963, -0.5139,  1.1370,  ..., -0.2954, -0.2420,  0.3309]],\n",
       "\n",
       "        [[-0.0392, -0.1226,  0.9448,  ...,  0.0526,  0.1083,  0.1754]],\n",
       "\n",
       "        [[ 0.2952, -0.4682, -0.3861,  ..., -0.3740,  0.2446,  0.0950]],\n",
       "\n",
       "        [[-1.1077,  0.9542,  0.1811,  ..., -0.4618,  0.7419,  0.3958]]]), tensor([[[ 0.5223, -0.3105,  1.4012,  ...,  0.0818, -0.5787,  0.4374]],\n",
       "\n",
       "        [[ 0.0909, -0.1317,  1.0713,  ...,  0.6061,  0.5060, -0.0129]],\n",
       "\n",
       "        [[ 0.3697, -0.1355, -0.5999,  ..., -0.0891, -0.5396, -0.1718]],\n",
       "\n",
       "        [[-0.9060,  0.3342, -1.0973,  ..., -0.5997,  1.6889, -0.0865]]]), tensor([[[ 6.8671,  0.3144,  1.9647,  ...,  5.1155, -5.2701, -0.2905]],\n",
       "\n",
       "        [[ 8.4827, -0.0141,  2.9547,  ..., 10.1506,  1.4818,  0.7651]],\n",
       "\n",
       "        [[ 5.9604,  1.7759, -3.3866,  ...,  6.2326, -5.6534, -2.4670]],\n",
       "\n",
       "        [[ 4.3836, -0.2963, -3.9889,  ..., -1.4357,  5.2408, -2.0975]]])), (tensor([[[ 1.1479,  0.7276,  0.7920,  ...,  0.5770, -0.2562,  0.6835]],\n",
       "\n",
       "        [[ 1.1479,  0.7276,  0.7920,  ...,  0.5770, -0.2562,  0.6835]],\n",
       "\n",
       "        [[ 0.6448,  0.4286,  0.7745,  ...,  0.0927,  0.5194, -0.2204]],\n",
       "\n",
       "        [[ 0.1630,  0.9444,  1.0975,  ...,  0.5192,  0.4252,  0.7792]]]), tensor([[[ 2.0141, -0.2072,  0.1000,  ..., -0.2684, -0.0357,  0.6847]],\n",
       "\n",
       "        [[ 1.9596,  0.1509, -0.0545,  ..., -0.1658, -0.0846,  0.7015]],\n",
       "\n",
       "        [[ 0.5406,  0.4224, -0.1765,  ..., -0.6808, -0.2171, -1.2616]],\n",
       "\n",
       "        [[ 0.8790, -0.1194, -0.1433,  ..., -0.7188,  0.4968,  0.7401]]]), tensor([[[ 1.7136, -0.8766, -0.0958,  ..., -0.6825,  0.4487,  0.2855]],\n",
       "\n",
       "        [[ 1.5217, -0.4128, -0.2345,  ..., -0.4097,  0.3929,  0.1266]],\n",
       "\n",
       "        [[ 0.5875, -0.7104,  0.0309,  ..., -1.1978,  0.3864, -1.7069]],\n",
       "\n",
       "        [[ 1.0982, -1.0355, -0.1570,  ..., -0.4095,  0.4333,  0.4515]]]), tensor([[[ 1.8342, -0.6643, -0.3671,  ..., -0.8203,  0.5229, -0.3958]],\n",
       "\n",
       "        [[ 1.5945, -0.1929, -0.6108,  ..., -0.4931,  0.2455, -0.3553]],\n",
       "\n",
       "        [[ 0.7768, -0.4572,  0.9122,  ..., -1.0015,  0.1091, -1.1313]],\n",
       "\n",
       "        [[ 1.2674, -0.6651, -0.2644,  ..., -0.5892,  0.1734,  0.6201]]]), tensor([[[ 0.8612, -1.1590, -0.3666,  ..., -0.7846,  0.2569,  0.3278]],\n",
       "\n",
       "        [[ 0.8045, -0.8630, -0.3976,  ..., -0.5012,  0.1785,  0.3768]],\n",
       "\n",
       "        [[ 0.1488, -0.1971,  1.1647,  ..., -0.8721,  0.0747, -0.7533]],\n",
       "\n",
       "        [[ 1.3568,  0.0945,  0.1329,  ..., -0.8566,  0.6048,  0.4781]]]), tensor([[[ 0.7099, -0.6942, -0.0769,  ..., -0.0860, -0.6823,  0.1207]],\n",
       "\n",
       "        [[ 0.8126, -0.3899, -0.2184,  ...,  0.1780, -0.6579, -0.1683]],\n",
       "\n",
       "        [[ 0.9170, -0.1253,  1.7311,  ...,  0.0699,  0.5918, -0.2794]],\n",
       "\n",
       "        [[ 1.5542,  0.2263,  0.1688,  ...,  0.0418,  0.2775,  0.1577]]]), tensor([[[ 5.3168,  0.6143, -2.9551,  ...,  5.6563, -7.5836, -2.1464]],\n",
       "\n",
       "        [[ 6.5826,  0.9498, -2.2169,  ...,  8.3863, -7.0443, -2.2352]],\n",
       "\n",
       "        [[ 8.3926, -0.5893,  6.6866,  ...,  4.8648,  1.4328,  0.2441]],\n",
       "\n",
       "        [[13.2284,  3.8744, -0.7293,  ..., 10.9088, -1.1516,  2.2559]]])), (tensor([[[-0.4513, -0.6551, -0.1825,  ...,  0.3269,  0.3788,  0.0785]],\n",
       "\n",
       "        [[-0.8269,  0.1611, -0.4391,  ...,  0.2206,  1.0618,  0.4410]],\n",
       "\n",
       "        [[-0.6121, -0.0013, -0.3834,  ...,  0.9918,  0.0318,  0.7557]],\n",
       "\n",
       "        [[-0.6121, -0.0013, -0.3834,  ...,  0.9918,  0.0318,  0.7557]]]), tensor([[[ 1.2487,  0.2139, -0.9652,  ...,  0.1044, -0.1009, -0.0617]],\n",
       "\n",
       "        [[ 0.9065,  0.5728, -0.1066,  ..., -0.2096, -0.1985, -0.0552]],\n",
       "\n",
       "        [[ 0.9858,  0.2660, -0.9309,  ..., -0.1074, -0.3745,  0.3543]],\n",
       "\n",
       "        [[ 0.9762,  0.2837, -0.9455,  ..., -0.1105, -0.3612,  0.3532]]]), tensor([[[ 1.1193,  0.2529, -0.9176,  ...,  0.1810,  0.2754, -0.4644]],\n",
       "\n",
       "        [[ 1.0675, -0.1174, -0.5790,  ..., -0.0618,  0.2600, -0.7228]],\n",
       "\n",
       "        [[ 1.1154, -0.3488, -1.1426,  ...,  0.0722,  0.2476, -0.0287]],\n",
       "\n",
       "        [[ 0.9424, -0.2078, -1.1723,  ...,  0.1864,  0.2657, -0.1628]]]), tensor([[[ 0.7463,  0.6394, -0.3111,  ...,  0.4897, -0.4000, -0.4116]],\n",
       "\n",
       "        [[ 0.3099,  0.4253, -0.3272,  ..., -0.0155, -0.3719, -1.1136]],\n",
       "\n",
       "        [[ 0.5298, -0.1925, -0.5699,  ...,  0.2893, -0.1962, -0.3401]],\n",
       "\n",
       "        [[ 0.3801, -0.0242, -0.6691,  ...,  0.6287, -0.1248, -0.1957]]]), tensor([[[ 0.8432,  0.1680,  0.3303,  ...,  0.1875, -0.2814, -0.3347]],\n",
       "\n",
       "        [[ 0.3424,  0.3089, -0.0638,  ...,  0.1112, -0.1223, -0.8203]],\n",
       "\n",
       "        [[ 0.5029, -0.2868, -0.3744,  ...,  0.2949, -0.0760, -0.3560]],\n",
       "\n",
       "        [[ 0.3802,  0.0486, -0.2333,  ...,  0.5928, -0.1190, -0.3305]]]), tensor([[[ 0.7273,  0.0840,  1.1742,  ..., -0.0510, -1.2408, -0.7058]],\n",
       "\n",
       "        [[ 0.0014,  0.1600,  0.8180,  ..., -0.1004, -0.7970, -1.0187]],\n",
       "\n",
       "        [[ 0.7785, -0.4086, -0.1639,  ..., -0.1127, -1.1221, -0.7342]],\n",
       "\n",
       "        [[ 0.7466, -0.0103,  0.0592,  ...,  0.1794, -1.1372, -0.7856]]]), tensor([[[ 3.5050,  3.3481,  2.3852,  ...,  2.6766, -4.5272, -3.9199]],\n",
       "\n",
       "        [[-0.4249,  2.1750,  1.6835,  ...,  1.3572, -2.4010, -4.3397]],\n",
       "\n",
       "        [[ 4.2093,  2.4347, -2.2539,  ...,  3.2859, -4.9463, -1.8884]],\n",
       "\n",
       "        [[ 4.0194,  3.1173, -0.6455,  ...,  5.0434, -3.8894, -2.3015]]])), (tensor([[[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]],\n",
       "\n",
       "        [[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]],\n",
       "\n",
       "        [[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]],\n",
       "\n",
       "        [[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]]]), tensor([[[ 0.5770,  0.4768, -0.4539,  ...,  0.1900,  0.0651,  0.2025]],\n",
       "\n",
       "        [[ 0.7021,  0.4195, -0.3997,  ...,  0.1820,  0.0366,  0.2762]],\n",
       "\n",
       "        [[ 0.6558,  0.4452, -0.4901,  ...,  0.2179, -0.0025,  0.1606]],\n",
       "\n",
       "        [[ 0.6521,  0.4601, -0.4922,  ...,  0.2187,  0.0051,  0.1552]]]), tensor([[[ 0.4750,  0.1106, -0.0414,  ...,  0.1398,  0.2365,  0.0154]],\n",
       "\n",
       "        [[ 0.4723,  0.0440,  0.0048,  ...,  0.1891,  0.1455,  0.1075]],\n",
       "\n",
       "        [[ 0.4302,  0.0800, -0.1065,  ...,  0.1249,  0.2341, -0.0224]],\n",
       "\n",
       "        [[ 0.4415,  0.0529, -0.1155,  ...,  0.1859,  0.2077, -0.0267]]]), tensor([[[ 0.4703,  0.3315,  0.0085,  ...,  0.0076, -0.1374, -0.4071]],\n",
       "\n",
       "        [[ 0.3847,  0.2337, -0.0981,  ...,  0.1733, -0.1773, -0.3822]],\n",
       "\n",
       "        [[ 0.3872,  0.2036, -0.1225,  ...,  0.0332, -0.1402, -0.4343]],\n",
       "\n",
       "        [[ 0.3451,  0.2346, -0.1439,  ...,  0.1459, -0.1980, -0.3148]]]), tensor([[[ 0.4618,  0.0932, -0.0572,  ...,  0.3011, -1.3014, -0.1619]],\n",
       "\n",
       "        [[ 0.3835,  0.0755, -0.1499,  ...,  0.4640, -1.3007, -0.1690]],\n",
       "\n",
       "        [[ 0.4286, -0.0084, -0.2452,  ...,  0.3620, -1.2974, -0.1465]],\n",
       "\n",
       "        [[ 0.4482, -0.0089, -0.3100,  ...,  0.4459, -1.3416, -0.1144]]]), tensor([[[ 0.0903, -0.5004,  0.7909,  ..., -0.5611, -1.6997, -0.8448]],\n",
       "\n",
       "        [[ 0.0453, -0.4721,  0.6763,  ..., -0.5324, -1.5529, -0.8912]],\n",
       "\n",
       "        [[ 0.1036, -0.5550,  0.6311,  ..., -0.5081, -1.6995, -0.8432]],\n",
       "\n",
       "        [[ 0.1602, -0.5452,  0.5648,  ..., -0.5043, -1.7187, -0.7966]]]), tensor([[[-1.1703, -0.2199,  3.7070,  ..., -2.5994, -3.1775, -2.2700]],\n",
       "\n",
       "        [[-1.3739,  0.0971,  3.1880,  ..., -3.1332, -2.7319, -2.6753]],\n",
       "\n",
       "        [[-1.1799, -0.2546,  3.1743,  ..., -2.2641, -2.9037, -2.2557]],\n",
       "\n",
       "        [[-0.9040, -0.1390,  3.0676,  ..., -2.3530, -2.6203, -2.1989]]]))))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 14:01:50.942 | DEBUG    | text2vec.sentence_model:__init__:74 - Use device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7635359  -0.39156067  0.63999367 ...  0.2583298  -0.5223585\n",
      "  -0.21127473]\n",
      " [ 1.2544321  -0.17131853  1.1561146  ...  0.10490549 -0.32768762\n",
      "  -0.14699247]]\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel\n",
    "sentences = ['æˆ‘æ˜¯ä»–çš„è€å…„å¼Ÿ', 'æˆ‘ä¸æ˜¯ä»–çš„è€å¼Ÿ']\n",
    "\n",
    "model2 = SentenceModel('shibing624/text2vec-base-chinese')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003824949264526"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "cos = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0153, -0.0026,  0.0152,  ..., -0.0576, -0.0526, -0.0256],\n",
       "         [ 0.0462,  0.0016, -0.0089,  ..., -0.0551, -0.0275, -0.0398],\n",
       "         [ 0.0034,  0.0044, -0.0047,  ..., -0.0065, -0.0797, -0.0053],\n",
       "         [-0.0086,  0.0047,  0.0133,  ..., -0.0568,  0.0043, -0.0624],\n",
       "         [-0.0349,  0.0133,  0.0089,  ...,  0.0094, -0.0346, -0.0145],\n",
       "         [ 0.0114,  0.0351, -0.0119,  ..., -0.0167, -0.0144, -0.0326]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æˆ‘å–œæ¬¢ç‹—ã€‚']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=tokenizer('I like dog.',return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input, output_scores=True, return_dict_in_generate=True, output_hidden_states=True)\n",
    "tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0308, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get your batch data: token_id, mask and labels\n",
    "token_ids = tokenizer('Meanwhile, over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo â€“ once Syriaâ€™s largest city, now utterly devastated by the war.', return_tensors=\"pt\").input_ids\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer('æ­¤å¤–ï¼Œè¿‡åŽ»ä¸€å‘¨ï¼Œå™åˆ©äºšæ”¿åºœå†›åœ¨ä¿„ç½—æ–¯å’Œä¼Šæœ—çš„æ”¯æŒä¸‹ï¼Œå·²ç»å¤ºå›žäº†å‡ ä¹Žæ•´ä¸ªé˜¿å‹’é¢‡â€”â€”é˜¿å‹’é¢‡æ›¾æ˜¯å™åˆ©äºšæœ€å¤§çš„åŸŽå¸‚ï¼ŒçŽ°åœ¨å·²ç»å½»åº•æ¯ç­åœ¨æˆ˜ç«ä¹‹ä¸­ã€‚', return_tensors=\"pt\").input_ids\n",
    "# get your token embeddings\n",
    "token_embeds=model.get_input_embeddings().weight[token_ids].clone()\n",
    "token_embeds.retain_grad() \n",
    "# get model output that contains loss value\n",
    "outs = model(inputs_embeds=token_embeds,labels=labels, output_hidden_states=True, output_attentions=True)\n",
    "loss=outs.loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 512])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "grad=token_embeds.grad\n",
    "grad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "idx = np.argsort(torch.norm(grad, dim=2).squeeze(dim=0)).tolist()[::-1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['over',\n",
       " ',',\n",
       " 'the',\n",
       " 'Meanwhile',\n",
       " 'past',\n",
       " 'week',\n",
       " 'Syrian',\n",
       " ',',\n",
       " 'Iran',\n",
       " 'government',\n",
       " 'troops',\n",
       " 'have',\n",
       " ',',\n",
       " 'and',\n",
       " 'backed']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "input_tokens = [token_ids[0][i] for i in idx ]\n",
    "[tokenizer.decode(t) for t in input_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': [{'en': '1929 or 1989?', 'zh': '1929å¹´è¿˜æ˜¯1989å¹´?'},\n",
       "  {'en': 'PARIS â€“ As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       "   'zh': 'å·´é»Ž-éšç€ç»æµŽå±æœºä¸æ–­åŠ æ·±å’Œè”“å»¶ï¼Œæ•´ä¸ªä¸–ç•Œä¸€ç›´åœ¨å¯»æ‰¾åŽ†å²ä¸Šçš„ç±»ä¼¼äº‹ä»¶å¸Œæœ›æœ‰åŠ©äºŽæˆ‘ä»¬äº†è§£ç›®å‰æ­£åœ¨å‘ç”Ÿçš„æƒ…å†µã€‚'},\n",
       "  {'en': 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       "   'zh': 'ä¸€å¼€å§‹ï¼Œå¾ˆå¤šäººæŠŠè¿™æ¬¡å±æœºæ¯”ä½œ1982å¹´æˆ–1973å¹´æ‰€å‘ç”Ÿçš„æƒ…å†µï¼Œè¿™æ ·å¾—ç±»æ¯”æ˜¯ä»¤äººå®½å¿ƒçš„ï¼Œå› ä¸ºè¿™ä¸¤æ®µæ—¶æœŸæ„å‘³ç€å…¸åž‹çš„å‘¨æœŸæ€§è¡°é€€ã€‚'},\n",
       "  {'en': 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "   'zh': 'å¦‚ä»Šäººä»¬çš„å¿ƒæƒ…å´æ˜¯æ²‰é‡å¤šäº†ï¼Œè®¸å¤šäººå¼€å§‹æŠŠè¿™æ¬¡å±æœºä¸Ž1929å¹´å’Œ1931å¹´ç›¸æ¯”ï¼Œå³ä½¿ä¸€äº›å›½å®¶æ”¿åºœçš„è¡¨çŽ°ä»ç„¶ä¼¼ä¹ŽæŠŠè§†ç›®å‰çš„æƒ…å†µä¸ºæ˜¯å…¸åž‹çš„è€Œçœ‹è§çš„è¡°é€€ã€‚'},\n",
       "  {'en': 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       "   'zh': 'ç›®å‰çš„è¶‹åŠ¿æ˜¯ï¼Œè¦ä¹ˆæ˜¯è¿‡åº¦çš„å…‹åˆ¶ï¼ˆæ¬§æ´²ï¼‰ï¼Œè¦ä¹ˆæ˜¯åŠªåŠ›çš„æ‰©å±•ï¼ˆç¾Žå›½ï¼‰ã€‚'},\n",
       "  {'en': 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "   'zh': 'æ¬§æ´²åœ¨é¿å…å€ºåŠ¡å’Œæå«æ¬§å…ƒçš„åä¹‰ä¸‹æ­£å˜å¾—è°¨æ…Žï¼Œè€Œç¾Žå›½å·²ç»åœ¨è®¸å¤šæ–¹é¢è¡ŒåŠ¨èµ·æ¥ï¼Œä»¥åˆ©ç”¨è¿™ä¸€ç†æƒ³çš„æ—¶æœºæ¥å®žè¡Œæ€¥éœ€çš„ç»“æž„æ€§æ”¹é©ã€‚'},\n",
       "  {'en': 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "   'zh': 'ç„¶è€Œï¼Œä½œä¸ºåœ°åŸŸæˆ˜ç•¥å­¦å®¶ï¼Œæ— è®ºæ˜¯ä»Žæ”¿æ²»æ„ä¹‰è¿˜æ˜¯ä»Žç»æµŽæ„ä¹‰ä¸Šï¼Œè®©æˆ‘è‡ªç„¶æƒ³åˆ°çš„å¹´ä»½æ˜¯1989å¹´ã€‚'},\n",
       "  {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "   'zh': 'å½“ç„¶ï¼Œé›·æ›¼å…„å¼Ÿå…¬å¸çš„å€’é—­å’ŒæŸæž—å¢™çš„å€’å¡Œæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚'},\n",
       "  {'en': 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       "   'zh': 'äº‹å®žä¸Šï¼Œä»Žè¡¨é¢ä¸Šçœ‹ï¼Œä¸¤è€…ä¼¼ä¹Žæ˜¯å®Œå…¨æ˜¯ç›¸åçš„ï¼šä¸€ä¸ªæ˜¯è±¡å¾ç€åŽ‹æŠ‘å’Œäººä¸ºåˆ†è£‚çš„æŸæž—å¢™çš„å€’å¡Œï¼Œè€Œå¦ä¸€ä¸ªæ˜¯çœ‹ä¼¼åšä¸å¯æ‘§çš„å¹¶ä»¤äººå®‰å¿ƒçš„é‡‘èžèµ„æœ¬ä¸»ä¹‰æœºæž„çš„å€’å¡Œã€‚'},\n",
       "  {'en': 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.',\n",
       "   'zh': 'ç„¶è€Œï¼Œå’Œ1989å¹´ä¸€æ ·ï¼Œ2008-2009å¹´å¾ˆå¯èƒ½ä¹Ÿèƒ½è¢«è§†ä¸ºä¸€ä¸ªåˆ’æ—¶ä»£çš„æ”¹å˜ï¼Œå…¶å¸¦æ¥çš„å‘äººæ·±çœçš„åŽæžœå°†åœ¨å‡ åå¹´åŽä»èƒ½è®©æˆ‘ä»¬æ„Ÿå—å¾—åˆ°ã€‚'}]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1224)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0][1]\n",
    "torch.norm(grad, dim=2).squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "def is_stopword(token_id, tokenizer):\n",
    "    word = tokenizer.decode(token_id)\n",
    "    if word in string.punctuation:\n",
    "        return True\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    stopword = stopwords.words('english') + [\"</s>\", \"<unk>\", \">>cmn_Hans<<\", \"<pad>\"]\n",
    "    if word in stopword or not word:\n",
    "        return True\n",
    "    return False \n",
    "def gradient_search(en_sentence, zh_sentence, k=5):\n",
    "    token_ids = tokenizer(en_sentence, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(zh_sentence, return_tensors=\"pt\").input_ids\n",
    "    # get your token embeddings\n",
    "    token_embeds=model.get_input_embeddings().weight[token_ids].clone()\n",
    "    token_embeds.retain_grad() \n",
    "    # get model output that contains loss value\n",
    "    outs = model(inputs_embeds=token_embeds,labels=labels, output_hidden_states=True, output_attentions=True)\n",
    "    loss=outs.loss\n",
    "    loss.backward(retain_graph=True)\n",
    "    grad=token_embeds.grad\n",
    "    grad_norm = torch.norm(grad, dim=2).squeeze(dim=0)\n",
    "    \n",
    "    for i in range(len(token_ids[0])):\n",
    "        current_token = token_ids[0][i]\n",
    "        if is_stopword(current_token, tokenizer):\n",
    "            grad_norm[i] = 0\n",
    "    idx = np.argsort(grad_norm).tolist()[::-1][:k]\n",
    "    \n",
    "    input_tokens = [token_ids[0][i] for i in idx ]\n",
    "    return idx, [tokenizer.decode(t) for t in input_tokens if tokenizer.decode(t) not in string.punctuation]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_data(dataset):\n",
    "    for pair in dataset['translation']:\n",
    "        pair[\"token_index\"], pair[\"top_tokens\"] = gradient_search(pair[\"en\"], pair[\"zh\"])\n",
    "    return dataset['translation']\n",
    "result = loop_data(raw_datasets[\"test\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': '28-Year-Old Chef Found Dead at San Francisco Mall',\n",
       "  'zh': '28å²åŽ¨å¸ˆè¢«å‘çŽ°æ­»äºŽæ—§é‡‘å±±ä¸€å®¶å•†åœº',\n",
       "  'token_index': [8, 10, 7, 6, 11],\n",
       "  'top_tokens': ['Dead', 'San', 'Found', 'Chef', 'Francisco']},\n",
       " {'en': 'A 28-year-old chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "  'zh': 'è¿‘æ—¥åˆšæ¬è‡³æ—§é‡‘å±±çš„ä¸€ä½28å²åŽ¨å¸ˆæœ¬å‘¨è¢«å‘çŽ°æ­»äºŽå½“åœ°ä¸€å®¶å•†åœºçš„æ¥¼æ¢¯é—´ã€‚',\n",
       "  'token_index': [1, 3, 6, 5, 21],\n",
       "  'top_tokens': ['28', 'year', 'chef', 'old', 'air']},\n",
       " {'en': 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "  'zh': 'ä½†å—å®³äººçš„å“¥å“¥è¡¨ç¤ºæƒ³ä¸å‡ºæœ‰è°ä¼šæƒ³è¦åŠ å®³äºŽä»–ï¼Œå¹¶ç§°â€œä¸€åˆ‡ç»ˆäºŽå¥½èµ·æ¥äº†ã€‚â€',\n",
       "  'token_index': [2, 5, 6, 11, 21],\n",
       "  'top_tokens': ['victim', 'brother', 'says', 'think', 'saying']},\n",
       " {'en': \"The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "  'zh': 'æ—§é‡‘å±±éªŒå°¸å®˜åŠžå…¬å®¤è¡¨ç¤ºï¼Œå‘¨ä¸‰æ—©ä¸ŠäºŽè¥¿ç”°è´­ç‰©ä¸­å¿ƒå‘çŽ°çš„å°¸ä½“ç¡®è®¤ä¸º28å²æ—§é‡‘å±±å±…æ°‘ Frank Galiciaã€‚',\n",
       "  'token_index': [1, 2, 14, 5, 6],\n",
       "  'top_tokens': ['body', 'found', '28', 'West', 'field']},\n",
       " {'en': 'The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.',\n",
       "  'zh': 'æ—§é‡‘å±±è­¦å¯Ÿå±€ç§°è¯¥èµ·æ­»äº¡æ¡ˆä»¶è¢«è£å®šä¸ºä»–æ€ï¼Œå¹¶æ­£åœ¨è¿›è¡Œè°ƒæŸ¥ã€‚',\n",
       "  'token_index': [9, 11, 2, 7, 1],\n",
       "  'top_tokens': ['ruled', 'homicide', 'Francisco', 'death', 'San']},\n",
       " {'en': \"The victim's brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "  'zh': 'å—å®³äººçš„å“¥å“¥ Louis Galicia å¯¹ç¾Žå›½å¹¿æ’­å…¬å¸ä½äºŽæ—§é‡‘å±±çš„ç”µå° KGO è¡¨ç¤ºï¼Œä¹‹å‰åœ¨æ³¢å£«é¡¿åšæµæ°´çº¿åŽ¨å¸ˆçš„Frank äºŽå…­ä¸ªæœˆå‰åœ¨æ—§é‡‘å±±çš„ Sons & Daughters é¤é¦†æ‰¾åˆ°ä¸€ä»½æµæ°´çº¿åŽ¨å¸ˆçš„ç†æƒ³å·¥ä½œã€‚',\n",
       "  'token_index': [1, 4, 7, 8, 6],\n",
       "  'top_tokens': ['victim', 'brother', 'Gali', 'cia', 'Louis']},\n",
       " {'en': 'A spokesperson for Sons & Daughters said they were \"shocked and devastated\" by his death.',\n",
       "  'zh': 'Sons & Daughters é¤é¦†çš„ä¸€ä½å‘è¨€äººè¡¨ç¤ºï¼Œä»–ä»¬å¯¹äºŽ Frank çš„æ­»æ„Ÿåˆ°â€œéžå¸¸éœ‡æƒŠâ€ã€‚',\n",
       "  'token_index': [13, 14, 1, 3, 16],\n",
       "  'top_tokens': ['shock', 'ed', 'spokesperson', 'Son', 'devastated']},\n",
       " {'en': '\"We are a small team that operates like a close knit family and he will be dearly missed,\" the spokesperson said.',\n",
       "  'zh': 'è¯¥å‘è¨€äººç§°ï¼Œâ€œæˆ‘ä»¬æ˜¯ç›¸å¤„å¾—åƒäº²å¯†æ— é—´çš„ä¸€å®¶äººçš„ä¸€ä¸ªå°å›¢é˜Ÿï¼Œæˆ‘ä»¬å°†æ·±æ·±æ€€å¿µä»–ã€‚â€',\n",
       "  'token_index': [18, 4, 5, 10, 11],\n",
       "  'top_tokens': ['dear', 'small', 'team', 'close', 'k']},\n",
       " {'en': \"Our thoughts and condolences are with Frank's family and friends at this difficult time.\",\n",
       "  'zh': 'åœ¨è¿™ä¸ªæ‚²ç—›çš„æ—¶åˆ»ï¼Œæˆ‘ä»¬å‘ Frank çš„å®¶äººåŠæœ‹å‹è¡¨è¾¾æˆ‘ä»¬æ·±åˆ‡çš„åŒæƒ…ä¸Žå“€æ‚¼ã€‚',\n",
       "  'token_index': [1, 11, 9, 3, 6],\n",
       "  'top_tokens': ['thoughts', 'friends', 'family', 'condolences', 'Frank']},\n",
       " {'en': 'Louis Galicia said Frank initially stayed in hostels, but recently, \"Things were finally going well for him.\"',\n",
       "  'zh': 'Louis Galicia ç§° Frank èµ·åˆä½åœ¨æ‹›å¾…æ‰€é‡Œï¼Œä½†æ˜¯æœ€è¿‘â€œä¸€åˆ‡ç»ˆäºŽå¥½èµ·æ¥äº†ã€‚â€',\n",
       "  'token_index': [15, 2, 16, 1, 0],\n",
       "  'top_tokens': ['Th', 'cia', 'ing', 'Gali', 'Louis']},\n",
       " {'en': '\"He found an apartment, he was dating a girl,\" Louis Galicia told KGO.',\n",
       "  'zh': 'Louis Galicia å‘Šè¯‰ KGOï¼šâ€œFrankæ‰¾åˆ°ä¸€é—´å…¬å¯“ï¼ŒåŒæ—¶åœ¨è·Ÿä¸€ä¸ªå¥³å­©äº¤å¾€ã€‚â€',\n",
       "  'token_index': [10, 12, 13, 2, 8],\n",
       "  'top_tokens': ['girl', 'Louis', 'Gali', 'found', 'dating']},\n",
       " {'en': 'Louis Galicia said he could not think of anyone who would want to hurt his younger brother.',\n",
       "  'zh': 'Louis Galicia è¡¨ç¤ºï¼Œä»–æƒ³ä¸å‡ºæœ‰è°ä¼šæƒ³è¦åŠ å®³ä»–çš„å¼Ÿå¼Ÿã€‚',\n",
       "  'token_index': [11, 0, 9, 1, 12],\n",
       "  'top_tokens': ['would', 'Louis', 'anyone', 'Gali', 'want']},\n",
       " {'en': 'He was a kind spirit with a big heart.',\n",
       "  'zh': 'Frank æ‹¥æœ‰å–„è‰¯çš„å¿ƒåœ°åŠå®½é˜”çš„èƒ¸æ€€ã€‚',\n",
       "  'token_index': [7, 8, 4, 3, 10],\n",
       "  'top_tokens': ['big', 'heart', 'spirit', 'kind', '</s>']},\n",
       " {'en': 'His way of connecting with the family was always making us a dish, making us dinner,\" Louis Galicia said.',\n",
       "  'zh': 'Louis Galicia ç§°ï¼šâ€œFrank ç»å¸¸ä¸ºå®¶äººçƒ¹é¥ªæ™šé¤ã€‚â€',\n",
       "  'token_index': [14, 1, 15, 12, 16],\n",
       "  'top_tokens': ['making', 'way', 'us', 'dish', 'dinner']},\n",
       " {'en': 'He never wanted to be in any kind of altercation.',\n",
       "  'zh': 'ä»–ä»Žä¸æ„¿æ„ä¸Žå®¶äººäº‰åµã€‚',\n",
       "  'token_index': [7, 9, 10, 2, 1],\n",
       "  'top_tokens': ['kind', 'alter', 'c', 'wanted', 'never']},\n",
       " {'en': 'He was the brother that went with the flow.',\n",
       "  'zh': 'ä»–æ˜¯ä¸€ä¸ªéšä»Žå¤§å®¶æ„è§çš„äººã€‚',\n",
       "  'token_index': [8, 5, 3, 10, 9],\n",
       "  'top_tokens': ['flow', 'went', 'brother', '</s>']},\n",
       " {'en': '\"With everything else that\\'s going wrong with the world, he was that diamond in the rough that was shining bright every day,\" he said.',\n",
       "  'zh': 'Louis Galicia è¯´ï¼šâ€œä¸–ä¸Šçš„æ‰€æœ‰äº‹ç‰©éƒ½æœ‰å¯èƒ½å‡ºé”™ï¼Œä½†Frank ç§°å¾—ä¸Šæ˜¯ä¸€ä¸ªæ¯å¤©éƒ½å‘å…‰çš„å¤–ç²—å†…ç§€ä¹‹äººã€‚â€',\n",
       "  'token_index': [2, 3, 16, 8, 7],\n",
       "  'top_tokens': ['everything', 'else', 'diamond', 'wrong', 'going']},\n",
       " {'en': 'Anyone with information is asked to call the SFPD Tip Line at 415-575-4444.',\n",
       "  'zh': 'æœ‰çº¿ç´¢äººå£«è¯·æ‹¨æ‰“æ—§é‡‘å±±è­¦å¯Ÿå±€ä¸¾æŠ¥ç”µè¯415-575-4444ã€‚',\n",
       "  'token_index': [14, 2, 0, 12, 15],\n",
       "  'top_tokens': ['415', 'information', 'Anyone', 'Line', '-5']},\n",
       " {'en': 'Junior doctors strike: Calls for fresh industrial action',\n",
       "  'zh': 'åˆçº§åŒ»ç”Ÿç½¢å·¥ï¼šå‘¼åæ–°çš„åŠ³å·¥è¡ŒåŠ¨',\n",
       "  'token_index': [7, 6, 8, 4, 0],\n",
       "  'top_tokens': ['industrial', 'fresh', 'action', 'Calls', 'Junior']},\n",
       " {'en': 'Representatives of junior doctors have called on their union to authorise fresh industrial action in their dispute about a new contract.',\n",
       "  'zh': 'åˆçº§åŒ»ç”Ÿä»£è¡¨å·å¬è”ç›Ÿæ‰¹å‡†å…¶é’ˆå¯¹æ–°åˆåŒçº çº·é‡‡å–æ–°çš„åŠ³å·¥è¡ŒåŠ¨ã€‚',\n",
       "  'token_index': [14, 13, 0, 2, 12],\n",
       "  'top_tokens': ['action',\n",
       "   'industrial',\n",
       "   'Representatives',\n",
       "   'junior',\n",
       "   'fresh']},\n",
       " {'en': 'The Junior Doctors Committee (JDC) of the British Medical Association (BMA) is to ask its full council to back more industrial action from early September.',\n",
       "  'zh': 'è‹±å›½åŒ»å­¦åä¼šåˆçº§åŒ»ç”Ÿå§”å‘˜ä¼šå°†è¦æ±‚å…¶å…¨ä½“æˆå‘˜è‡ªä¹æœˆåˆå¼€å§‹æ”¯æŒæ›´å¤šçš„åŠ³å·¥è¡ŒåŠ¨ã€‚',\n",
       "  'token_index': [1, 2, 20, 4, 22],\n",
       "  'top_tokens': ['Junior', 'Doctor', 'ask', 'Committee', 'full']},\n",
       " {'en': 'The JDC says ministers have failed to address concerns about the contract.',\n",
       "  'zh': 'åˆçº§åŒ»ç”Ÿå§”å‘˜ä¼šè¡¨ç¤ºï¼Œéƒ¨é•¿ä»¬æœªè§£å†³å¯¹äºŽåˆåŒçš„å¿§è™‘ã€‚',\n",
       "  'token_index': [9, 8, 1, 2, 6],\n",
       "  'top_tokens': ['concerns', 'address', 'J', 'DC', 'failed']},\n",
       " {'en': 'Junior doctors and medical students voted in July to reject a contract deal agreed with the BMA.',\n",
       "  'zh': 'åˆçº§åŒ»ç”Ÿä¸ŽåŒ»ç§‘å­¦ç”ŸäºŽ7æœˆä»½æŠ•ç¥¨æŠµåˆ¶ä¸Žè‹±å›½åŒ»å­¦åä¼šè¾¾æˆçš„åˆåŒäº¤æ˜“ã€‚',\n",
       "  'token_index': [11, 12, 0, 1, 9],\n",
       "  'top_tokens': ['contract', 'deal', 'Junior', 'doctors', 'reject']},\n",
       " {'en': 'It was rejected by 58% of its members who voted in the ballot.',\n",
       "  'zh': 'å‚ä¸ŽæŠ•ç¥¨çš„æˆå‘˜ä¸­ï¼Œ58%åå¯¹è¯¥åˆåŒäº¤æ˜“ã€‚',\n",
       "  'token_index': [10, 2, 8, 4, 13],\n",
       "  'top_tokens': ['voted', 'rejected', 'members', '58', 'ballot']},\n",
       " {'en': 'In a letter to members released on Twitter on Thursday night, the JDC\\'s chair Ellen McCourt said the government had remained \"persistently silent\" on issues which, she said, had resulted in the contract being rejected.',\n",
       "  'zh': 'åˆçº§åŒ»ç”Ÿå§”å‘˜ä¼šä¸»å¸­ Ellen McCourt åœ¨å‘¨å››æ™šé—´äºŽæŽ¨ç‰¹ä¸Šå‘è¡¨çš„ä¸€å°è‡´æˆå‘˜ä¿¡ä¸­è¡¨ç¤ºï¼Œå¯¹äºŽå¯¼è‡´åˆåŒé­åˆ°æŠµåˆ¶çš„é—®é¢˜ï¼Œæ”¿åºœâ€œä¸€è´¯ä¿æŒæ²‰é»˜â€ã€‚',\n",
       "  'token_index': [2, 4, 7, 5, 9],\n",
       "  'top_tokens': ['letter', 'members', 'Twitter', 'released', 'Thursday']},\n",
       " {'en': 'She said: \"In light of this, the JDC Executive has voted to reject the proposed new contract in full and to call for formal re-negotiations on all of your concerns.\"',\n",
       "  'zh': 'å¥¹è¡¨ç¤ºï¼šâ€œé‰´äºŽæ­¤ï¼Œåˆçº§åŒ»ç”Ÿå§”å‘˜ä¼šæ‰§è¡Œå§”å‘˜ä¼šå·²æŠ•ç¥¨å…¨ç›˜é©³å›žæè®®çš„æ–°åˆåŒå¹¶å‘¼åå°±æ‰€æœ‰æ‹…å¿§é‡æ–°è¿›è¡Œæ­£å¼åå•†ã€‚â€',\n",
       "  'token_index': [1, 25, 14, 27, 5],\n",
       "  'top_tokens': ['said', 'call', 'voted', 'formal', 'light']},\n",
       " {'en': \"In response to the government's silence, JDC exec has today made a formal request for a special meeting of BMA Council to authorise a rolling programme of escalated industrial action beginning in early September.\",\n",
       "  'zh': 'é’ˆå¯¹æ”¿åºœçš„æ²‰é»˜æ€åº¦ï¼Œåˆçº§åŒ»ç”Ÿå§”å‘˜ä¼šæ‰§è¡Œå§”å‘˜ä¼šå·²äºŽä»Šæ—¥æ­£å¼è¦æ±‚è‹±å›½åŒ»å­¦åä¼šç†äº‹ä¼šå¬å¼€ç‰¹åˆ«ä¼šè®®æ‰¹å‡†æ—¨åœ¨ä»Žä¹æœˆåˆå¼€å§‹å‡çº§åŠ³å·¥è¡ŒåŠ¨çš„ä¸€é¡¹é•¿æœŸè®¡åˆ’ã€‚',\n",
       "  'token_index': [1, 7, 9, 4, 10],\n",
       "  'top_tokens': ['response', 'silence', 'J', 'government', 'DC']},\n",
       " {'en': 'The dispute has led to junior doctors taking part in six strikes this year, including the first all-out stoppages in the history of the NHS.',\n",
       "  'zh': 'è¯¥çº çº·å·²å¯¼è‡´åˆçº§åŒ»ç”Ÿä»Šå¹´å…±å‚ä¸Žå…­æ¬¡ç½¢å·¥ï¼ŒåŒ…æ‹¬è‹±å›½å›½å®¶åŒ»ç–—æœåŠ¡ä½“ç³»åŽ†å²ä¸Šçš„é¦–æ¬¡å…¨é¢ç½¢å·¥ã€‚',\n",
       "  'token_index': [1, 3, 5, 6, 22],\n",
       "  'top_tokens': ['dispute', 'led', 'junior', 'doctors', 'page']},\n",
       " {'en': \"The BMA's junior doctor leader, Dr Johann Malawana, resigned following the vote to reject the negotiated terms of the contract, which the BMA had recommended.\",\n",
       "  'zh': 'è‹±å›½åŒ»å­¦åä¼šåˆçº§åŒ»ç”Ÿé¢†å¯¼è€… Johann Malawana åšå£«åœ¨è‹±å›½åŒ»å­¦åä¼šæ‰€æŽ¨èçš„åˆåŒè®®å®šæ¡æ¬¾é­åˆ°æŠ•ç¥¨æŠµåˆ¶ä¹‹åŽè¾žèŒã€‚',\n",
       "  'token_index': [1, 23, 5, 12, 6],\n",
       "  'top_tokens': ['B', 'negotiated', 'junior', 'Mala', 'doctor']},\n",
       " {'en': 'He had told BMA members the deal was a good one that should be accepted, during meetings ahead of the poll of 54,000 junior doctors and medical students.',\n",
       "  'zh': 'åœ¨54,000ååˆçº§åŒ»ç”Ÿä¸ŽåŒ»ç§‘å­¦ç”ŸæŠ•ç¥¨ä¹‹å‰çš„ä¼šè®®ä¸­ï¼Œä»–æ›¾å¯¹è‹±å›½åŒ»å­¦åä¼šæˆå‘˜è¡¨ç¤ºï¼Œè¯¥äº¤æ˜“ä¸é”™ï¼Œå› æ­¤åº”è¯¥äºˆä»¥æŽ¥å—ã€‚',\n",
       "  'token_index': [2, 22, 3, 19, 5],\n",
       "  'top_tokens': ['told', 'poll', 'B', 'ahead', 'members']},\n",
       " {'en': 'Following the vote, Health Secretary Jeremy Hunt said the contract would be imposed on medics in England.',\n",
       "  'zh': 'æŠ•ç¥¨åŽï¼Œå«ç”Ÿéƒ¨é•¿ Jeremy Hunt è¡¨ç¤ºï¼Œè¯¥åˆåŒå°†åœ¨è‹±æ ¼å…°åŒ»æŠ¤äººå‘˜ä¸­å¼ºåˆ¶å®žæ–½ã€‚',\n",
       "  'token_index': [0, 2, 13, 4, 6],\n",
       "  'top_tokens': ['Following', 'vote', 'imposed', 'Health', 'Jeremy']},\n",
       " {'en': \"Russia and Turkey: An 'alliance of misfits'?\",\n",
       "  'zh': 'ä¿„ç½—æ–¯ä¸ŽåœŸè€³å…¶ï¼šâ€œæ ¼æ ¼ä¸å…¥è”ç›Ÿâ€ï¼Ÿ',\n",
       "  'token_index': [8, 10, 0, 2, 11],\n",
       "  'top_tokens': ['ance', 'mis', 'Russia', 'Turkey', 'fits']},\n",
       " {'en': 'It was a gesture that ended a crisis.',\n",
       "  'zh': 'ä¸€ä¸ªæ‰‹åŠ¿ç»“æŸä¸€åœºå±æœºã€‚',\n",
       "  'token_index': [7, 5, 3, 9, 8],\n",
       "  'top_tokens': ['crisis', 'ended', 'gesture', '</s>']},\n",
       " {'en': 'The leaders of Russia and Turkey met on Tuesday to shake hands and declare a formal end to an eight-month long war of words and economic sanctions.',\n",
       "  'zh': 'ä¿„ç½—æ–¯ä¸ŽåœŸè€³å…¶é¢†å¯¼äººå‘¨äºŒè¿›è¡Œä¼šè§ï¼ŒåŒæ–¹æ¡æ‰‹å¹¶å®£å¸ƒæ­£å¼ç»“æŸé•¿è¾¾å…«ä¸ªæœˆçš„å£æ°´æˆ˜ä¸Žç»æµŽåˆ¶è£ã€‚',\n",
       "  'token_index': [1, 3, 5, 6, 11],\n",
       "  'top_tokens': ['leaders', 'Russia', 'Turkey', 'met', 'hands']},\n",
       " {'en': 'But, as Vladimir Putin greeted his Turkish counterpart in the gilded hall of a St. Petersburg palace, I got the distinct impression that Ankara wants this reconciliation the most.',\n",
       "  'zh': 'ç„¶è€Œï¼Œå½“å¼—æ‹‰åŸºç±³å°”Â·æ™®äº¬åœ¨ä¸€åº§åœ£å½¼å¾·æ–¯å ¡å®«æ®¿é‡‘ç¢§è¾‰ç…Œçš„å¤§åŽ…ä¸­è¿ŽæŽ¥åœŸè€³å…¶æ€»ç»Ÿæ—¶ï¼Œæˆ‘æœ‰ç§å¼ºçƒˆçš„æ„Ÿè§‰ï¼šæ­¤æ¬¡å’Œè§£æ˜¯å®‰å¡æ‹‰æœ€å¸Œæœ›çœ‹åˆ°çš„ã€‚',\n",
       "  'token_index': [3, 4, 16, 25, 15],\n",
       "  'top_tokens': ['Vladimir', 'Putin', 'hall', 'got', 'ed']},\n",
       " {'en': 'There was the handshake, yes.',\n",
       "  'zh': 'æ˜¯çš„ï¼ŒåŒæ–¹ç¡®å®žæ¡æ‰‹äº†ã€‚',\n",
       "  'token_index': [7, 4, 5, 3, 9],\n",
       "  'top_tokens': ['yes', 'hak', 'e', 'hands', '</s>']},\n",
       " {'en': \"But Mr Putin's smile looked thin and he was hardly oozing warmth even by his own restrained standards.\",\n",
       "  'zh': 'ç„¶è€Œï¼Œå³ä½¿æ˜¯æŒ‰ç…§æ™®äº¬å…ˆç”Ÿä¸€å‘å†…æ•›çš„æ ‡å‡†æ¥çœ‹ï¼Œæ™®äº¬ä¹Ÿåªæ˜¯æµ…æµ…å¾®ç¬‘ï¼Œå‡ ä¹Žæœªæµéœ²å‡ºçƒ­æƒ…ä¹‹æ„ã€‚',\n",
       "  'token_index': [1, 2, 15, 16, 17],\n",
       "  'top_tokens': ['Mr', 'Putin', 'warm', 'th', 'even']},\n",
       " {'en': 'Recep Tayyip Erdogan by contrast talked repeatedly of his \"dear friend\" Mr Putin - five times, according to one report.',\n",
       "  'zh': 'æ®æŠ¥é“ï¼Œé›·æ°æ™®Â·å¡”ä¼Šæ™®Â·åŸƒå°”å¤šå®‰åˆ™äº”æ¬¡æåˆ°â€œå¥½æœ‹å‹â€æ™®äº¬ã€‚',\n",
       "  'token_index': [2, 1, 0, 4, 19],\n",
       "  'top_tokens': ['Tay', 'ep', 'Rec', 'ip', 'Mr']},\n",
       " {'en': 'I lost count.',\n",
       "  'zh': 'æˆ‘è®°ä¸æ¸…äº†ã€‚',\n",
       "  'token_index': [2, 1, 4, 3, 0],\n",
       "  'top_tokens': ['count', 'lost', '</s>', 'I']},\n",
       " {'en': 'He also pledged that relations with Russia would return not just to their pre-crisis level, but even higher.',\n",
       "  'zh': 'åŒæ—¶ï¼Œä»–æ‰¿è¯ºä¸Žä¿„ç½—æ–¯çš„å…³ç³»å°†å›žå½’è‡³å±æœºå‘ç”Ÿå‰çš„æ°´å¹³ï¼Œç”šè‡³è¾¾åˆ°æ›´é«˜æ°´å¹³ã€‚',\n",
       "  'token_index': [15, 2, 13, 16, 1],\n",
       "  'top_tokens': ['crisis', 'pledged', 'pre', 'level', 'also']},\n",
       " {'en': 'The next day one newspaper here described Mr Erdogan as acting as if nothing bad had ever happened.',\n",
       "  'zh': 'æ¬¡æ—¥ï¼Œæ­¤é—´ä¸€å®¶æŠ¥çº¸å†™åˆ°ï¼ŒåŸƒå°”å¤šå®‰å…ˆç”Ÿåˆ™è¡¨çŽ°çš„å¥½åƒä»€ä¹ˆéƒ½æ²¡å‘ç”Ÿä¸€æ ·ã€‚',\n",
       "  'token_index': [2, 1, 12, 3, 4],\n",
       "  'top_tokens': ['day', 'next', 'acting', 'one', 'newspaper']},\n",
       " {'en': 'To me, his enthusiasm implied the opposite.',\n",
       "  'zh': 'ä½†æ˜¯ï¼Œæˆ‘è®¤ä¸ºä»–çš„çƒ­æƒ…æš—ç¤ºäº†æŸäº›ç›¸åçš„ä¸œè¥¿ã€‚',\n",
       "  'token_index': [7, 4, 5, 9, 8],\n",
       "  'top_tokens': ['opposite', 'enthusiasm', 'implied', '</s>']},\n",
       " {'en': \"But a lingering coolness emanating from Mr Putin showed that Russia's leader has forgotten nothing.\",\n",
       "  'zh': 'ç„¶è€Œï¼Œæ™®äº¬å…ˆç”ŸæŒç»­å†·æ·¡çš„æ€åº¦è¯´æ˜Žä»–ä»€ä¹ˆéƒ½æ²¡æœ‰å¿˜è®°ã€‚',\n",
       "  'token_index': [12, 2, 10, 15, 3],\n",
       "  'top_tokens': ['Russia', 'linger', 'showed', 'leader', 'ing']},\n",
       " {'en': \"In fact, the cause of the crisis was the first thing he mentioned in his opening comments: Turkey's shooting down of a Russian fighter plane on the Syrian border.\",\n",
       "  'zh': 'å®žé™…ä¸Šï¼Œä»–åœ¨å¼€åœºç™½ä¸­æåˆ°çš„ç¬¬ä¸€ä»¶äº‹ä¾¿æ˜¯æ­¤æ¬¡å±æœºçš„åŽŸå› ï¼šåœŸè€³å…¶äºŽå™åˆ©äºšè¾¹å¢ƒå‡»è½ä¸€æž¶ä¿„ç½—æ–¯æˆ˜æœºã€‚',\n",
       "  'token_index': [1, 4, 11, 10, 7],\n",
       "  'top_tokens': ['fact', 'cause', 'thing', 'first', 'crisis']},\n",
       " {'en': \"Moscow's reaction at the time was furious.\",\n",
       "  'zh': 'èŽ«æ–¯ç§‘å½“æ—¶çš„ååº”å¾ˆæ¿€çƒˆã€‚',\n",
       "  'token_index': [8, 6, 0, 3, 10],\n",
       "  'top_tokens': ['furious', 'time', 'Moscow', 'reaction', '</s>']},\n",
       " {'en': 'Mr Putin lashed out, accusing Ankara of stabbing Moscow in the back.',\n",
       "  'zh': 'æ™®äº¬å…ˆç”Ÿé’ˆå¯¹è¯¥äº‹ä»¶è¿›è¡ŒçŒ›çƒˆæŠ¨å‡»ï¼ŒæŒ‡è´£å®‰å¡æ‹‰åœ¨èƒŒåŽæš—ç®—èŽ«æ–¯ç§‘ã€‚',\n",
       "  'token_index': [10, 11, 12, 13, 8],\n",
       "  'top_tokens': ['stab', 'b', 'ing', 'Moscow', 'Ankara']},\n",
       " {'en': 'The offence was even greater, coming from a supposed friend.',\n",
       "  'zh': 'æ¥è‡ªæ‰€è°“æœ‹å‹çš„æ”»å‡»æ›´è®©äººéš¾ä»¥æŽ¥å—ã€‚',\n",
       "  'token_index': [6, 1, 4, 9, 3],\n",
       "  'top_tokens': ['coming', 'offence', 'greater', 'supposed', 'even']},\n",
       " {'en': 'Rebuilding real trust will be hard, perhaps impossible.',\n",
       "  'zh': 'åŒæ–¹å¾ˆéš¾ï¼Œç”šè‡³ä¸å¯èƒ½é‡æ–°å»ºç«‹çœŸæ­£çš„ä¿¡ä»»ã€‚',\n",
       "  'token_index': [8, 6, 1, 3, 2],\n",
       "  'top_tokens': ['perhaps', 'hard', 'building', 'trust', 'real']},\n",
       " {'en': 'Russian public opinion has also turned since November.',\n",
       "  'zh': 'è‡ª11æœˆä»½å¼€å§‹ï¼Œä¿„ç½—æ–¯æ°‘æ„ä¹Ÿæœ‰æ‰€æ‰­è½¬ã€‚',\n",
       "  'token_index': [7, 6, 1, 2, 5],\n",
       "  'top_tokens': ['November', 'since', 'public', 'opinion', 'turned']},\n",
       " {'en': 'For months, state-controlled media conducted a staggering, all-out offensive against Ankara.',\n",
       "  'zh': 'æ•°æœˆæ¥ï¼Œè¢«æ”¿åºœæŽ§åˆ¶çš„åª’ä½“å¯¹å®‰å¡æ‹‰è¿›è¡Œäº†ä»¤äººéœ‡æƒŠçš„å…¨åŠ›è¿›æ”»ã€‚',\n",
       "  'token_index': [9, 1, 3, 7, 14],\n",
       "  'top_tokens': ['staggering', 'months', 'state', 'conducted', 'offensive']},\n",
       " {'en': 'All of a sudden, it seemed like Turks were to blame for everything.',\n",
       "  'zh': 'çªç„¶ä¹‹é—´ï¼ŒåœŸè€³å…¶äººä¼¼ä¹Žè¦ä¸ºæ‰€æœ‰äº‹æƒ…è´Ÿè´£ã€‚',\n",
       "  'token_index': [8, 11, 3, 7, 6],\n",
       "  'top_tokens': ['Turks', 'blame', 'sudden', 'like', 'seemed']},\n",
       " {'en': \"Most serious were accusations from top officials that Mr Erdogan's own family has profited from an illegal trade in oil from areas of Syria controlled by the so-called Islamic State.\",\n",
       "  'zh': 'å…¶ä¸­ï¼Œæœ€ä¸¥é‡çš„æ˜¯æ¥è‡ªé«˜çº§å®˜å‘˜çš„æŒ‡è´£ï¼šåŸƒå°”å¤šå®‰å…ˆç”Ÿçš„å®¶äººä»Žæ‰€è°“ä¼Šæ–¯å…°å›½æŽ§åˆ¶çš„å™åˆ©äºšåœ°åŒºçš„éžæ³•çŸ³æ²¹äº¤æ˜“ä¸­èŽ·åˆ©ã€‚',\n",
       "  'token_index': [1, 3, 5, 6, 9],\n",
       "  'top_tokens': ['serious', 'accusations', 'top', 'officials', 'Er']},\n",
       " {'en': \"He's denied that emphatically.\",\n",
       "  'zh': 'ä»–å·²æ–­ç„¶å¦è®¤è¯¥ç§è¯´æ³•ã€‚',\n",
       "  'token_index': [6, 5, 3, 8, 7],\n",
       "  'top_tokens': ['ally', 'emphatic', 'denied', '</s>']},\n",
       " {'en': \"But in St. Petersburg came the official message that it's time to move on.\",\n",
       "  'zh': 'ç„¶è€Œï¼Œåœ£å½¼å¾·æ–¯å ¡ä¼ æ¥çš„æ¶ˆæ¯æ˜¾ç¤ºï¼Œæ˜¯æ—¶å€™è¯¥å‘å‰çœ‹äº†ã€‚',\n",
       "  'token_index': [2, 4, 13, 8, 7],\n",
       "  'top_tokens': ['St', 'Petersburg', 'time', 'message', 'official']},\n",
       " {'en': 'After all, this meeting only happened because Mr Putin got the apology he demanded from President Erdogan.',\n",
       "  'zh': 'æ¯•ç«Ÿæ­¤æ¬¡ä¼šé¢çš„å‰ææ˜¯æ™®äº¬æ”¶åˆ°åœŸè€³å…¶æ€»ç»ŸåŸƒå°”å¤šå®‰çš„é“æ­‰ã€‚',\n",
       "  'token_index': [12, 14, 4, 9, 16],\n",
       "  'top_tokens': ['apology', 'demanded', 'meeting', 'Putin', 'President']},\n",
       " {'en': 'Russia could claim a victory of sorts.',\n",
       "  'zh': 'ä¿„ç½—æ–¯å¯ä»¥å®£å¸ƒèƒœåˆ©äº†ã€‚',\n",
       "  'token_index': [6, 1, 4, 0, 2],\n",
       "  'top_tokens': ['sorts', 'could', 'victory', 'Russia', 'claim']},\n",
       " {'en': 'For Ankara the benefits of calling a truce are clear.',\n",
       "  'zh': 'å¯¹äºŽå®‰å¡æ‹‰æ¥è¯´ï¼Œä¼‘æˆ˜çš„å¥½å¤„æ˜¾è€Œæ˜“è§ã€‚',\n",
       "  'token_index': [7, 9, 1, 5, 3],\n",
       "  'top_tokens': ['truce', 'clear', 'Ankara', 'calling', 'benefits']},\n",
       " {'en': 'First and foremost, Erdogan needs all the friends he can get after he was nearly ousted from power last month in a failed coup.',\n",
       "  'zh': 'é¦–å…ˆï¼ŒåŸƒå°”å¤šå®‰åœ¨ä¸Šä¸ªæœˆæ”¿å˜å¤±è´¥åŽé™©äº›è¢«èµ¶ä¸‹å°ï¼Œå› æ­¤ä»–éœ€è¦æ‰€æœ‰æœ‹å‹çš„æ”¯æŒã€‚',\n",
       "  'token_index': [2, 0, 4, 18, 17],\n",
       "  'top_tokens': ['foremost', 'First', 'Er', 'oust', 'nearly']},\n",
       " {'en': 'Repeat terror attacks on Turkey have clearly shaken him too.',\n",
       "  'zh': 'ä¸æ–­çš„ææ€–è¢­å‡»æ˜¾ç„¶å·²å¯¹ä»–é€ æˆå¾ˆå¤§æ‰“å‡»ã€‚',\n",
       "  'token_index': [7, 6, 4, 0, 1],\n",
       "  'top_tokens': ['shaken', 'clearly', 'Turkey', 'Repeat', 'terror']},\n",
       " {'en': 'There is also an economic motive.',\n",
       "  'zh': 'åŒæ—¶ä¹Ÿæœ‰ç»æµŽåŠ¨æœºã€‚',\n",
       "  'token_index': [5, 4, 2, 7, 6],\n",
       "  'top_tokens': ['motive', 'economic', 'also', '</s>']},\n",
       " {'en': 'Russian sanctions have hit hard - particularly the ban on charter flights, which usually carry several million Russian tourists to the Turkish coast each year.',\n",
       "  'zh': 'ä¿„ç½—æ–¯åˆ¶è£å¯¹åœŸè€³å…¶æ‰“å‡»å¾ˆå¤§ï¼Œå°¤å…¶æ˜¯ç¦æ­¢æ¯å¹´å¯è¾“é€å‡ ç™¾ä¸‡ä¿„ç½—æ–¯æ¸¸å®¢è‡³åœŸè€³å…¶æµ·å²¸çš„åŒ…æœºæœåŠ¡ã€‚',\n",
       "  'token_index': [0, 1, 17, 16, 18],\n",
       "  'top_tokens': ['Russian', 'sanctions', 'million', 'several', 'Russian']},\n",
       " {'en': 'The number has slumped by almost 90%.',\n",
       "  'zh': 'è¯¥æ•°å­—å·²å¤§å¹…ä¸‹æ»‘è¿‘90%ã€‚',\n",
       "  'token_index': [7, 6, 1, 4, 3],\n",
       "  'top_tokens': ['90', 'almost', 'number', 'ped', 'slum']},\n",
       " {'en': 'As for Russia, tour operators and charter companies here will certainly be relieved when flights eventually resume.',\n",
       "  'zh': 'éšç€èˆªç­æ¢å¤ï¼Œä¿„ç½—æ–¯çš„æ—…è¡Œç¤¾ä¸ŽåŒ…æœºå…¬å¸å¿…å°†æ¾ä¸€å£æ°”ã€‚',\n",
       "  'token_index': [11, 13, 2, 15, 4],\n",
       "  'top_tokens': ['certainly', 'relieved', 'Russia', 'flights', 'tour']},\n",
       " {'en': \"They're banking on a late-season rush to the Mediterranean.\",\n",
       "  'zh': 'ä»–ä»¬å°†è¿Žæ¥å­£æœ«åœ°ä¸­æµ·æ—…æ¸¸æ½®ã€‚',\n",
       "  'token_index': [9, 8, 10, 6, 13],\n",
       "  'top_tokens': ['son', 'sea', 'rush', 'late', 'Mediterranean']},\n",
       " {'en': 'And even this week, state TV has been predicting cheaper fruit and vegetables once Turkish agricultural imports are permitted again.',\n",
       "  'zh': 'å›½å®¶ç”µè§†å°ç”šè‡³åœ¨æœ¬å‘¨é¢„æµ‹ï¼Œä¸€æ—¦åœŸè€³å…¶å†œäº§å“è¿›å£é‡æ–°æ”¾å¼€ï¼Œæ°´æžœåŠè”¬èœçš„ä»·æ ¼å°†ä¸‹é™ã€‚',\n",
       "  'token_index': [1, 14, 16, 3, 17],\n",
       "  'top_tokens': ['even', 'vegetables', 'Turkish', 'week', 'agricultural']},\n",
       " {'en': '\"Tourists that way, tomatoes back here,\" as a report in Vedemosti newspaper phrased it.',\n",
       "  'zh': 'èŽ«æ–¯ç§‘æŠ¥çº¸ Vedemosti çš„ä¸€ç¯‡æŠ¥é“ç§°ï¼šâ€œæ¸¸å®¢åŽ»äº†ï¼Œç•ªèŒ„å›žæ¥äº†ã€‚â€',\n",
       "  'token_index': [16, 3, 13, 19, 5],\n",
       "  'top_tokens': ['de', 'ists', 'report', 'newspaper', 'way']},\n",
       " {'en': 'But the visit also had additional political value for Moscow.',\n",
       "  'zh': 'å¯¹äºŽèŽ«æ–¯ç§‘æ¥è¯´ï¼Œæ­¤æ¬¡è®¿é—®ä¹Ÿå…·æœ‰é¢å¤–çš„æ”¿æ²»ä»·å€¼ã€‚',\n",
       "  'token_index': [9, 7, 6, 2, 3],\n",
       "  'top_tokens': ['Moscow', 'value', 'political', 'visit', 'also']},\n",
       " {'en': 'Ankara is angry with the West for what it considers a weak response to the attempted takeover.',\n",
       "  'zh': 'å®‰å¡æ‹‰å¯¹äºŽè¥¿æ–¹ä¸–ç•Œå¯¹æŽ¥ç®¡æ„å›¾çš„å¾®å¼±ååº”æ„Ÿåˆ°æ„¤æ€’ã€‚',\n",
       "  'token_index': [11, 12, 9, 0, 2],\n",
       "  'top_tokens': ['weak', 'response', 'considers', 'Ankara', 'angry']},\n",
       " {'en': \"Add to that its long-standing grudge at the snail's pace of talks to join the EU and step in Mr Putin - who is keen to capitalise on the chill and chip away at Turkey's ties with the West.\",\n",
       "  'zh': 'æ­¤å¤–ï¼Œå®‰å¡æ‹‰å¯¹äºŽåŠ å…¥æ¬§ç›Ÿè°ˆåˆ¤çš„ç¼“æ…¢è¿›å±•åŠæ™®äº¬çš„æ’æ‰‹é•¿æœŸæ„Ÿåˆ°ä¸æ»¡ï¼Œæ™®äº¬çƒ­è¡·äºŽåˆ©ç”¨æ”¿æ²»å¯’æ„ä»¥åŠå‰Šå¼±åœŸè€³å…¶ä¸Žè¥¿æ–¹ä¸–ç•Œçš„å…³ç³»ã€‚',\n",
       "  'token_index': [0, 10, 4, 7, 6],\n",
       "  'top_tokens': ['Add', 'snail', 'long', 'grudge', 'standing']},\n",
       " {'en': 'The Russian leader certainly won bonus points with Ankara for calling in support of the elected authorities after the attempted coup.',\n",
       "  'zh': 'ç”±äºŽåœ¨æ”¿å˜å¤±è´¥åŽæ‹¥æŠ¤å½“é€‰å½“å±€ï¼Œä¿„ç½—æ–¯é¢†å¯¼äººå¿…å°†èŽ·å¾—å®‰å¡æ‹‰çš„åŠ åˆ†ã€‚',\n",
       "  'token_index': [15, 1, 2, 16, 12],\n",
       "  'top_tokens': ['elected', 'Russian', 'leader', 'authorities', 'support']},\n",
       " {'en': \"Mind you, that's a given for Moscow which has its own deep-seated fear of regime change.\",\n",
       "  'zh': 'æ³¨æ„ï¼Œè¿™å¯¹äºŽä¸€ç›´å¯¹æ”¿æƒæ›´è¿­æ€€æŠ±æ ¹æ·±è’‚å›ºææƒ§çš„èŽ«æ–¯ç§‘æ¥è¯´æ˜¯ä¸€ç§é¦ˆèµ ã€‚',\n",
       "  'token_index': [0, 14, 16, 17, 18],\n",
       "  'top_tokens': ['Mind', 'deep', 'se', 'ated', 'fear']},\n",
       " {'en': 'So the summit at this glitzy, seaside palace allowed Russia and Turkey to present what one analyst described to me as an \"alliance of misfits\": two countries that feel rejected and mistreated by the West, joining forces.',\n",
       "  'zh': 'å› æ­¤ï¼Œåœ¨è¿™ä¸ªé‡‘ç¢§è¾‰ç…Œçš„æµ·è¾¹å®«æ®¿æ‰€ä¸¾è¡Œçš„ä¼šé¢ä½¿ä¿„ç½—æ–¯ä¸ŽåœŸè€³å…¶ä¸¤ä¸ªè¢«è¥¿æ–¹ä¸–ç•Œæ‹’ç»ä¸Žè™å¾…çš„å›½å®¶ç»“æˆç›Ÿå‹ï¼Œä¸€ä½åˆ†æžå¸ˆå°†å…¶æè¿°ä¸ºâ€œæ ¼æ ¼ä¸å…¥è”ç›Ÿâ€ã€‚',\n",
       "  'token_index': [2, 17, 15, 7, 9],\n",
       "  'top_tokens': ['summit', 'present', 'Turkey', 'zy', 'sea']},\n",
       " {'en': 'Still, despite the public display of reconciliation, the two still have major differences.',\n",
       "  'zh': 'ç„¶è€Œï¼Œå°½ç®¡å…¬å¼€å’Œè§£ï¼Œä½†åŒæ–¹ä»å­˜åœ¨é‡å¤§åˆ†æ­§ã€‚',\n",
       "  'token_index': [0, 2, 10, 11, 7],\n",
       "  'top_tokens': ['Still', 'despite', 'two', 'still', 'reconciliation']},\n",
       " {'en': 'The key one is Syria, where Moscow has recently been casting itself as peacemaker but where Russia and Turkey back opposite sides.',\n",
       "  'zh': 'å™åˆ©äºšæ˜¯å…³é”®å› ç´ ä¹‹ä¸€ã€‚èŽ«æ–¯ç§‘è¿‘æ—¥åœ¨å™åˆ©äºšæ‰®æ¼”å’Œäº‹ä½¬çš„è§’è‰²ï¼Œè€Œä¿„ç½—æ–¯ä¸ŽåœŸè€³å…¶å´æ”¯æŒç›¸åæ´¾åˆ«ã€‚',\n",
       "  'token_index': [1, 2, 4, 15, 18],\n",
       "  'top_tokens': ['key', 'one', 'Syria', 'maker', 'Russia']},\n",
       " {'en': \"It could be telling that after almost three hours of initial talks, the two presidents told a press conference that they hadn't even touched on the topic.\",\n",
       "  'zh': 'å¯ä»¥é¢„è§åˆ°çš„æ˜¯ï¼Œåœ¨ç»è¿‡è¿‘ä¸‰ä¸ªå°æ—¶çš„åˆæ­¥è°ˆè¯åŽï¼Œä¸¤ä½æ€»ç»Ÿåœ¨å‘å¸ƒä¼šä¸Šè¡¨ç¤ºï¼Œå°šæœªè°ˆåŠé‚£ä¸ªè¯é¢˜ã€‚',\n",
       "  'token_index': [1, 3, 6, 18, 19],\n",
       "  'top_tokens': ['could', 'telling', 'almost', 'press', 'conference']},\n",
       " {'en': \"Turkey's president deliberately avoided answering a question on their differences, while Mr Putin chose to underline them.\",\n",
       "  'zh': 'åœŸè€³å…¶æ€»ç»Ÿåˆ»æ„å›žé¿å…³äºŽåŒæ–¹åˆ†æ­§çš„é—®é¢˜ï¼Œè€Œæ™®äº¬åˆ™äºˆä»¥å¼ºè°ƒã€‚',\n",
       "  'token_index': [0, 14, 11, 3, 15],\n",
       "  'top_tokens': ['Turkey', 'Mr', 'differences', 'president', 'Putin']},\n",
       " {'en': 'There is no clear consensus on where they can seek common ground on Syria.',\n",
       "  'zh': 'åŒæ–¹å°±å¦‚ä½•åœ¨å™åˆ©äºšé—®é¢˜ä¸Šæ±‚åŒå­˜å¼‚æœªè¾¾æˆæ˜Žç¡®å…±è¯†ã€‚',\n",
       "  'token_index': [9, 10, 11, 4, 3],\n",
       "  'top_tokens': ['seek', 'common', 'ground', 'consensus', 'clear']},\n",
       " {'en': 'But after months of open hostility - and given the potential for utter disaster when Nato member Turkey shot down that Russian fighter jet - it is surely better that the two leaders are at least talking again.',\n",
       "  'zh': 'åœ¨åŒ—å¤§è¥¿æ´‹å…¬çº¦ç»„ç»‡æˆå‘˜å›½åœŸè€³å…¶å‡»è½ä¿„æˆ˜æœºæ‰€å¸¦æ¥çš„æ•°æœˆå…¬å¼€æ•Œå¯¹åŠå¼•å‘å¤§åž‹ç¾éš¾çš„å¯èƒ½ä¸‹ï¼Œä¸¤å›½é¢†å¯¼äººå†æ¬¡é‡å¯å¯¹è¯è‚¯å®šæ˜¯ä»¶å¥½äº‹ã€‚',\n",
       "  'token_index': [2, 4, 5, 12, 10],\n",
       "  'top_tokens': ['months', 'open', 'hostility', 'utter', 'potential']},\n",
       " {'en': 'Royal Bank of Scotland to disappear for customers outside Scotland',\n",
       "  'zh': 'è‹æ ¼å…°çš‡å®¶é“¶è¡Œå°†ä¸å†ä¸ºè‹æ ¼å…°ä»¥å¤–å®¢æˆ·æœåŠ¡',\n",
       "  'token_index': [7, 8, 1, 3, 5],\n",
       "  'top_tokens': ['customers', 'outside', 'Bank', 'Scotland', 'disappear']},\n",
       " {'en': \"The brand RBS is to be reduced to a back office role, according to the bank's chief executive.\",\n",
       "  'zh': 'æ®è‹æ ¼å…°çš‡å®¶é“¶è¡Œé¦–å¸­æ‰§è¡Œå®˜é€éœ²ï¼Œé“¶è¡Œå°†ç®€åŒ–ä¸ºä¸€ä¸ªåŽå‹¤éƒ¨é—¨ã€‚',\n",
       "  'token_index': [1, 14, 2, 12, 17],\n",
       "  'top_tokens': ['brand', 'according', 'RB', 'role', 'bank']},\n",
       " {'en': 'Royal Bank of Scotland will disappear for customers outside Scotland.',\n",
       "  'zh': 'è‹æ ¼å…°çš‡å®¶é“¶è¡Œå°†ä¸å†ä¸ºè‹æ ¼å…°ä»¥å¤–çš„å®¢æˆ·æœåŠ¡ã€‚',\n",
       "  'token_index': [8, 7, 9, 0, 5],\n",
       "  'top_tokens': ['outside', 'customers', 'Scotland', 'Royal', 'disappear']},\n",
       " {'en': \"Ross McEwan told BBC Scotland that the RBS brand was associated with the bank's global ambitions.\",\n",
       "  'zh': 'ç½—æ–¯Â·éº¦å…‹å°¤æ©å‘Šè¯‰è‹±å›½å¹¿æ’­å…¬å¸è‹æ ¼å…°åˆ†éƒ¨ï¼Œè‹æ ¼å…°çš‡å®¶é“¶è¡Œå“ç‰Œæ›¾ç»è‡´åŠ›äºŽå…¶å…¨çƒé›„å¿ƒå£®å¿—ã€‚',\n",
       "  'token_index': [1, 13, 0, 2, 16],\n",
       "  'top_tokens': ['Mc', 'associated', 'Ross', 'E', 'bank']},\n",
       " {'en': 'It has retreated from them since it nearly collapsed eight years ago and had to be bailed out.',\n",
       "  'zh': 'ä½†å…«å¹´å‰æ¿’ä¸´å€’é—­ï¼Œä¸å¾—ä¸æŽ¥å—æ•‘åŠ©ä»Žé‚£æ—¶å¼€å§‹ä¾¿æ”¾å¼ƒäº†é‚£æ ·çš„è¿½æ±‚ã€‚',\n",
       "  'token_index': [2, 3, 12, 11, 9],\n",
       "  'top_tokens': ['retreat', 'ed', 'ago', 'years', 'collapsed']},\n",
       " {'en': \"During that time, brand strategists have used 'RBS' to protect other consumer finance brands.\",\n",
       "  'zh': 'é‚£æ®µæ—¶é—´ï¼Œå“ç‰Œç­–ç•¥å¸ˆä½¿ç”¨â€˜RBSâ€™æ¥ä¿æŠ¤å…¶å®ƒæ¶ˆè´¹é‡‘èžå“ç‰Œã€‚',\n",
       "  'token_index': [2, 17, 4, 13, 19],\n",
       "  'top_tokens': ['time', 'protect', 'brand', 'RB', 'consumer']},\n",
       " {'en': 'It was backed with millions of pounds in sponsorship of international sport, from Six Nations rugby to Wimbledon champion Andy Murray.',\n",
       "  'zh': 'é›†å›¢æ›¾è€—è´¹æ•°ç™¾ä¸‡è‹±é•‘ä»¥èµžåŠ©æ©„æ¦„çƒå…­å›½èµ›å’Œæ¸©å¸ƒå°”é¡¿ç½‘çƒé”¦æ ‡èµ›å† å†›å®‰è¿ªÂ·ç©†é›·ç­‰æ–¹å¼èµžåŠ©å›½é™…ä½“è‚²èµ›äº‹ã€‚',\n",
       "  'token_index': [2, 4, 8, 16, 14],\n",
       "  'top_tokens': ['backed', 'millions', 'sponsorship', 'rug', 'Six']},\n",
       " {'en': 'But now, it has been judged right to let more national brands come to the fore.',\n",
       "  'zh': 'ä½†æ˜¯çŽ°åœ¨ï¼Œæ­£ç¡®çš„åšæ³•æ˜¯è®©æ›´å¤šçš„å›½å®¶å“ç‰Œå´­éœ²å¤´è§’ã€‚',\n",
       "  'token_index': [12, 11, 14, 9, 7],\n",
       "  'top_tokens': ['brand', 'national', 'come', 'let', 'right']},\n",
       " {'en': 'Royal Bank of Scotland will be used with Scottish customers, but will not be initialised.',\n",
       "  'zh': 'è‹æ ¼å…°çš‡å®¶é“¶è¡Œå°†ä¸ºè‹æ ¼å…°å®¢æˆ·æ‰€ç”¨ï¼Œä½†ä¸ä¼šå›žå¤åŽŸæ ·ã€‚',\n",
       "  'token_index': [9, 0, 1, 8, 3],\n",
       "  'top_tokens': ['customers', 'Royal', 'Bank', 'Scottish', 'Scotland']},\n",
       " {'en': 'In England and Wales, all RBS references, outside head office and the stock exchange listing, will be changed to NatWest.',\n",
       "  'zh': 'åœ¨è‹±æ ¼å…°å’Œå¨å°”å£«ï¼Œæ‰€æœ‰åœ¨æ€»éƒ¨åŠè¯åˆ¸äº¤æ˜“æ‰€ä»¥å¤–çš„ææ³•å°†æ›´æ”¹ä¸ºå›½æ°‘è¥¿æ•å¯ºé“¶è¡Œã€‚',\n",
       "  'token_index': [1, 3, 17, 16, 15],\n",
       "  'top_tokens': ['England', 'Wales', 'listing', 'exchange', 'stock']},\n",
       " {'en': 'The Ulster Bank brand is already used for customers in the Republic of Ireland and Northern Ireland.',\n",
       "  'zh': 'é˜¿å°”æ–¯ç‰¹é“¶è¡Œå“ç‰Œå·²ç”¨äºŽçˆ±å°”å…°å…±å’Œå›½ä¸ŽåŒ—çˆ±å°”å…°è‡ªæ²»åŒºçš„å®¢æˆ·ã€‚',\n",
       "  'token_index': [12, 1, 9, 14, 2],\n",
       "  'top_tokens': ['Republic', 'Ul', 'customers', 'Ireland', 'ster']},\n",
       " {'en': \"There are other, smaller brands for private banking, which will get more prominence - Coutts, Adam & Co, Drummond, and Holt's Military Bank.\",\n",
       "  'zh': \"è¿˜æœ‰ä¸€äº›å…¶å®ƒæ›´çªå‡ºçš„è¾ƒå°ç§äººé“¶è¡Œå“ç‰Œï¼Œä¾‹å¦‚é¡¾èµ„é“¶è¡Œã€Adam & Coã€ Drummond ä¸Ž Holt's Military Bankã€‚\",\n",
       "  'token_index': [8, 9, 4, 5, 13],\n",
       "  'top_tokens': ['private', 'banking', 'smaller', 'brand', 'get']},\n",
       " {'en': 'Mr McEwan was interviewed during a tour of customers and staff in Inverness-shire.',\n",
       "  'zh': 'éº¦å…‹å°¤æ©å…ˆç”Ÿæ›¾åœ¨å› å¼—å†…æ–¯éƒ¡å®¢æˆ·ä¸Žå‘˜å·¥çš„å‚è§‚ä¹‹æ—…ä¸­æŽ¥å—é‡‡è®¿ã€‚',\n",
       "  'token_index': [0, 1, 2, 12, 3],\n",
       "  'top_tokens': ['Mr', 'Mc', 'E', 'staff', 'wan']},\n",
       " {'en': 'He told BBC Scotland: \"The RBS brand will end up becoming our investor brand and the one that our staff are employed with, because we are now becoming much more a bank of brands.\"',\n",
       "  'zh': 'ä»–å‘Šè¯‰è‹±å›½å¹¿æ’­å…¬å¸è‹æ ¼å…°åˆ†éƒ¨è¯´ï¼šâ€œè‹æ ¼å…°çš‡å®¶é“¶è¡Œå“ç‰Œå°†æœ€ç»ˆæˆä¸ºæˆ‘ä»¬çš„æŠ•èµ„è€…å“ç‰Œï¼ŒåŒæ—¶ä¹Ÿæ˜¯å‘˜å·¥å—é›‡å“ç‰Œä¹‹ä¸€ï¼Œå› ä¸ºç›®å‰æ›´å¤§æ„ä¹‰ä¸Šæˆ‘ä»¬å·²æˆä¸ºä¸€ä¸ªæ‹¥æœ‰ä¼—å¤šå“ç‰Œçš„é“¶è¡Œã€‚â€',\n",
       "  'token_index': [2, 3, 7, 24, 1],\n",
       "  'top_tokens': ['BBC', 'Scotland', 'RB', 'employed', 'told']},\n",
       " {'en': 'As the bank itself became a global brand, RBS became the global brand.',\n",
       "  'zh': 'ç”±äºŽè¯¥é“¶è¡Œæœ¬èº«æˆä¸ºä¸€ä¸ªå›½é™…å“ç‰Œï¼ŒRBSä¹Ÿæˆä¸ºå…¨çƒå“ç‰Œã€‚',\n",
       "  'token_index': [9, 11, 2, 7, 4],\n",
       "  'top_tokens': ['RB', 'became', 'bank', 'brand', 'became']},\n",
       " {'en': \"I'm now saying we no longer have global aspirations, we have local aspirations.\",\n",
       "  'zh': 'ç›®å‰ï¼Œæˆ‘è¦è¯´çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸å†é‡ç‚¹å…³æ³¨å…¨çƒï¼Œæˆ‘ä»¬å·²å°†ç›®å…‰è½¬å‘æœ¬åœŸã€‚',\n",
       "  'token_index': [10, 9, 7, 4, 14],\n",
       "  'top_tokens': ['aspirations', 'global', 'longer', 'saying', 'local']},\n",
       " {'en': 'Each one of those brands will stand for something quite different in their own communities, and our staff will work with customers under those brands.',\n",
       "  'zh': 'æ¯ä¸€ä¸ªå“ç‰Œä»£è¡¨ç€å„è‡ªæ‰€åœ¨è¡Œä¸šçš„ä¸åŒï¼Œè€Œæˆ‘ä»¬çš„å‘˜å·¥é€šè¿‡ä¸åŒå“ç‰Œä¸ºå®¢æˆ·æœåŠ¡ã€‚',\n",
       "  'token_index': [1, 4, 19, 15, 7],\n",
       "  'top_tokens': ['one', 'brand', 'staff', 'communities', 'stand']},\n",
       " {'en': 'RBS had already stated that it would not to continue its Six Nations sponsorship, and it has been raising the profile of different brands in its sports sponsorship.',\n",
       "  'zh': 'è‹æ ¼å…°çš‡å®¶é“¶è¡Œå·²è¡¨æ˜Žä¸ä¼šç»§ç»­èµžåŠ©å…­å›½é”¦æ ‡èµ›ï¼›åŒæ—¶ï¼Œå·²åŠªåŠ›æå‡ä¸åŒå“ç‰Œåœ¨ä½“è‚²èµ›äº‹èµžåŠ©çš„å½¢è±¡ã€‚',\n",
       "  'token_index': [0, 20, 22, 3, 4],\n",
       "  'top_tokens': ['RB', 'raising', 'profile', 'already', 'stated']},\n",
       " {'en': '\"The time is right for us to move to the bank of brands, because underneath (we\\'ve been asking) how do we focus on making this a better bank for customers?\" said the chief executive.',\n",
       "  'zh': 'é¦–å¸­æ‰§è¡Œå®˜è¡¨ç¤ºï¼šâ€œçŽ°åœ¨åˆ°äº†æˆä¸ºä¸€å®¶å¤šå“ç‰Œé“¶è¡Œçš„æ—¶å€™ï¼Œå› ä¸ºç§ä¸‹æˆ‘ä»¬ä¸€ç›´åœ¨æŽ¢æ±‚å¦‚ä½•æˆä¸ºä¸€å®¶æ›´å¥½çš„æœåŠ¡äºŽå®¢æˆ·çš„é“¶è¡Œã€‚â€',\n",
       "  'token_index': [2, 4, 8, 6, 11],\n",
       "  'top_tokens': ['time', 'right', 'move', 'us', 'bank']},\n",
       " {'en': \"It would have been very cynical three years ago if we'd said we're going to be a great bank for customers and put those brands out there.\",\n",
       "  'zh': 'å¦‚æžœä¸‰å¹´å‰æˆ‘ä»¬ç«‹å¿—æˆä¸ºä¸€å®¶æ‹¥æœ‰ä¼—å¤šå“ç‰Œçš„å¤§é“¶è¡Œå°†å¤‡å—å˜²è®½ã€‚',\n",
       "  'token_index': [1, 5, 22, 21, 6],\n",
       "  'top_tokens': ['would', 'cynical', 'bank', 'great', 'three']},\n",
       " {'en': \"But with the work we've been doing, focussing on the customers needs, not our own, I think you're seeing a lot of change.\",\n",
       "  'zh': 'ç›®å‰æˆ‘ä»¬é‡ç‚¹å…³æ³¨å®¢æˆ·éœ€æ±‚ï¼Œå¹¶ä¸”æˆ‘ç›¸ä¿¡æ‚¨å·²æ³¨æ„åˆ°äº†è¿™äº›å˜åŒ–ã€‚',\n",
       "  'token_index': [3, 10, 23, 12, 15],\n",
       "  'top_tokens': ['work', 'focus', 'think', 'ing', 'customers']},\n",
       " {'en': 'We can bring those brands back up again, so I think the time is right.',\n",
       "  'zh': 'æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥é‡æ–°å¯åŠ¨è¿™äº›å“ç‰Œï¼Œè€Œä¸”çŽ°åœ¨æ—¶é—´æ­£åˆé€‚ã€‚',\n",
       "  'token_index': [12, 2, 14, 4, 6],\n",
       "  'top_tokens': ['think', 'bring', 'time', 'brand', 'back']}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = count_pos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def find_tokenidx(token, sentence_lst):\n",
    "  for i in range(len(sentence_lst)):\n",
    "    if token.lower() in sentence_lst[i].lower():\n",
    "      return i\n",
    "def token_vec_sum(text, word):\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = bert_tokenizer.tokenize(marked_text)\n",
    "  index = find_tokenidx(word, tokenized_text)\n",
    "  print(index)\n",
    "  print(tokenized_text)\n",
    "  indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                    )\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      outputs = model(tokens_tensor, segments_tensors)\n",
    "      hidden_states = outputs[2]\n",
    "      token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "      token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "      token_embeddings = token_embeddings.permute(1,0,2)\n",
    "      token_vecs_sum = []\n",
    "      for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "  return token_vecs_sum[index]\n",
    "def similarity(origin, mutant, word, mutant_word):\n",
    "  \n",
    "  origin_sum, mutant_sum =  token_vec_sum(origin, word), token_vec_sum(mutant, mutant_word)\n",
    "  return 1 - cosine(origin_sum, mutant_sum)\n",
    "      \n",
    "  \n",
    "def mutant_pairs(examples):\n",
    "  pairs=[]\n",
    "  inputs = [ex[\"en\"] for ex in examples]\n",
    "  for i, input in enumerate(inputs):\n",
    "    input_list = input.split()\n",
    "    tokens = examples[i]['top_tokens']\n",
    "    #print(tokens)\n",
    "    token_idx = find_tokenidx(tokens[0], input_list)\n",
    "    #print(input_list[token_idx])\n",
    "    word = input_list[token_idx]\n",
    "\n",
    "    input_list[token_idx] = \"[MASK]\"\n",
    "    candidate = unmasker(\" \".join(input_list))\n",
    "\n",
    "    mutants = []\n",
    "    for c in candidate[:3]:\n",
    "      mutant = input_list.copy()\n",
    "      mutant[token_idx] = c[\"token_str\"]\n",
    "      \n",
    "      mutant_sent = \" \".join(mutant)\n",
    "      # if similarity(input, mutant_sent, word, c[\"token_str\"]) >=0.9:\n",
    "      mutants.append(\" \".join(mutant))\n",
    "    pairs.append({'en': input, 'mutants': mutants})\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': '28-Year-Old Chef Found Dead at San Francisco Mall',\n",
       "  'mutants': ['28-Year-Old Chef Found work at San Francisco Mall',\n",
       "   '28-Year-Old Chef Found himself at San Francisco Mall',\n",
       "   '28-Year-Old Chef Found food at San Francisco Mall']},\n",
       " {'en': 'A 28-year-old chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "  'mutants': ['A young chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "   'A french chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "   'A local chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.']},\n",
       " {'en': 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "  'mutants': ['But the older brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "   'But the younger brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "   'But the other brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"']},\n",
       " {'en': \"The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "  'mutants': [\"The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "   \"The man found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "   \"The person found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\"]},\n",
       " {'en': 'The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.',\n",
       "  'mutants': ['The San Francisco Police Department said the death was ruled a suicide and an investigation is ongoing.',\n",
       "   'The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.',\n",
       "   'The San Francisco Police Department said the death was ruled a murder and an investigation is ongoing.']},\n",
       " {'en': \"The victim's brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "  'mutants': [\"The youngest brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "   \"The younger brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "   \"The older brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\"]},\n",
       " {'en': 'A spokesperson for Sons & Daughters said they were \"shocked and devastated\" by his death.',\n",
       "  'mutants': ['A spokesperson for Sons & Daughters said they were \" and devastated\" by his death.',\n",
       "   'A spokesperson for Sons & Daughters said they were shocked and devastated\" by his death.',\n",
       "   'A spokesperson for Sons & Daughters said they were surprised and devastated\" by his death.']},\n",
       " {'en': '\"We are a small team that operates like a close knit family and he will be dearly missed,\" the spokesperson said.',\n",
       "  'mutants': ['\"We are a small team that operates like a close knit family and he will be easily missed,\" the spokesperson said.',\n",
       "   '\"We are a small team that operates like a close knit family and he will be greatly missed,\" the spokesperson said.',\n",
       "   '\"We are a small team that operates like a close knit family and he will be much missed,\" the spokesperson said.']},\n",
       " {'en': \"Our thoughts and condolences are with Frank's family and friends at this difficult time.\",\n",
       "  'mutants': [\"Our apologies and condolences are with Frank's family and friends at this difficult time.\",\n",
       "   \"Our prayers and condolences are with Frank's family and friends at this difficult time.\",\n",
       "   \"Our thoughts and condolences are with Frank's family and friends at this difficult time.\"]},\n",
       " {'en': 'Louis Galicia said Frank initially stayed in hostels, but recently, \"Things were finally going well for him.\"',\n",
       "  'mutants': ['Louis Galicia said Frank initially stayed in hostels, but recently, things were finally going well for him.\"',\n",
       "   'Louis Galicia said Frank initially stayed in hostels, but recently, they were finally going well for him.\"',\n",
       "   'Louis Galicia said Frank initially stayed in hostels, but recently, \" were finally going well for him.\"']}]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutant_pairs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "def sentence_sim(text1, text2):\n",
    "    origin = model.encode(text1)\n",
    "    mutant = model.encode(text2)\n",
    "    return cosine_similarity(\n",
    "    [origin],\n",
    "    [mutant])[0][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c818903a972b15f948093850adeb30c84eba2d54a31a843f1ceb959d2841b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
