{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/hanyings/conda_envs/tnmt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset wmt17 (/data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953)\n",
      "100%|██████████| 3/3 [00:36<00:00, 12.11s/it]\n",
      "/tmp/ipykernel_3385/2303173043.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"wmt17\", \"zh-en\", cache_dir=\"/data2/hanyings/.cache\")\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 18:44:07.488185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 18:44:12.927504: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
      "2022-12-12 18:44:12.927881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n",
      "2022-12-12 18:44:12.927890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline, MarianTokenizer, MarianTokenizer, TFMarianMTModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 25134743\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2002\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = raw_datasets[\"train\"].train_test_split(test_size=1)\n",
    "raw_datasets[\"train\"] = split[\"test\"]\n",
    "raw_datasets[\"validation\"] = split[\"test\"]\n",
    "raw_datasets[\"test\"] = split[\"test\"]\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = \"\"\n",
    "# max_input_length = 128\n",
    "# max_target_length = 128\n",
    "# source_lang = \"en\"\n",
    "# target_lang = \"zh\"\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "#     targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "#     # Setup the tokenizer for targets\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data2/hanyings/hf_transformer/tranformer_gradient.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m preprocess \u001b[39m=\u001b[39m preprocess_function(raw_datasets[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m preprocess\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_function' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess = preprocess_function(raw_datasets['train'][:1])\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 171.79ba/s]\n",
      "Loading cached processed dataset at /data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953/cache-7ffeacd63e3c9838.arrow\n",
      "Loading cached processed dataset at /data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953/cache-7ffeacd63e3c9838.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': '(c) Encourage and support participatory research with street-connected children and families to inform policy-making and design of specialized interventions.',\n",
       "  'zh': '鼓励和支助围绕与街头有联系的儿童及其家人开展参与型研究，为制定政策和设计专门干预措施提供信息。'},\n",
       " 'input_ids': [22,\n",
       "  149,\n",
       "  17,\n",
       "  20327,\n",
       "  7,\n",
       "  125,\n",
       "  10252,\n",
       "  1304,\n",
       "  29,\n",
       "  8979,\n",
       "  16,\n",
       "  56308,\n",
       "  238,\n",
       "  7,\n",
       "  2106,\n",
       "  9,\n",
       "  3842,\n",
       "  419,\n",
       "  16,\n",
       "  2516,\n",
       "  7,\n",
       "  3634,\n",
       "  4,\n",
       "  2372,\n",
       "  6529,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [5078,\n",
       "  12103,\n",
       "  13169,\n",
       "  67,\n",
       "  21047,\n",
       "  25153,\n",
       "  568,\n",
       "  28152,\n",
       "  589,\n",
       "  467,\n",
       "  3517,\n",
       "  751,\n",
       "  2,\n",
       "  76,\n",
       "  25892,\n",
       "  18,\n",
       "  3808,\n",
       "  2728,\n",
       "  15031,\n",
       "  12224,\n",
       "  10,\n",
       "  0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenized_datasets[\"train\"][0][\"input_ids\"][1]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0029,  0.0093, -0.0038,  ..., -0.0296, -0.0405, -0.0319],\n",
       "        [ 0.0101,  0.0050,  0.0023,  ..., -0.0351, -0.0636, -0.0068],\n",
       "        [ 0.0057,  0.0075,  0.0078,  ..., -0.0462, -0.0066, -0.0438],\n",
       "        ...,\n",
       "        [ 0.0026,  0.0119, -0.0073,  ..., -0.0304, -0.0043, -0.0533],\n",
       "        [-0.0153,  0.0381, -0.0026,  ..., -0.0481, -0.0228, -0.0146],\n",
       "        [ 0.0114,  0.0352, -0.0117,  ..., -0.0167, -0.0142, -0.0327]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "old_embed = model.get_encoder().embed_tokens(torch.tensor(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
    "old_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data2/hanyings/hf_transformer/tranformer_gradient.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     result \u001b[39m=\u001b[39m {k: \u001b[39mround\u001b[39m(v, \u001b[39m4\u001b[39m) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     args,\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bomen.encs.concordia.ca/data2/hanyings/hf_transformer/tranformer_gradient.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"./tmp_trainer\",\n",
    "    evaluation_strategy = \"epoch\", # evaluate on valid dataset at emd pf each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    metric_for_best_model = \"bleu\",  #Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "    # bleu in the compute metric ?   \n",
    "    save_strategy=\"no\"\n",
    "    \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import numpy as np\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, tokenize=\"zh\",smooth_method=\"add-k\")\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0144e-02,  5.0447e-03,  2.2518e-03, -1.7374e-02,  2.7206e-02,\n",
       "          1.6035e-02, -1.3437e-02, -1.8341e-02,  3.6228e-02, -2.5258e-02,\n",
       "         -1.5547e-02, -8.9262e-04, -2.0592e-02,  1.1161e-02,  7.9412e-03,\n",
       "         -4.9970e-03, -2.5273e-02, -1.9579e-02, -3.4272e-02, -1.3408e-02,\n",
       "         -2.3376e-03, -2.8470e-02, -1.8016e-02,  5.9842e-04,  2.8821e-03,\n",
       "          2.2967e-03, -1.5486e-02,  1.0980e-01,  7.5325e-03,  3.2127e-03,\n",
       "         -8.1589e-02, -1.0854e-02, -1.1434e-02,  1.8830e-02, -6.8241e-03,\n",
       "          6.0114e-03, -3.5065e-02, -7.3030e-02,  2.9641e-02, -1.3873e-02,\n",
       "          2.6968e-02, -3.1924e-02, -2.0414e-05, -2.8745e-02, -1.6473e-02,\n",
       "         -1.7155e-02,  1.2791e-02,  3.6385e-02, -4.6023e-02, -7.3509e-02,\n",
       "          3.4076e-02, -2.7510e-02, -7.1822e-02,  5.6939e-03,  1.2389e-02,\n",
       "         -1.3336e-02, -1.4755e-02,  1.6470e-02, -2.7795e-02, -3.4384e-02,\n",
       "         -3.8219e-02, -2.6174e-02, -3.0673e-02, -1.9236e-02, -2.7159e-03,\n",
       "          2.3940e-03,  1.1802e-01, -2.5492e-02, -9.2612e-03, -7.1148e-03,\n",
       "          1.4022e-01, -2.8818e-02, -1.4391e-02, -3.4423e-02, -6.0696e-03,\n",
       "         -1.2687e-02, -3.5247e-02, -3.0287e-02, -7.9636e-03,  2.6026e-03,\n",
       "         -4.4743e-02, -1.1297e-02,  2.2631e-02, -2.1105e-02, -1.5501e-02,\n",
       "          1.1369e-03, -1.7882e-02,  1.6888e-02, -3.1032e-02, -3.6135e-02,\n",
       "         -1.7765e-02, -7.8464e-03, -1.4416e-02, -8.0215e-02,  1.2121e-02,\n",
       "         -7.2677e-02,  1.0158e-02, -3.9664e-02,  2.6785e-02, -6.7036e-03,\n",
       "         -9.9276e-03, -2.8247e-02,  2.5079e-02,  4.1802e-02, -4.1905e-02,\n",
       "         -1.9546e-02, -1.5264e-02,  2.3602e-02, -1.4398e-02, -3.5282e-02,\n",
       "         -4.6212e-02,  1.8887e-02,  5.7713e-03, -4.7070e-02,  4.4385e-02,\n",
       "          1.5856e-02,  4.6985e-03, -3.9593e-02, -5.7036e-02, -1.8022e-02,\n",
       "          1.4588e-02,  1.9499e-02,  2.6011e-02, -3.7851e-02,  1.9428e-03,\n",
       "         -1.0478e-02, -1.3784e-01,  2.4836e-03, -1.2924e-02,  4.4306e-03,\n",
       "          4.8342e-03, -1.9531e-02,  1.0859e-02,  4.2091e-02, -8.9936e-03,\n",
       "         -7.5945e-03, -3.6594e-02, -1.1918e-02,  2.7858e-02,  6.5688e-03,\n",
       "         -3.9132e-03, -5.2792e-02,  8.0137e-03, -3.8845e-02, -3.7652e-02,\n",
       "         -2.0016e-02, -1.5774e-02,  1.0347e-02, -1.0737e-02, -8.2504e-03,\n",
       "         -9.9088e-02, -1.3425e-02,  2.1717e-02,  1.9104e-02, -8.1028e-02,\n",
       "         -1.2665e-02,  3.0977e-02,  4.2822e-03, -2.8734e-03, -4.1488e-03,\n",
       "         -1.2540e-02,  4.7409e-02, -2.1923e-02, -3.0786e-02, -1.1249e-02,\n",
       "         -3.3386e-02,  1.3177e-02, -1.5061e-02, -5.2403e-02,  7.2520e-04,\n",
       "         -4.0260e-03, -8.4852e-03,  6.0007e-04,  7.7135e-03,  7.4816e-03,\n",
       "          5.7486e-04, -4.2414e-03, -9.0401e-05, -4.2420e-02,  3.8312e-02,\n",
       "          5.9253e-03,  5.9632e-03, -1.3232e-02, -1.0571e-02, -3.9798e-02,\n",
       "         -1.4132e-02,  2.0193e-02, -1.4147e-03, -7.2251e-03, -3.8267e-02,\n",
       "         -3.5801e-02, -3.7000e-03,  7.5324e-03, -1.1646e-02, -2.4298e-02,\n",
       "         -3.4766e-02,  2.9306e-03, -2.3712e-02,  1.7135e-02, -1.8196e-02,\n",
       "          3.4036e-02, -1.2833e-02, -6.5631e-04, -1.9171e-02,  3.5188e-03,\n",
       "          8.9714e-04, -2.1791e-02,  2.8418e-02,  1.7828e-03,  2.4052e-02,\n",
       "         -1.6332e-02, -1.9221e-02, -3.2166e-02, -9.3802e-03,  2.5117e-03,\n",
       "         -2.5607e-02,  1.5074e-03, -2.0983e-02, -1.0713e-02,  2.3681e-02,\n",
       "          3.7187e-03,  5.1689e-02,  1.7011e-02, -4.6576e-02, -2.1775e-02,\n",
       "          3.7017e-02, -2.7272e-02,  2.8393e-02, -1.3220e-02, -1.9342e-02,\n",
       "         -1.2248e-04,  1.2896e-02,  4.9169e-02, -2.3413e-02, -5.7153e-03,\n",
       "         -2.1099e-02, -8.5127e-05, -3.8823e-03,  1.4314e-02, -7.5083e-03,\n",
       "         -2.2582e-02,  1.2176e-02, -2.3239e-02, -2.5366e-02, -1.3950e-02,\n",
       "         -2.1838e-02,  5.4910e-03,  4.1100e-02, -2.0033e-02, -6.2552e-03,\n",
       "         -5.8284e-03, -7.9814e-04,  9.5206e-03, -2.0854e-02, -3.9550e-02,\n",
       "         -1.7424e-02, -2.5270e-02, -1.4128e-02,  2.0236e-03,  1.3963e-03,\n",
       "         -4.6856e-03,  8.2189e-03, -3.2945e-03,  2.8958e-03,  2.1670e-03,\n",
       "         -1.1171e-01, -2.2781e-02,  6.7681e-03,  1.1717e-02, -8.7468e-03,\n",
       "         -2.1943e-03,  2.4086e-02, -1.8727e-02, -2.2616e-02,  2.0182e-02,\n",
       "         -2.2768e-02, -9.4120e-03, -1.8339e-02,  3.0388e-02, -2.9767e-02,\n",
       "          4.0143e-02, -1.5286e-02,  5.6731e-02, -3.1053e-03, -1.2375e-02,\n",
       "         -4.3095e-02,  2.8043e-02, -1.6439e-02,  2.0403e-02, -1.0477e-02,\n",
       "         -2.8225e-02,  1.1060e-02,  2.5165e-03,  4.8313e-02,  2.1227e-02,\n",
       "         -1.6243e-02,  3.4914e-02, -1.3750e-02, -9.0781e-03, -4.9718e-02,\n",
       "         -3.7223e-02,  2.3713e-02,  1.1352e-03,  3.2946e-03, -4.7943e-02,\n",
       "         -2.4482e-03, -1.0544e-02,  1.0671e-02,  1.2282e-02,  1.1301e-02,\n",
       "          3.9140e-02,  9.5448e-03, -3.2689e-02, -1.7156e-02, -4.3522e-04,\n",
       "         -1.8948e-02,  1.5231e-02,  1.2044e-02, -2.0130e-02, -3.7036e-02,\n",
       "          6.9039e-03,  4.6164e-03, -1.3590e-02,  3.4360e-02, -4.1272e-03,\n",
       "         -5.1465e-02,  3.3865e-02, -3.5306e-02,  3.1607e-02, -1.2925e-02,\n",
       "         -4.4976e-02, -3.9552e-02,  4.1883e-02, -3.4554e-02, -2.5847e-02,\n",
       "         -2.5894e-02,  4.7220e-03,  1.7151e-02,  4.2185e-02,  6.4257e-02,\n",
       "         -2.7060e-02, -4.1588e-02, -3.7941e-02, -2.2800e-02, -7.4876e-02,\n",
       "         -5.2146e-03, -2.0583e-02, -5.5267e-03, -3.7849e-02, -3.9694e-02,\n",
       "         -3.1545e-02, -2.2449e-02,  1.9640e-02, -1.3358e-02, -5.3818e-02,\n",
       "         -1.7025e-02, -3.0966e-02, -3.6936e-02, -3.1892e-02,  5.8818e-03,\n",
       "         -3.5266e-02, -1.5223e-02, -1.5426e-02, -4.6621e-02,  6.7564e-06,\n",
       "         -6.3605e-02, -2.0153e-02, -1.4933e-02, -3.5448e-02, -4.9301e-02,\n",
       "         -1.0240e-01, -7.2155e-02, -4.5576e-02, -5.2396e-02, -1.8503e-02,\n",
       "         -3.4641e-02, -5.0109e-02, -1.5780e-02, -1.0548e-02,  2.2084e-02,\n",
       "         -3.9116e-02, -5.8432e-02,  3.6269e-02,  8.6668e-02,  6.9612e-03,\n",
       "         -5.3569e-02, -1.4835e-02,  2.6466e-02, -4.8096e-02,  1.8791e-03,\n",
       "         -8.2218e-02,  1.3346e-02, -4.7724e-02, -3.4380e-02, -7.7694e-03,\n",
       "         -5.1997e-02, -6.9080e-02, -3.4150e-02, -3.4739e-02, -3.4111e-02,\n",
       "         -7.8168e-02, -1.0128e-02, -3.2493e-02, -7.4757e-02, -5.7266e-03,\n",
       "         -5.3008e-02, -3.6866e-03, -3.2419e-02, -4.8439e-02, -2.3148e-02,\n",
       "         -4.1254e-02, -3.2640e-02, -4.7205e-02, -7.7067e-02, -4.0622e-02,\n",
       "         -4.0189e-02, -2.7122e-02, -2.8383e-02, -6.5369e-02, -1.7008e-02,\n",
       "         -3.3952e-02, -2.8662e-02, -2.1175e-02, -7.7075e-02, -2.3717e-02,\n",
       "         -3.8173e-02, -6.3327e-02,  1.1757e-02, -2.2832e-02, -3.3648e-02,\n",
       "         -1.9979e-03,  1.1446e-02,  1.9439e-02,  1.7276e-02,  3.9171e-03,\n",
       "          9.0313e-03, -3.4779e-02, -1.6778e-02, -5.7317e-02, -5.6195e-02,\n",
       "          6.0527e-03, -3.1159e-02,  2.3826e-02, -4.2355e-02, -1.9771e-02,\n",
       "         -4.8552e-02, -4.6355e-02, -1.6896e-02, -1.5037e-02, -5.2866e-02,\n",
       "         -4.2994e-02, -1.7111e-02, -4.1910e-02, -4.7766e-02, -3.2400e-02,\n",
       "         -8.0628e-02, -2.5397e-02, -4.4414e-02, -1.0951e-02, -9.3462e-03,\n",
       "         -9.2441e-03, -3.9443e-02, -3.2005e-02, -3.1103e-02, -5.5955e-02,\n",
       "         -1.2078e-02, -5.1119e-03,  1.2339e-02, -4.6944e-02, -2.7991e-02,\n",
       "         -7.8758e-03, -5.6421e-02, -4.5338e-03, -4.1237e-02,  4.8806e-03,\n",
       "          4.9462e-03, -3.7349e-02, -3.5336e-02, -3.3513e-02, -3.6708e-02,\n",
       "         -1.2737e-02, -5.8968e-02, -2.3300e-02, -5.7048e-02, -8.6473e-02,\n",
       "         -5.1932e-02, -5.9022e-02,  1.6152e-02, -2.5908e-02, -2.6014e-02,\n",
       "         -2.9393e-02, -5.2143e-02, -2.8282e-02, -6.7060e-02, -5.7476e-02,\n",
       "         -4.8307e-02, -4.6125e-02, -4.5338e-02, -2.6244e-02,  3.0352e-02,\n",
       "         -5.4137e-02, -6.0205e-02, -2.4088e-02, -3.1778e-02,  4.9711e-02,\n",
       "         -4.8104e-02,  4.0377e-02, -3.8231e-02, -2.0145e-02, -3.5142e-02,\n",
       "         -6.3647e-02, -6.8191e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "new_embed = model.get_encoder().embed_tokens(torch.tensor([token]))\n",
    "new_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999964237213135"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "1 - cosine(old_embed[0].detach().numpy(), new_embed[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.arrow_dataset import Dataset\n",
    "Dataset.from_dict(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions wrapup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "Found cached dataset wmt17 (/data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953)\n",
      "100%|██████████| 3/3 [00:00<00:00, 69.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline, MarianTokenizer, MarianTokenizer, TFMarianMTModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name, return_tensors=\"pt\")\n",
    "from datasets import load_dataset, load_metric\n",
    "raw_datasets = load_dataset(\"wmt17\", \"zh-en\", cache_dir=\"/data2/hanyings/.cache\")\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "from datasets.arrow_dataset import Dataset\n",
    "prefix = \"\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"translation\"] = examples[\"translation\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "def preprocess_dataset(num_ex):\n",
    "    raw_datasets = load_dataset(\"wmt17\", \"zh-en\", cache_dir=\"/data2/hanyings/.cache\")   \n",
    "    preprocess = preprocess_function(raw_datasets['train'][num_ex:num_ex+1])\n",
    "    return Dataset.from_dict(preprocess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt17 (/data2/hanyings/.cache/wmt17/zh-en/1.0.0/2d49e0ac9500439706ca425bb2059f0db0d024ab28ca19b0b64fc0030a714953)\n",
      "100%|██████████| 3/3 [00:00<00:00, 78.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'translation'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = preprocess_dataset(7)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2666,\n",
       "  1306,\n",
       "  2,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  2793,\n",
       "  4,\n",
       "  6035,\n",
       "  1116,\n",
       "  2229,\n",
       "  49763,\n",
       "  66,\n",
       "  2535,\n",
       "  9,\n",
       "  146,\n",
       "  29,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  23396,\n",
       "  17878,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3952,\n",
       "  2,\n",
       "  2905,\n",
       "  3649,\n",
       "  7370,\n",
       "  11801,\n",
       "  3921,\n",
       "  16562,\n",
       "  18,\n",
       "  18332,\n",
       "  11360,\n",
       "  12,\n",
       "  3921,\n",
       "  34390,\n",
       "  5145,\n",
       "  1720,\n",
       "  10,\n",
       "  0],\n",
       " 'translation': {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'zh': '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline, MarianTokenizer, MarianTokenizer, TFMarianMTModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "def get_old_embed(dataset):\n",
    "    mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "    model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "    return model.get_encoder().embed_tokens(torch.tensor(dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed = get_old_embed(dataset)\n",
    "old_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0153, -0.0038,  0.0085,  ..., -0.0577, -0.0065, -0.0415],\n",
       "        [-0.0123, -0.0079, -0.0223,  ..., -0.0177, -0.0312, -0.0419],\n",
       "        [ 0.0114,  0.0256,  0.0116,  ..., -0.0563, -0.0548, -0.0277],\n",
       "        ...,\n",
       "        [ 0.0466, -0.0057,  0.0314,  ..., -0.0157, -0.0185, -0.0190],\n",
       "        [-0.0154,  0.0380, -0.0025,  ..., -0.0482, -0.0226, -0.0147],\n",
       "        [ 0.0114,  0.0351, -0.0119,  ..., -0.0167, -0.0144, -0.0326]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "def retrain(dataset):\n",
    "    mode_name = '/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000'\n",
    "    model =AutoModelForSeq2SeqLM.from_pretrained(mode_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mode_name, return_tensors=\"pt\")\n",
    "    \n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        \"./tmp_trainer\",\n",
    "        evaluation_strategy = \"epoch\", # evaluate on valid dataset at emd pf each epoch\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=10,\n",
    "        predict_with_generate=True,\n",
    "        metric_for_best_model = \"bleu\",  #Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "        # bleu in the compute metric ?   \n",
    "        save_strategy=\"no\"\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    import numpy as np\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        return preds, labels\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, tokenize=\"zh\",smooth_method=\"add-k\")\n",
    "        result = {\"bleu\": result[\"score\"]}\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        return result\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353989</td>\n",
       "      <td>51.321600</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227323</td>\n",
       "      <td>67.208600</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.150327</td>\n",
       "      <td>89.820700</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.103348</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.071431</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.052304</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.040667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033638</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.029591</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.027723</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = retrain(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_embed(model, dataset):\n",
    "    model.to(\"cpu\")\n",
    "    return model.get_encoder().embed_tokens(torch.tensor(dataset[0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0153, -0.0037,  0.0086,  ..., -0.0578, -0.0064, -0.0415],\n",
       "        [-0.0124, -0.0079, -0.0223,  ..., -0.0178, -0.0311, -0.0419],\n",
       "        [ 0.0115,  0.0256,  0.0115,  ..., -0.0563, -0.0549, -0.0276],\n",
       "        ...,\n",
       "        [ 0.0467, -0.0057,  0.0314,  ..., -0.0156, -0.0185, -0.0190],\n",
       "        [-0.0153,  0.0381, -0.0025,  ..., -0.0482, -0.0226, -0.0147],\n",
       "        [ 0.0113,  0.0350, -0.0119,  ..., -0.0167, -0.0144, -0.0325]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embed = get_new_embed(model, dataset)\n",
    "new_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_sim(dataset, old_embed, new_embed):\n",
    "    output = dataset[0]\n",
    "    output[\"cosine_sim\"] = []\n",
    "    for i in range(len(old_embed)):\n",
    "        cos = 1 - cosine(old_embed[i].detach().numpy(), new_embed[i].detach().numpy())\n",
    "        output[\"cosine_sim\"].append(cos)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2666,\n",
       "  1306,\n",
       "  2,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  2793,\n",
       "  4,\n",
       "  6035,\n",
       "  1116,\n",
       "  2229,\n",
       "  49763,\n",
       "  66,\n",
       "  2535,\n",
       "  9,\n",
       "  146,\n",
       "  29,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  23396,\n",
       "  17878,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3952,\n",
       "  2,\n",
       "  2905,\n",
       "  3649,\n",
       "  7370,\n",
       "  11801,\n",
       "  3921,\n",
       "  16562,\n",
       "  18,\n",
       "  18332,\n",
       "  11360,\n",
       "  12,\n",
       "  3921,\n",
       "  34390,\n",
       "  5145,\n",
       "  1720,\n",
       "  10,\n",
       "  0],\n",
       " 'translation': {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'zh': '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。'},\n",
       " 'cosine_sim': [0.9999985694885254,\n",
       "  0.9999991059303284,\n",
       "  0.9999980926513672,\n",
       "  0.9999982714653015,\n",
       "  0.9999990463256836,\n",
       "  0.9999983310699463,\n",
       "  0.9999982714653015,\n",
       "  0.9999990463256836,\n",
       "  0.9999983310699463,\n",
       "  0.9999989867210388,\n",
       "  0.9999985098838806,\n",
       "  0.9999988675117493,\n",
       "  0.9999985098838806,\n",
       "  0.9999980926513672,\n",
       "  0.9999982714653015,\n",
       "  0.9999979138374329,\n",
       "  0.9999983906745911,\n",
       "  0.9999982714653015,\n",
       "  0.9999982714653015,\n",
       "  0.9999990463256836,\n",
       "  0.9999983310699463,\n",
       "  0.9999982714653015,\n",
       "  0.9999987483024597,\n",
       "  0.9999991655349731,\n",
       "  0.9999985694885254,\n",
       "  0.9999986886978149]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_data = cosine_sim(dataset, old_embed, new_embed)\n",
    "cosine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁1929'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(51091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "def top_k_token(dataset_cos, tokenizer, k=3):\n",
    "    idx = np.argsort(dataset_cos[\"cosine_sim\"])[:k]\n",
    "    input_tokens = [dataset_cos['input_ids'][i] for i in idx ]\n",
    "    return [tokenizer.decode(t) for t in input_tokens if tokenizer.decode(t) not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'has']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_token(cosine_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "punc = string.punctuation\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74, 2]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['▁been', ','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁been'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁nothing'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string('▁nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "def is_stopword(token_id, tokenizer):\n",
    "    word = tokenizer.decode(token_id)\n",
    "    if word in string.punctuation:\n",
    "        return True\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    stopword = stopwords.words('english') + [\"</s>\", \"<unk>\", \">>cmn_Hans<<\", \"<pad>\"]\n",
    "    if word in stopword or not word:\n",
    "        return True\n",
    "    return False  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_stopword(243, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def wrap(num_ex, k=3):\n",
    "    output = []\n",
    "    for i in range(num_ex):\n",
    "        id = random.randint(0, 2000)\n",
    "        dataset = preprocess_dataset(i)\n",
    "        old_embed = get_old_embed(dataset)\n",
    "        model, tokenizer = retrain(dataset)\n",
    "        new_embed = get_new_embed(model, dataset)\n",
    "        cosine_data = cosine_sim(dataset, old_embed, new_embed)\n",
    "        output.append({\"translation\": cosine_data[\"translation\"], \"top_tokens\": top_k_token(cosine_data, tokenizer, k=k)})\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_sim_nostop(dataset, old_embed, new_embed, tokenizer):\n",
    "    output = dataset[0]\n",
    "    output[\"cosine_sim\"] = []\n",
    "    for i in range(len(old_embed)):\n",
    "        current_token = output[\"input_ids\"][i]\n",
    "        if is_stopword(current_token, tokenizer):\n",
    "            output[\"cosine_sim\"].append(100)\n",
    "            continue\n",
    "        cos = 1 - cosine(old_embed[i].detach().numpy(), new_embed[i].detach().numpy())\n",
    "        output[\"cosine_sim\"].append(cos)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2666,\n",
       "  1306,\n",
       "  2,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  2793,\n",
       "  4,\n",
       "  6035,\n",
       "  1116,\n",
       "  2229,\n",
       "  49763,\n",
       "  66,\n",
       "  2535,\n",
       "  9,\n",
       "  146,\n",
       "  29,\n",
       "  3,\n",
       "  5031,\n",
       "  4,\n",
       "  3,\n",
       "  23396,\n",
       "  17878,\n",
       "  6,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [3952,\n",
       "  2,\n",
       "  2905,\n",
       "  3649,\n",
       "  7370,\n",
       "  11801,\n",
       "  3921,\n",
       "  16562,\n",
       "  18,\n",
       "  18332,\n",
       "  11360,\n",
       "  12,\n",
       "  3921,\n",
       "  34390,\n",
       "  5145,\n",
       "  1720,\n",
       "  10,\n",
       "  0],\n",
       " 'translation': {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'zh': '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。'},\n",
       " 'cosine_sim': [100,\n",
       "  0.9999991059303284,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999990463256836,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999990463256836,\n",
       "  100,\n",
       "  0.9999989867210388,\n",
       "  0.9999985098838806,\n",
       "  0.9999988675117493,\n",
       "  0.9999985098838806,\n",
       "  100,\n",
       "  0.9999982714653015,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999990463256836,\n",
       "  100,\n",
       "  100,\n",
       "  0.9999987483024597,\n",
       "  0.9999991655349731,\n",
       "  100,\n",
       "  100]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_data = cosine_sim_nostop(dataset, old_embed, new_embed, tokenizer)\n",
    "cosine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nothing', 'Brothers']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_token(cosine_data, tokenizer, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def wrap_nostop(num_ex, k=3):\n",
    "    output = []\n",
    "    for i in range(num_ex):\n",
    "        id = random.randint(0, 2000)\n",
    "        dataset = preprocess_dataset(id)\n",
    "        old_embed = get_old_embed(dataset)\n",
    "        model, tokenizer = retrain(dataset)\n",
    "        new_embed = get_new_embed(model, dataset)\n",
    "        cosine_data = cosine_sim_nostop(dataset, old_embed, new_embed, tokenizer)\n",
    "        \n",
    "        output.append({\"translation\": cosine_data[\"translation\"], \"top_tokens\": top_k_token(cosine_data, tokenizer, k=k)})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"/data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      65000\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 65000,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 65000,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65001\n",
      "}\n",
      "\n",
      "loading weights file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/added_tokens.json. We won't load it.\n",
      "Didn't find file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer.json. We won't load it.\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/source.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/target.spm\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/vocab.json\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/tokenizer_config.json\n",
      "loading file None\n",
      "loading file /data2/hanyings/opus-mt-en-zh-finetuned-en-to-zh-1109/checkpoint-208000/special_tokens_map.json\n",
      "loading file None\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "result = wrap_nostop(100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation': {'en': 'Moreover, the CAP’s role as a motor of political and social integration in Europe could be restored once renewed policies are in place.',\n",
       "   'zh': '而且只要更新后的政策就位，共同农业政策在欧洲政治与社会整合过程中所扮演的发动机角色就能恢复。'},\n",
       "  'top_tokens': ['Moreover', 'motor', 'renewed', 'place', 'social']},\n",
       " {'translation': {'en': 'In the end, the Addis Ababa Action Agenda provides that the current Committee of Experts will continue to function according to its 2004 mandate, with three additional meeting days per year, all funded through voluntary contributions.',\n",
       "   'zh': '最终，亚的斯亚贝巴行动日程（Addis Ababa Action Agenda）规定，目前的专家委员会将继续根据其2004年的权限行使功能，每年增加三次会议，完全以自愿捐款的方式筹集资金。'},\n",
       "  'top_tokens': ['function', 'three', '2004', 'continue', 'funded']},\n",
       " {'translation': {'en': 'Now, with household debt sustained on a knife-edge after feverish government intervention, the fiscal position has deteriorated dramatically and the current-account balance has worsened again.',\n",
       "   'zh': '如今政府大力干预后的家庭负债状况依然处于危险边缘，同时财政状况急剧恶化，经常账户平衡状况也再度变差。'},\n",
       "  'top_tokens': ['dramatically',\n",
       "   'worsened',\n",
       "   'deteriorated',\n",
       "   'intervention',\n",
       "   'fever']},\n",
       " {'translation': {'en': 'If donor governments really want results, they should take the money out of the hands of thirty or more separate aid bureaucracies and pool it in one or two places, the most logical being the World Bank in Washington and the International Fund for Agricultural Development (IFAD) in Rome.',\n",
       "   'zh': '如果捐助国政府真的看重结果，就应该从30家或者更多的独立官僚援助机构中把款项收回，集中交给一两家机构管理。 最顺理成章的管理机构是华盛顿世界银行和罗马联合国国际农业发展基金。'},\n",
       "  'top_tokens': ['want', 'logical', 'Agricultural', 'one', 'two']},\n",
       " {'translation': {'en': 'Developing nations are grasping just how outrageous the current distribution of greenhouse-gas emissions really is.',\n",
       "   'zh': '发展中国家都意识到了当前温室气体排放量分配方案的不合理之处。'},\n",
       "  'top_tokens': ['outrageous',\n",
       "   'grasp',\n",
       "   'distribution',\n",
       "   'Developing',\n",
       "   'greenhouse']},\n",
       " {'translation': {'en': 'For the sake of all Japanese – not to mention a world economy in need of a new source of dynamism – that promise deserves to be met.',\n",
       "   'zh': '为了日本的整体利益——不要说一个需要新动力来源的世界经济——这个承诺值得去兑现。'},\n",
       "  'top_tokens': ['dynamism', 'deserves', 'mention', 'Japanese', 'economy']},\n",
       " {'translation': {'en': 'Tetapi, sama mendesaknya, lebih dari sepertiga anak usia sekolah dasar – 250 juta anak – tidak mendapatkan pembelajaran yang sifatnya mendasar, menurut temuan dalam UNESCO Education for All Global Monitoring Report.',\n",
       "   'zh': '但是，同样紧迫的是，据联合国教科文组织全民教育全球检测报告，有超过三分之一的小学适龄儿童——2. 5亿人——没有学到基础知识。'},\n",
       "  'top_tokens': ['Te', 'Report', 'Monitoring', 'sama', 'Education']},\n",
       " {'translation': {'en': 'Or do you want universities that regard the idea of a “safe space” – in terms of closing down debate in case it offends someone – as an oxymoron in an academic setting?',\n",
       "   'zh': '你心目中的大学是否将“安全空间”理念——即在冒犯他人的情况下停止讨论——视为学术环境中的矛盾修饰法？'},\n",
       "  'top_tokens': ['offend', 'closing', 'setting', 'oxy', 'want']},\n",
       " {'translation': {'en': 'Thus, scientists working in developing countries face a dilemma: either work on rich-world problems for which there is abundant data, or risk career advancement by conducting qualitative work that will not make it into A-level journals.',\n",
       "   'zh': '因此，在发展中国家工作的科学家们面临着囚徒困境：或者研究有着丰富数据的富裕国家问题，或者甘冒职业生涯之险而进行无缘进入A级期刊的定性研究。'},\n",
       "  'top_tokens': ['abundant', 'work', 'work', 'conducting', 'journals']},\n",
       " {'translation': {'en': 'The fact is that the challenges inherent in completing the TTIP are no more intractable than those that EU leaders have faced in the last few years of crisis.',\n",
       "   'zh': '事实上，完成TTIP所包含的挑战并不比欧盟领导人在过去几年的危机中所遇到的挑战更棘手。'},\n",
       "  'top_tokens': ['intractable',\n",
       "   'inherent',\n",
       "   'challenges',\n",
       "   'completing',\n",
       "   'fact']},\n",
       " {'translation': {'en': 'Global trade as a share of GDP may therefore decline, but without adverse consequences for global economic growth.',\n",
       "   'zh': '全球贸易作为全球GDP的组成部分，其比例可能因此下降，但不会对全球经济增长产生不利影响。'},\n",
       "  'top_tokens': ['adverse', 'consequences', 'share', 'may', 'therefore']},\n",
       " {'translation': {'en': 'Importantly, the report, “Research and Development to Meet Health Needs in Developing Countries,” recommends a comprehensive approach, including mandatory funding contributions from governments for research on developing countries’ health needs; international coordination of health-care priorities and implementation; and a global observatory that would monitor where needs are greatest.',\n",
       "   'zh': '有一点很重要，这份题为《用研发满足发展中国家的卫生需要》（“Research and Development to Meet Health Needs in Developing Countries）给出了一个完整的方法，包括来自政府的指令性发展中国家卫生研究资金分配； 优先卫生项目和实施的国际合作；'},\n",
       "  'top_tokens': ['Importantly',\n",
       "   'mandatory',\n",
       "   'Meet',\n",
       "   'comprehensive',\n",
       "   'observatory']},\n",
       " {'translation': {'en': 'This intractability is, in some cases, understandable; the pain of survivors and their descendants remains acute.',\n",
       "   'zh': '从某种程度上，这个棘手问题不难理解； 幸存者及其后代的痛苦依然剧烈。'},\n",
       "  'top_tokens': ['understandable', 'tract', 'acute', 'cases', 'ability']},\n",
       " {'translation': {'en': 'Nor is inclusion of the renminbi in the SDR basket likely to provide as big a boost to the currency’s internationalization as many believe.',\n",
       "   'zh': '加入SDR篮子是否能给人民币国际化带来巨大的提振也不像许多人认为的那样确定。'},\n",
       "  'top_tokens': ['renminbi', 'boost', 'inclusion', 'ization', 'currency']},\n",
       " {'translation': {'en': 'Thus, 9/11 has meant, directly or indirectly, a great shock, both psychologically and to our political systems.',\n",
       "   'zh': '这样，9/11已经直接或间接地意味着一个巨大的震惊事件，在心理上以及对于政治体系而言都是如此。'},\n",
       "  'top_tokens': ['Thus', '9/11', 'shock', 'systems', 'psychological']},\n",
       " {'translation': {'en': 'The WHO’s efforts to encourage broad reforms at the international level are crucial.',\n",
       "   'zh': '世界卫生组织旨在促进国际层面的广泛改革的努力非常关键。'},\n",
       "  'top_tokens': ['encourage', 'crucial', 'international', 'level', 'broad']},\n",
       " {'translation': {'en': 'Until now, Russia has cared less about a new PCA than the EU, because two-thirds of Russia’s exports to the Union comprise natural resources, which bring in cash even without the strong rules that a PCA provides.',\n",
       "   'zh': '\\xa0\\xa0\\xa0 目前为止，俄国对协议的关心并不像欧盟那样强烈，因为俄国向欧盟出口的三分之二是自然资源。 因此，即使没有协议规定的硬性条款也能够带来现金收入。'},\n",
       "  'top_tokens': ['PCA', 'PCA', 'Russia', 'Russia', 'comprise']},\n",
       " {'translation': {'en': 'When demand begins to exceed supply, demand-side stimulus policies will become increasingly ineffective, and it will be time to launch the third arrow of Abenomics: growth-enhancing structural reforms.',\n",
       "   'zh': '当需求开始超过供给的时候，需求端的刺激政策将逐渐变得无效，是时候射出安倍经济学的第三支箭：强化增长的结构性改革。'},\n",
       "  'top_tokens': ['enhancing',\n",
       "   'ineffective',\n",
       "   'arrow',\n",
       "   'launch',\n",
       "   'increasingly']},\n",
       " {'translation': {'en': 'Because rapid fiscal deterioration now has investors worrying about capital losses on US government securities, devaluation would make foreigners more hesitant to finance America’s budget deficit.',\n",
       "   'zh': '因为财政状况的迅速恶化已经令投资者们对自己美国政府债券上的资本损失忧虑不已，此外贬值也将使外国人更不愿意资助美国的预算赤字。'},\n",
       "  'top_tokens': ['hesitant', 'worrying', 'devaluation', 'rapid', 'deficit']},\n",
       " {'translation': {'en': 'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic controversies that the entire project may well be scuttled.',\n",
       "   'zh': '在TPP谈判不断推进的同时，欧盟-美国跨大西洋贸易和投资伙伴关系（Transatlantic Trade and Investment Partnership，TTIP）谈判却深陷国内矛盾的掣肘，整个工程都有可能夭折。'},\n",
       "  'top_tokens': ['controversies', 'problem', 'project', 'well', 'led']},\n",
       " {'translation': {'en': 'Such depictions are breathtaking in their audacity, given Japan’s seven-decade record as a peaceful and constructive member of the international community.',\n",
       "   'zh': '这样的描述不可不谓厚颜无耻，因为七十年来日本一直是国际社会和平而具有建设性的成员。'},\n",
       "  'top_tokens': ['decade', 'constructive', 'depict', 'seven', 'uda']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']},\n",
       " {'translation': {'en': 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       "   'zh': '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。'},\n",
       "  'top_tokens': ['Anyone', 'would', 'carry', 'visiting', 'learn']},\n",
       " {'translation': {'en': 'The British security services were deeply suspicious of him.',\n",
       "   'zh': '英国安全部门非常怀疑他。'},\n",
       "  'top_tokens': ['deeply', 'suspicious', 'services', 'security', 'British']},\n",
       " {'translation': {'en': 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       "   'zh': '在国事活动中，外表应当是遮人耳目的。'},\n",
       "  'top_tokens': ['ceiving', 'realm', 'meant', 'de', 'craft']},\n",
       " {'translation': {'en': 'These lessons apply equally to advanced economies, which also suffer from rising inequality and subtle forms of corruption.',\n",
       "   'zh': '这些教训对发达经济体同样有效，因为发达经济体也在遭受不平等性恶化和腐败潜规则的问题。'},\n",
       "  'top_tokens': ['subtle', 'apply', 'rising', 'equally', 'advanced']},\n",
       " {'translation': {'en': 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       "   'zh': '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。'},\n",
       "  'top_tokens': ['diminish',\n",
       "   'would',\n",
       "   'competitiveness',\n",
       "   'substantially',\n",
       "   'recognition']},\n",
       " {'translation': {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "   'zh': '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。'},\n",
       "  'top_tokens': ['loser', 'Everyone', 'seems', 'even', 'others']},\n",
       " {'translation': {'en': 'Despite her charisma, Park is neither a Sarah Palin nor an Eva Peron.',\n",
       "   'zh': '尽管拥有超凡的个人魅力，但朴既不同于萨拉·佩林也不同于伊娃·裴隆。'},\n",
       "  'top_tokens': ['Eva', 'neither', 'Sarah', 'Pal', 'Per']}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happening', 'widen', 'analog', 'deepen', 'understand']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][\"top_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#pos = nltk.pos_tag(result[0][\"top_tokens\"])\n",
    "def count_pos(result_dict):\n",
    "    all_tokens = []\n",
    "    for sent in result_dict:\n",
    "        all_tokens += sent[\"top_tokens\"]\n",
    "    pos = nltk.pos_tag(all_tokens)\n",
    "    the_count = Counter(tag for _, tag in pos)\n",
    "    \n",
    "    labels, values = zip(*the_count.items())\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "\n",
    "    plt.title(\"Distribution of Tokens POS\")\n",
    "    plt.bar(indexes, values, width, color=(0.3, 0.4, 0.7, 0.6))\n",
    "    plt.xticks(indexes + width * 0.5, labels, rotation=90)\n",
    "    plt.show()\n",
    "    return the_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = count_pos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'RB': 50,\n",
       "         'NN': 122,\n",
       "         'VBD': 2,\n",
       "         'JJ': 79,\n",
       "         'CD': 6,\n",
       "         'VBN': 3,\n",
       "         'VBP': 15,\n",
       "         'NNP': 59,\n",
       "         'NNS': 30,\n",
       "         'VBZ': 13,\n",
       "         'IN': 1,\n",
       "         'VBG': 28,\n",
       "         'MD': 23,\n",
       "         'VB': 23,\n",
       "         'JJR': 1,\n",
       "         'DT': 12,\n",
       "         'FW': 33})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/users/as/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/users/as/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/users/as/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/users/as/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/users/as/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "#words = unmasker(\"Hello I'm a [MASK] boy.\")\n",
    "from nltk.corpus import wordnet as wn\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def find_tokenidx(token, sentence_lst):\n",
    "  for i in range(len(sentence_lst)):\n",
    "    if token in sentence_lst[i]:\n",
    "      return i\n",
    "def mutant_pairs(examples):\n",
    "  pairs=[]\n",
    "  inputs = [ex[\"translation\"][source_lang] for ex in examples]\n",
    "  for i, input in enumerate(inputs):\n",
    "    input_list = input.split()\n",
    "    tokens = examples[i]['top_tokens']\n",
    "    # sys = wn.synsets(input_list[tokens])\n",
    "    # idx = 0\n",
    "    # while 1:\n",
    "    #   if len(wn.synset(sys[idx].name()).lemmas()) >= 2:\n",
    "    #     break\n",
    "    #   idx += 1\n",
    "    # candidate = [str(lemma.name()) for lemma in wn.synset(sys[idx].name()).lemmas()]\n",
    "    print(tokens)\n",
    "    token_idx = find_tokenidx(tokens[0], input_list)\n",
    "    print(input_list[token_idx])\n",
    "    input_list[token_idx] = \"[MASK]\"\n",
    "    candidate = unmasker(\" \".join(input_list))\n",
    "    #print(candidate)\n",
    "    mutants = []\n",
    "    for c in candidate[:3]:\n",
    "      mutant = input_list.copy()\n",
    "      mutant[token_idx] = c[\"token_str\"]\n",
    "      mutants.append(\" \".join(mutant))\n",
    "    pairs.append({'en': input, 'mutants': mutants})\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929', '1989', 'or', '</s>']\n",
      "1929\n",
      "['happening', 'widen', 'analog', 'deepen', 'understand']\n",
      "happening.\n",
      "['classical', 'reassuring', 'refer', 'cyclical', 'many']\n",
      "classical\n",
      "['grim', 'mood', 'classical', 'behave', 'governments']\n",
      "grimmer,\n",
      "['diffusion', 'tendency', 'restraint', 'States', 'Europe']\n",
      "diffusion\n",
      "['moved', 'badly', 'avoiding', 'implement', 'fronts']\n",
      "moved\n",
      "['geo', 'naturally', 'comes', 'year', 'economic']\n",
      "geo-strategists,\n",
      "['nothing', 'Brothers', 'h', 'Berlin', 'man']\n",
      "nothing\n",
      "['reassuring', 'destruct', 'oppression', 'versus', 'perfect']\n",
      "reassuring\n",
      "['unfolding', 'correspond', '2008-2009,', 'felt', 'whose']\n",
      "unfolding\n",
      "['end', 'end', 'ideological', 'divide', 'absolute']\n",
      "end\n",
      "['jeopardize', '2009', '1989,', 'tendencies', 'principles']\n",
      "jeopardize\n",
      "['Blo', 'promoted', 'c', 'ideology', 'triumph']\n",
      "Bloc.\n",
      "['deliberate', 'brink', 'Ronald', 'pushed', 'supporters']\n",
      "deliberate\n",
      "['differences', 'obvious', '1989', 'course', 'Of']\n",
      "differences\n",
      "['collapse', 'bi', 'First', 'arity', '1989']\n",
      "collapse\n",
      "['substituting', '2009', 'pave', 'contrast', 'way']\n",
      "substituting\n",
      "['losers', 'Second', 'democracy', 'winners', 'winners']\n",
      "losers.\n",
      "['loser', 'Everyone', 'seems', 'even', 'others']\n",
      "loser,\n",
      "['Yet', 'unfair', 'greater', 'may', 'better']\n",
      "Yet,\n",
      "['shape', 'better', 'alone', 'In']\n",
      "shape,\n",
      "['preview', 'Harvard', 'getting', 'finally', 'passes']\n",
      "preview\n",
      "['One', 'dominated', 'universe', 'Asian', 'making']\n",
      "One\n",
      "['incredible', 'Indians', 'Athens', 'admiration', 'Harvard']\n",
      "incredible\n",
      "['spreading', 'outright', 'disorder', 'new', 'may']\n",
      "spreading\n",
      "['central', 'producing', 'crisis', 'Egyptian', 'Gulf']\n",
      "central\n",
      "['poorer', 'get', 'less', 'get', 'rich']\n",
      "poorer.\n",
      "['supposedly', 'explosions', 'foreign', 'xenophobia', 'potential']\n",
      "supposedly\n",
      "['enduring', 'would', 'less', 'many', 'consequences']\n",
      "enduring\n",
      "['reflex', '2009', 'dramatic', 'hope', 'historical']\n",
      "reflexes\n",
      "['2008', 'Failed', 'What', 'in']\n",
      "2008?\n",
      "['solve', 'know', 'problem', 'E', 'RK']\n",
      "solve\n",
      "['thought', 'willing', 'turns', 'implement', 'quite']\n",
      "thought.\n",
      "['regulated', 'result', 'easier', 'perceived', 'de']\n",
      "deregulated,\n",
      "['proliferate', 'wild', 'imagination', 'risk', 'result']\n",
      "proliferated\n",
      "['incorrect', 'tested', 'ultimately', 'created', 'Un']\n",
      "incorrect\n",
      "['underestimated', 'tail', 'Officials', 'risks']\n",
      "underestimated\n",
      "['maneuver', '2', 'set', 'chop', 'leaving']\n",
      "maneuver\n",
      "['currency', 'common', 'euro', 'uda', 'cious']\n",
      "currency.\n",
      "['headed', 'began', 'continued', 'long', 'Indeed']\n",
      "wrongheaded\n",
      "['worsening', 'Depression', 'Politi', 'address', 'Great']\n",
      "worsening\n",
      "['smart', 'ail', 'un', 'simple', 'countering']\n",
      "smart,\n",
      "['suggests', 'especially', 'short', 'issue', 'reserve']\n",
      "suggests\n",
      "['argues', '4', 'raise', 'even', 'targets']\n",
      "argues,\n",
      "['discourage', 'Wolf', 'lower', 'put', 'place']\n",
      "discourage\n",
      "['contradictions', 'eurozone', 'institutions', 'properly', 'must']\n",
      "contradictions,\n",
      "['tackling', 'craft', 'inequality', 'Wolf', 'countries']\n",
      "tackling\n",
      "['proposals', 'implement', 'Wolf', 'little', 'may']\n",
      "proposals\n",
      "['Mirror', 'found', 'patron', 'E', 'Barry']\n",
      "Mirrors,\n",
      "['E', 'Fried', 'Milton', 'Minsk', 'chen']\n",
      "Eichengreen\n",
      "['2008', 'Fried', 'erupted', 'tried', 'apply']\n",
      "2008\n",
      "['radically', 'incomplete', 'significant', 'turned', 'Depression']\n",
      "radically\n",
      "['prevent', 'depression', 'c', 'politicians', 'embrace']\n",
      "prevent\n",
      "['stagnant', 'become', 'threatens', 'normal', 'today']\n",
      "stagnant\n",
      "['exposed', 'strengthen', 'failure', 'major', 'another']\n",
      "exposed\n",
      "['E', 'Wolf', 'agree', 'underpin', 'shortcomings']\n",
      "Eichengreen\n",
      "['never', 'learned', 'truly', 'true', 'Indeed']\n",
      "never\n",
      "['Come', 'back', 'Strategy', 'Europe', 'A']\n",
      "Comeback\n",
      "['pleasant', 'vitality', 'lacking', 'grandmother', 'OCK']\n",
      "pleasant\n",
      "['forge', 'recognize', 'tackling', 'argued', 'high']\n",
      "forge\n",
      "['Admittedly', 'alarming', 'characterization', 'accurate', 'e']\n",
      "Admittedly,\n",
      "['strengths', 'significant', 'seem', 'ing', 'retain']\n",
      "strengths.\n",
      "['encompassing', 'impressive', 'built', 'billion', 'competitive']\n",
      "encompassing\n",
      "['influencing', 'changing', 'increasingly', 'world', 'economic']\n",
      "influencing\n",
      "['shift', 'mega', 'join', 'accelerate', 'eventually']\n",
      "shift\n",
      "['faces', 'hurdle', 'underestimated', 'shortage', 'augment']\n",
      "faces\n",
      "['enhancing', 'work', 'ties', 'beginning', 'secure']\n",
      "enhancing\n",
      "['controversies', 'problem', 'project', 'well', 'led']\n",
      "controversies\n",
      "['substantial', 'convinced', 'Business', 'perception', 'benefits']\n",
      "substantial\n",
      "['trivial', 'dominate', 'Yet', 'issues', 'chlorinat']\n",
      "trivial\n",
      "['TT', 'unleash', 'half', 'trade', 'wealth']\n",
      "TTIP’s\n",
      "['greater', 'even', 'benefits', 'would', 'TT']\n",
      "greater.)\n",
      "['catastrophic', 'failure', 'compelling', 'benefits', 'agreement']\n",
      "catastrophic\n",
      "['implemented', 'ammunition', 'give', 'unlikely', 'withdrawal']\n",
      "implemented,\n",
      "['squander', 'perception', 'disengagement', 'EU', 'drive']\n",
      "squander\n",
      "['invariably', 'exert', 'Putin', 'would', 'Vladimir']\n",
      "invariably\n",
      "['regain', 'stark', 'collapse', 'tip', 'moves']\n",
      "regaining\n",
      "['seemed', 'TT', 'recognize', 'first', 'proposed']\n",
      "seemed\n",
      "['EU', 'doubted', 'Indeed', 'process', 'initially']\n",
      "EU\n",
      "['ambition', 'tank', 'gas', 'complete', 'negotiations']\n",
      "ambition\n",
      "['one', 'protracted', 'associated', 'endure', 'wanted']\n",
      "one\n",
      "['seemingly', 'fears', 'essentially', 'abandoned', 'confirming']\n",
      "seemingly\n",
      "['headway', 'presenting', 'negotiators', 'discourse', 'public']\n",
      "headway,\n",
      "['inaccurate', 'tract', 'talk', 'gaining', 'agreement']\n",
      "inaccurate\n",
      "['revive', 'conclude', 'successfully', 'commitment', '2015.']\n",
      "revive\n",
      "['simple', 'remaining', 'resolving', 'issues', 'say']\n",
      "simple.\n",
      "['entails', 'establishing', 'always', 'difficult', 'many']\n",
      "entails\n",
      "['intractable', 'inherent', 'challenges', 'completing', 'fact']\n",
      "intractable\n",
      "['push', 'completing', 'TT', 'genuine', 'must']\n",
      "push\n",
      "['improved', 'chances', 'might', 'US', 'good']\n",
      "improved\n",
      "['Barack', 'get', 'Obama', 'might', 'negotiating']\n",
      "Barack\n",
      "['picking', 'would', 'apart', 'negotiated', 'simply']\n",
      "picking\n",
      "['easily', 'year', 'could', 'agenda', 'starting']\n",
      "easily\n",
      "['leaders', 'waste', 'time', 'Europe', 'That']\n",
      "leaders\n",
      "['must', 'seize', 'strategic', 'economic', 'avert']\n",
      "must\n",
      "['Ended', 'E', 'Year', 'ch', 'po']\n",
      "Ended\n",
      "['shroud', '2016', 'outlook', 'uncertainty', '2017']\n",
      "shrouded\n",
      "['rising', 'Tension', 'East', 'States', 'movements']\n",
      "rising,\n",
      "['rapprochement', 'disagreement', 'Bashar', 'despite', 'fundamental']\n",
      "rapprochement,\n",
      "['Meanwhile', 'devastated', 'utterly', 'backed', 'war']\n",
      "Meanwhile,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'en': '1929 or 1989?',\n",
       "  'mutants': ['1988 or 1989?', '1989 or 1989?', '1987 or 1989?']},\n",
       " {'en': 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       "  'mutants': ['PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been .',\n",
       "   'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been ;',\n",
       "   'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been ?']},\n",
       " {'en': 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       "  'mutants': ['At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to the cyclical downturns.',\n",
       "   'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to recent cyclical downturns.',\n",
       "   'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to similar cyclical downturns.']},\n",
       " {'en': 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "  'mutants': ['Today, the mood is much lighter with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "   'Today, the mood is much darker with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "   'Today, the mood is much improved with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.']},\n",
       " {'en': 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       "  'mutants': ['The tendency is either excessive restraint (Europe) or a failure of the effort (the United States).',\n",
       "   'The tendency is either excessive restraint (Europe) or a lack of the effort (the United States).',\n",
       "   'The tendency is either excessive restraint (Europe) or a waste of the effort (the United States).']},\n",
       " {'en': 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "  'mutants': ['Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has intervened on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "   'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has acted on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "   'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has invested on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.']},\n",
       " {'en': 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "  'mutants': ['For me however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "   'For us however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "   'For him however, the year that naturally comes to mind, in both politics and economics, is 1989.']},\n",
       " {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "  'mutants': ['Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "   'Of course, the fall of the house of Lehman Brothers has something to do with the fall of the Berlin Wall.',\n",
       "   'Of course, the fall of the house of Lehman Brothers has everything to do with the fall of the Berlin Wall.']},\n",
       " {'en': 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       "  'mutants': ['Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and corrupt institution of financial capitalism.',\n",
       "   'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and unstable institution of financial capitalism.',\n",
       "   'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and fragile institution of financial capitalism.']},\n",
       " {'en': 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.',\n",
       "  'mutants': ['Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose immediate consequences will be felt for decades.',\n",
       "   'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose negative consequences will be felt for decades.',\n",
       "   'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose lasting consequences will be felt for decades.']},\n",
       " {'en': 'The end of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       "  'mutants': ['The end of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       "   'The beginning of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       "   'The introduction of the East-West ideological divide and the end of absolute faith in markets are historical turning points.']},\n",
       " {'en': 'And what happens in 2009 may jeopardize some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       "  'mutants': ['And what happens in 2009 may reflect some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       "   'And what happens in 2009 may explain some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       "   'And what happens in 2009 may be some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.']},\n",
       " {'en': 'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet Bloc.',\n",
       "  'mutants': ['In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet .',\n",
       "   'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet ;',\n",
       "   'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet !']},\n",
       " {'en': 'For many of his supporters, it was President Ronald Reagan who, with his deliberate escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       "  'mutants': ['For many of his supporters, it was President Ronald Reagan who, with his dramatic escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       "   'For many of his supporters, it was President Ronald Reagan who, with his rapid escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       "   'For many of his supporters, it was President Ronald Reagan who, with his subsequent escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.']},\n",
       " {'en': 'Of course, there are obvious differences between 1989 and now.',\n",
       "  'mutants': ['Of course, there are obvious differences between 1989 and now.',\n",
       "   'Of course, there are obvious gaps between 1989 and now.',\n",
       "   'Of course, there are obvious changes between 1989 and now.']},\n",
       " {'en': 'First, and perhaps above all, the revolutions of 1989 and the subsequent collapse of the Soviet Union put an end to global bipolarity.',\n",
       "  'mutants': ['First, and perhaps above all, the revolutions of 1989 and the subsequent fall of the Soviet Union put an end to global bipolarity.',\n",
       "   'First, and perhaps above all, the revolutions of 1989 and the subsequent collapse of the Soviet Union put an end to global bipolarity.',\n",
       "   'First, and perhaps above all, the revolutions of 1989 and the subsequent dissolution of the Soviet Union put an end to global bipolarity.']},\n",
       " {'en': 'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China substituting for the Soviet Union.',\n",
       "  'mutants': ['By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China falling for the Soviet Union.',\n",
       "   'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China voting for the Soviet Union.',\n",
       "   'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China leaving for the Soviet Union.']},\n",
       " {'en': 'Second, whereas democracy and market capitalism appeared as clear – if more fragile than expected – winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from losers.',\n",
       "  'mutants': ['Second, whereas democracy and market capitalism appeared as clear – if more fragile than expected – winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from .',\n",
       "   'Second, whereas democracy and market capitalism appeared as clear – if more fragile than expected – winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from ;',\n",
       "   'Second, whereas democracy and market capitalism appeared as clear – if more fragile than expected – winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from ?']},\n",
       " {'en': 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       "  'mutants': ['Everyone seems to be a mess even if some are more affected than others.',\n",
       "   'Everyone seems to be a failure even if some are more affected than others.',\n",
       "   'Everyone seems to be a wreck even if some are more affected than others.']},\n",
       " {'en': 'Yet, history is unfair, and the US, despite its greater responsibility for today’s global crisis, may emerge in better shape than most countries from the morass.',\n",
       "  'mutants': ['this history is unfair, and the US, despite its greater responsibility for today’s global crisis, may emerge in better shape than most countries from the morass.',\n",
       "   'the history is unfair, and the US, despite its greater responsibility for today’s global crisis, may emerge in better shape than most countries from the morass.',\n",
       "   'its history is unfair, and the US, despite its greater responsibility for today’s global crisis, may emerge in better shape than most countries from the morass.']},\n",
       " {'en': 'In better shape, but not alone.',\n",
       "  'mutants': ['In better shape but not alone.',\n",
       "   'In better health but not alone.',\n",
       "   'In better form but not alone.']},\n",
       " {'en': 'As a visiting professor at Harvard and MIT, I am getting a good preview of what the world could look like when the crisis finally passes.',\n",
       "  'mutants': ['As a visiting professor at Harvard and MIT, I am getting a good idea of what the world could look like when the crisis finally passes.',\n",
       "   'As a visiting professor at Harvard and MIT, I am getting a good view of what the world could look like when the crisis finally passes.',\n",
       "   'As a visiting professor at Harvard and MIT, I am getting a good understanding of what the world could look like when the crisis finally passes.']},\n",
       " {'en': 'One senses something like the making of an American-Asian dominated universe.',\n",
       "  'mutants': ['he senses something like the making of an American-Asian dominated universe.',\n",
       "   'she senses something like the making of an American-Asian dominated universe.',\n",
       "   'it senses something like the making of an American-Asian dominated universe.']},\n",
       " {'en': 'From the incredible media lab at MIT to the mathematics and economics departments at Harvard, Asians – Chinese and Indians, in particular – are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       "  'mutants': ['From the mit media lab at MIT to the mathematics and economics departments at Harvard, Asians – Chinese and Indians, in particular – are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       "   'From the new media lab at MIT to the mathematics and economics departments at Harvard, Asians – Chinese and Indians, in particular – are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       "   'From the digital media lab at MIT to the mathematics and economics departments at Harvard, Asians – Chinese and Indians, in particular – are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.']},\n",
       " {'en': 'But before this new order appears, the world may be faced with spreading disorder, if not outright chaos.',\n",
       "  'mutants': ['But before this new order appears, the world may be faced with complete disorder, if not outright chaos.',\n",
       "   'But before this new order appears, the world may be faced with total disorder, if not outright chaos.',\n",
       "   'But before this new order appears, the world may be faced with utter disorder, if not outright chaos.']},\n",
       " {'en': 'What, for example, will happen to a country as central and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       "  'mutants': ['What, for example, will happen to a country as poor and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       "   'What, for example, will happen to a country as isolated and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       "   'What, for example, will happen to a country as impoverished and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?']},\n",
       " {'en': 'When the rich get less rich, the poor get poorer.',\n",
       "  'mutants': ['When the rich get less rich, the poor get .',\n",
       "   'When the rich get less rich, the poor get ;',\n",
       "   'When the rich get less rich, the poor get !']},\n",
       " {'en': 'And what about the foreign workers who have reached for the “European dream” and are now faced with potential explosions of xenophobia in Europe’s supposedly open countries?',\n",
       "  'mutants': ['And what about the foreign workers who have reached for the “European dream” and are now faced with potential explosions of xenophobia in Europe’s most open countries?',\n",
       "   'And what about the foreign workers who have reached for the “European dream” and are now faced with potential explosions of xenophobia in Europe’s more open countries?',\n",
       "   'And what about the foreign workers who have reached for the “European dream” and are now faced with potential explosions of xenophobia in Europe’s least open countries?']},\n",
       " {'en': 'The consequences of 1989 ended up being less enduring than many observers, including me, would have assumed.',\n",
       "  'mutants': ['The consequences of 1989 ended up being less severe than many observers, including me, would have assumed.',\n",
       "   'The consequences of 1989 ended up being less serious than many observers, including me, would have assumed.',\n",
       "   'The consequences of 1989 ended up being less significant than many observers, including me, would have assumed.']},\n",
       " {'en': 'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now – intuitively and in our historical reflexes – feel them to be.',\n",
       "  'mutants': ['We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now – intuitively and in our historical context – feel them to be.',\n",
       "   'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now – intuitively and in our historical experience – feel them to be.',\n",
       "   'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now – intuitively and in our historical perspective – feel them to be.']},\n",
       " {'en': 'What Failed in 2008?',\n",
       "  'mutants': ['What Failed in ?', 'What Failed in .', 'What Failed in ;']},\n",
       " {'en': 'BERKELEY – To solve a problem, it is not enough to know what to do.',\n",
       "  'mutants': ['BERKELEY – To be a problem, it is not enough to know what to do.',\n",
       "   'BERKELEY – To solve a problem, it is not enough to know what to do.',\n",
       "   'BERKELEY – To have a problem, it is not enough to know what to do.']},\n",
       " {'en': 'You actually have to implement the solution – and be willing to change course if it turns out that you did not know quite as much as you thought.',\n",
       "  'mutants': ['You actually have to implement the solution – and be willing to change course if it turns out that you did not know quite as much as you .',\n",
       "   'You actually have to implement the solution – and be willing to change course if it turns out that you did not know quite as much as you ;',\n",
       "   'You actually have to implement the solution – and be willing to change course if it turns out that you did not know quite as much as you !']},\n",
       " {'en': 'As a result, markets were deregulated, making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       "  'mutants': ['As a result, markets were increasingly making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       "   'As a result, markets were changing making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       "   'As a result, markets were growing making it easier to trade assets that were perceived to be safe, but were in fact not.']},\n",
       " {'en': 'As a result, systemic risk proliferated beyond central bankers’ wildest imagination.',\n",
       "  'mutants': ['As a result, systemic risk is beyond central bankers’ wildest imagination.',\n",
       "   'As a result, systemic risk goes beyond central bankers’ wildest imagination.',\n",
       "   'As a result, systemic risk extends beyond central bankers’ wildest imagination.']},\n",
       " {'en': 'Untested – and ultimately incorrect – assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       "  'mutants': ['Untested – and ultimately incorrect – assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       "   'Untested – and ultimately false – assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       "   'Untested – and ultimately inaccurate – assumptions created a policymaking environment defined by what can only be called hubris.']},\n",
       " {'en': 'Officials underestimated tail risks.',\n",
       "  'mutants': ['Officials report tail risks.',\n",
       "   'Officials reported tail risks.',\n",
       "   'Officials avoid tail risks.']},\n",
       " {'en': 'They set inflation targets at around 2% – leaving little room for maneuver when the water got choppy.',\n",
       "  'mutants': ['They set inflation targets at around 2% – leaving little room for inflation when the water got choppy.',\n",
       "   'They set inflation targets at around 2% – leaving little room for improvement when the water got choppy.',\n",
       "   'They set inflation targets at around 2% – leaving little room for change when the water got choppy.']},\n",
       " {'en': 'And, most audaciously of all, the European Union introduced the euro as a common currency.',\n",
       "  'mutants': ['And, most audaciously of all, the European Union introduced the euro as a common .',\n",
       "   'And, most audaciously of all, the European Union introduced the euro as a common ;',\n",
       "   'And, most audaciously of all, the European Union introduced the euro as a common !']},\n",
       " {'en': 'Indeed, wrongheaded policymaking continued long after the crisis began.',\n",
       "  'mutants': ['Indeed, foreign policymaking continued long after the crisis began.',\n",
       "   'Indeed, soviet policymaking continued long after the crisis began.',\n",
       "   'Indeed, american policymaking continued long after the crisis began.']},\n",
       " {'en': 'Politicians responded to worsening economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       "  'mutants': ['Politicians responded to the economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       "   'Politicians responded to poor economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       "   'Politicians responded to difficult economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.']},\n",
       " {'en': 'Wolf’s prescription for countering the crisis is simple, smart, and unassailable.',\n",
       "  'mutants': ['Wolf’s prescription for countering the crisis is simple, effective and unassailable.',\n",
       "   'Wolf’s prescription for countering the crisis is simple, straightforward and unassailable.',\n",
       "   'Wolf’s prescription for countering the crisis is simple, simple and unassailable.']},\n",
       " {'en': 'In the short term, he suggests that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       "  'mutants': ['In the short term, he argued that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       "   'In the short term, he suggested that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       "   'In the short term, he proposed that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.']},\n",
       " {'en': 'Their central banks, he argues, should raise inflation targets to 3% or even 4% per year.',\n",
       "  'mutants': ['Their central banks, he said should raise inflation targets to 3% or even 4% per year.',\n",
       "   'Their central banks, he argued should raise inflation targets to 3% or even 4% per year.',\n",
       "   'Their central banks, he suggested should raise inflation targets to 3% or even 4% per year.']},\n",
       " {'en': 'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and discourage overleveraging.',\n",
       "  'mutants': ['Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and prevent overleveraging.',\n",
       "   'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and reduce overleveraging.',\n",
       "   'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and avoid overleveraging.']},\n",
       " {'en': 'The eurozone, too, must resolve its internal contradictions, either by disbanding or by introducing “a minimum set of institutions and policies” that allow the monetary union to function properly.',\n",
       "  'mutants': ['The eurozone, too, must resolve its internal problems either by disbanding or by introducing “a minimum set of institutions and policies” that allow the monetary union to function properly.',\n",
       "   'The eurozone, too, must resolve its internal conflicts either by disbanding or by introducing “a minimum set of institutions and policies” that allow the monetary union to function properly.',\n",
       "   'The eurozone, too, must resolve its internal issues either by disbanding or by introducing “a minimum set of institutions and policies” that allow the monetary union to function properly.']},\n",
       " {'en': 'Wolf’s long-term solutions include tackling inequality, “more global regulation,” a greater degree of “freedom for individual countries to craft their own responses,” and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       "  'mutants': ['Wolf’s long-term solutions include greater inequality, “more global regulation,” a greater degree of “freedom for individual countries to craft their own responses,” and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       "   'Wolf’s long-term solutions include lower inequality, “more global regulation,” a greater degree of “freedom for individual countries to craft their own responses,” and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       "   'Wolf’s long-term solutions include increasing inequality, “more global regulation,” a greater degree of “freedom for individual countries to craft their own responses,” and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.']},\n",
       " {'en': 'And yet, as recommendable as Wolf’s proposals may be, little has been done to implement them.',\n",
       "  'mutants': ['And yet, as recommendable as Wolf’s rules may be, little has been done to implement them.',\n",
       "   'And yet, as recommendable as Wolf’s laws may be, little has been done to implement them.',\n",
       "   'And yet, as recommendable as Wolf’s suggestions may be, little has been done to implement them.']},\n",
       " {'en': 'The reasons why are found in the second book: Hall of Mirrors, by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       "  'mutants': ['The reasons why are found in the second book: Hall of fame by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       "   'The reasons why are found in the second book: Hall of mirrors by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       "   'The reasons why are found in the second book: Hall of honor by&nbsp;my friend, teacher, and patron, Barry Eichengreen.']},\n",
       " {'en': 'Eichengreen traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers – at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       "  'mutants': ['it traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers – at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       "   'he traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers – at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       "   'she traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers – at least when it comes to interpretations of the causes and consequences of the Great Depression.']},\n",
       " {'en': 'When the 2008 financial crisis erupted, policymakers tried to apply Friedman’s proposed solutions to the Great Depression.',\n",
       "  'mutants': ['When the 1929 financial crisis erupted, policymakers tried to apply Friedman’s proposed solutions to the Great Depression.',\n",
       "   'When the great financial crisis erupted, policymakers tried to apply Friedman’s proposed solutions to the Great Depression.',\n",
       "   'When the world financial crisis erupted, policymakers tried to apply Friedman’s proposed solutions to the Great Depression.']},\n",
       " {'en': 'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and radically incomplete.',\n",
       "  'mutants': ['Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and ultimately incomplete.',\n",
       "   'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and therefore incomplete.',\n",
       "   'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and thus incomplete.']},\n",
       " {'en': 'The resulting policies were enough to prevent the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       "  'mutants': ['The resulting policies were enough to prevent the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       "   'The resulting policies were enough to stop the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       "   'The resulting policies were enough to keep the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.']},\n",
       " {'en': 'The result is today’s stagnant economy, marked by anemic growth that threatens to become the new normal.',\n",
       "  'mutants': ['The result is today’s mixed economy, marked by anemic growth that threatens to become the new normal.',\n",
       "   'The result is today’s global economy, marked by anemic growth that threatens to become the new normal.',\n",
       "   'The result is today’s market economy, marked by anemic growth that threatens to become the new normal.']},\n",
       " {'en': 'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy exposed to the risk of another major crisis.',\n",
       "  'mutants': ['The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy exposed to the risk of another major crisis.',\n",
       "   'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy vulnerable to the risk of another major crisis.',\n",
       "   'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy open to the risk of another major crisis.']},\n",
       " {'en': 'Wolf and Eichengreen would agree that the main shortcomings that led to the 2008 financial crisis – and that continue to underpin our inadequate response to it – are intellectual.',\n",
       "  'mutants': ['Wolf and colleagues would agree that the main shortcomings that led to the 2008 financial crisis – and that continue to underpin our inadequate response to it – are intellectual.',\n",
       "   'Wolf and others would agree that the main shortcomings that led to the 2008 financial crisis – and that continue to underpin our inadequate response to it – are intellectual.',\n",
       "   'Wolf and wilson would agree that the main shortcomings that led to the 2008 financial crisis – and that continue to underpin our inadequate response to it – are intellectual.']},\n",
       " {'en': 'Indeed, the only true lesson of the crisis so far seems to be that its lessons will never truly be learned.',\n",
       "  'mutants': ['Indeed, the only true lesson of the crisis so far seems to be that its lessons will never truly be learned.',\n",
       "   'Indeed, the only true lesson of the crisis so far seems to be that its lessons will not truly be learned.',\n",
       "   'Indeed, the only true lesson of the crisis so far seems to be that its lessons will ever truly be learned.']},\n",
       " {'en': 'A Comeback Strategy for Europe',\n",
       "  'mutants': ['A national Strategy for Europe',\n",
       "   'A new Strategy for Europe',\n",
       "   'A strategic Strategy for Europe']},\n",
       " {'en': 'STOCKHOLM/MADRID – When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother – pleasant and rich with experience, but lacking the vitality and energy of the past.',\n",
       "  'mutants': ['STOCKHOLM/MADRID – When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother – young and rich with experience, but lacking the vitality and energy of the past.',\n",
       "   'STOCKHOLM/MADRID – When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother – old and rich with experience, but lacking the vitality and energy of the past.',\n",
       "   'STOCKHOLM/MADRID – When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother – strong and rich with experience, but lacking the vitality and energy of the past.']},\n",
       " {'en': 'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and forge a clear policy for tackling them.',\n",
       "  'mutants': ['It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and develop a clear policy for tackling them.',\n",
       "   'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and establish a clear policy for tackling them.',\n",
       "   'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and set a clear policy for tackling them.']},\n",
       " {'en': 'Admittedly, the pope’s characterization was alarmingly accurate in some respects.',\n",
       "  'mutants': ['but the pope’s characterization was alarmingly accurate in some respects.',\n",
       "   'yet the pope’s characterization was alarmingly accurate in some respects.',\n",
       "   'and the pope’s characterization was alarmingly accurate in some respects.']},\n",
       " {'en': 'But, despite its seeming lassitude, Europe retains significant strengths.',\n",
       "  'mutants': ['But, despite its seeming lassitude, Europe retains significant .',\n",
       "   'But, despite its seeming lassitude, Europe retains significant ;',\n",
       "   'But, despite its seeming lassitude, Europe retains significant !']},\n",
       " {'en': 'It is a hub of high-level thought and innovation; it is home to some of the world’s most competitive regions and industries; and, perhaps most impressive, it has built a community and market encompassing a half-billion people.',\n",
       "  'mutants': ['It is a hub of high-level thought and innovation; it is home to some of the world’s most competitive regions and industries; and, perhaps most impressive, it has built a community and market for a half-billion people.',\n",
       "   'It is a hub of high-level thought and innovation; it is home to some of the world’s most competitive regions and industries; and, perhaps most impressive, it has built a community and market of a half-billion people.',\n",
       "   'It is a hub of high-level thought and innovation; it is home to some of the world’s most competitive regions and industries; and, perhaps most impressive, it has built a community and market with a half-billion people.']},\n",
       " {'en': 'But the world is changing: the Asia-Pacific region is increasingly influencing global developments, economic and otherwise.',\n",
       "  'mutants': ['But the world is changing: the Asia-Pacific region is increasingly experiencing global developments, economic and otherwise.',\n",
       "   'But the world is changing: the Asia-Pacific region is increasingly undergoing global developments, economic and otherwise.',\n",
       "   'But the world is changing: the Asia-Pacific region is increasingly seeing global developments, economic and otherwise.']},\n",
       " {'en': 'The Trans-Pacific Partnership – by which the United States and 11 other countries would create a mega-regional free-trade zone – would most likely accelerate this shift (all the more so if China eventually joins).',\n",
       "  'mutants': ['The Trans-Pacific Partnership – by which the United States and 11 other countries would create a mega-regional free-trade zone – would most likely accelerate this process (all the more so if China eventually joins).',\n",
       "   'The Trans-Pacific Partnership – by which the United States and 11 other countries would create a mega-regional free-trade zone – would most likely accelerate this development (all the more so if China eventually joins).',\n",
       "   'The Trans-Pacific Partnership – by which the United States and 11 other countries would create a mega-regional free-trade zone – would most likely accelerate this growth (all the more so if China eventually joins).']},\n",
       " {'en': 'Though the TPP faces no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asia’s economic power cannot be underestimated.',\n",
       "  'mutants': ['Though the TPP has no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asia’s economic power cannot be underestimated.',\n",
       "   'Though the TPP faces no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asia’s economic power cannot be underestimated.',\n",
       "   'Though the TPP poses no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asia’s economic power cannot be underestimated.']},\n",
       " {'en': 'Europe must work to secure its position in the new world order – beginning by enhancing its own trade and investment ties with the US.',\n",
       "  'mutants': ['Europe must work to secure its position in the new world order – beginning by establishing its own trade and investment ties with the US.',\n",
       "   'Europe must work to secure its position in the new world order – beginning by building its own trade and investment ties with the US.',\n",
       "   'Europe must work to secure its position in the new world order – beginning by strengthening its own trade and investment ties with the US.']},\n",
       " {'en': 'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic controversies that the entire project may well be scuttled.',\n",
       "  'mutants': ['The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic politics that the entire project may well be scuttled.',\n",
       "   'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic problems that the entire project may well be scuttled.',\n",
       "   'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic issues that the entire project may well be scuttled.']},\n",
       " {'en': 'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring substantial economic benefits – a perception that many studies reinforce.',\n",
       "  'mutants': ['Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring significant economic benefits – a perception that many studies reinforce.',\n",
       "   'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring substantial economic benefits – a perception that many studies reinforce.',\n",
       "   'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring many economic benefits – a perception that many studies reinforce.']},\n",
       " {'en': 'Yet trivial issues – for example, the use of chlorinated chicken and settlements of investor disputes – continue to dominate the debate.',\n",
       "  'mutants': ['Yet other issues – for example, the use of chlorinated chicken and settlements of investor disputes – continue to dominate the debate.',\n",
       "   'Yet environmental issues – for example, the use of chlorinated chicken and settlements of investor disputes – continue to dominate the debate.',\n",
       "   'Yet legal issues – for example, the use of chlorinated chicken and settlements of investor disputes – continue to dominate the debate.']},\n",
       " {'en': 'The TTIP’s goal is to unleash the power of the transatlantic economy, which remains by far the world’s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       "  'mutants': ['The ultimate goal is to unleash the power of the transatlantic economy, which remains by far the world’s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       "   'The main goal is to unleash the power of the transatlantic economy, which remains by far the world’s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       "   'The stated goal is to unleash the power of the transatlantic economy, which remains by far the world’s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.']},\n",
       " {'en': '(If the TTIP was opened to other economies – such as Turkey, Mexico, and Canada – the benefits would be even greater.)',\n",
       "  'mutants': ['(If the TTIP was opened to other economies – such as Turkey, Mexico, and Canada – the benefits would be even .',\n",
       "   '(If the TTIP was opened to other economies – such as Turkey, Mexico, and Canada – the benefits would be even ;',\n",
       "   '(If the TTIP was opened to other economies – such as Turkey, Mexico, and Canada – the benefits would be even )']},\n",
       " {'en': 'Even more compelling than the benefits of achieving an agreement, though, are the potentially catastrophic consequences of failure.',\n",
       "  'mutants': ['Even more compelling than the benefits of achieving an agreement, though, are the potentially devastating consequences of failure.',\n",
       "   'Even more compelling than the benefits of achieving an agreement, though, are the potentially disastrous consequences of failure.',\n",
       "   'Even more compelling than the benefits of achieving an agreement, though, are the potentially fatal consequences of failure.']},\n",
       " {'en': 'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were implemented, the UK would be unwise – and thus unlikely – to leave.',\n",
       "  'mutants': ['For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were dissolved the UK would be unwise – and thus unlikely – to leave.',\n",
       "   'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were successful the UK would be unwise – and thus unlikely – to leave.',\n",
       "   'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were established the UK would be unwise – and thus unlikely – to leave.']},\n",
       " {'en': 'Moreover, the perception that the EU’s internal squabbles had led it to squander a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       "  'mutants': ['Moreover, the perception that the EU’s internal squabbles had led it to seize a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       "   'Moreover, the perception that the EU’s internal squabbles had led it to seek a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       "   'Moreover, the perception that the EU’s internal squabbles had led it to see a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.']},\n",
       " {'en': 'And Russian President Vladimir Putin would invariably regard the EU’s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       "  'mutants': ['And Russian President Vladimir Putin would later regard the EU’s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       "   'And Russian President Vladimir Putin would also regard the EU’s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       "   'And Russian President Vladimir Putin would likely regard the EU’s failure as a major opportunity to exert more influence over parts of Europe.']},\n",
       " {'en': 'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asia’s favor – and Europe will have few options, if any, for regaining its economic and geopolitical influence.',\n",
       "  'mutants': ['All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asia’s favor – and Europe will have few options, if any, for increasing its economic and geopolitical influence.',\n",
       "   'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asia’s favor – and Europe will have few options, if any, for reducing its economic and geopolitical influence.',\n",
       "   'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asia’s favor – and Europe will have few options, if any, for extending its economic and geopolitical influence.']},\n",
       " {'en': 'When the TTIP was first proposed, Europe seemed to recognize its value.',\n",
       "  'mutants': ['When the TTIP was first proposed, Europe failed to recognize its value.',\n",
       "   'When the TTIP was first proposed, Europe refused to recognize its value.',\n",
       "   'When the TTIP was first proposed, Europe began to recognize its value.']},\n",
       " {'en': 'Indeed, it was the EU that pushed the US, which initially doubted Europe’s commitment, to launch the negotiation process in June 2013.',\n",
       "  'mutants': ['Indeed, it was the negotiations that pushed the US, which initially doubted Europe’s commitment, to launch the negotiation process in June 2013.',\n",
       "   'Indeed, it was the us that pushed the US, which initially doubted Europe’s commitment, to launch the negotiation process in June 2013.',\n",
       "   'Indeed, it was the vote that pushed the US, which initially doubted Europe’s commitment, to launch the negotiation process in June 2013.']},\n",
       " {'en': 'The ambition was to complete the negotiations on “one tank of gas.”',\n",
       "  'mutants': ['The goal was to complete the negotiations on “one tank of gas.”',\n",
       "   'The plan was to complete the negotiations on “one tank of gas.”',\n",
       "   'The aim was to complete the negotiations on “one tank of gas.”']},\n",
       " {'en': 'No one wanted to endure protracted talks – or the associated political pain.',\n",
       "  'mutants': ['No one wanted to endure protracted talks – or the associated political pain.',\n",
       "   'No ones wanted to endure protracted talks – or the associated political pain.',\n",
       "   'No man wanted to endure protracted talks – or the associated political pain.']},\n",
       " {'en': 'But EU leaders essentially abandoned the project, seemingly confirming American fears.',\n",
       "  'mutants': ['But EU leaders essentially abandoned the project, further confirming American fears.',\n",
       "   'But EU leaders essentially abandoned the project, thus confirming American fears.',\n",
       "   'But EU leaders essentially abandoned the project, thereby confirming American fears.']},\n",
       " {'en': 'Trade negotiators struggled to make headway, while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europe’s democracy to its health.',\n",
       "  'mutants': ['Trade negotiators struggled to make concessions while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europe’s democracy to its health.',\n",
       "   'Trade negotiators struggled to make money while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europe’s democracy to its health.',\n",
       "   'Trade negotiators struggled to make gains while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europe’s democracy to its health.']},\n",
       " {'en': 'This is dangerously inaccurate talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       "  'mutants': ['This is dangerously small talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       "   'This is dangerously slow talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       "   'This is dangerously dangerous talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.']},\n",
       " {'en': 'And they must revive their commitment to conclude the talks successfully in 2015.',\n",
       "  'mutants': ['And they must show their commitment to conclude the talks successfully in 2015.',\n",
       "   'And they must maintain their commitment to conclude the talks successfully in 2015.',\n",
       "   'And they must demonstrate their commitment to conclude the talks successfully in 2015.']},\n",
       " {'en': 'This is not to say that resolving the remaining issues in the TTIP negotiations will be simple.',\n",
       "  'mutants': ['This is not to say that resolving the remaining issues in the TTIP negotiations will be .',\n",
       "   'This is not to say that resolving the remaining issues in the TTIP negotiations will be ;',\n",
       "   'This is not to say that resolving the remaining issues in the TTIP negotiations will be ?']},\n",
       " {'en': 'But establishing a trade agreement, especially one that entails so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       "  'mutants': ['But establishing a trade agreement, especially one that addresses so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       "   'But establishing a trade agreement, especially one that has so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       "   'But establishing a trade agreement, especially one that covers so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.']},\n",
       " {'en': 'The fact is that the challenges inherent in completing the TTIP are no more intractable than those that EU leaders have faced in the last few years of crisis.',\n",
       "  'mutants': ['The fact is that the challenges inherent in completing the TTIP are no more serious than those that EU leaders have faced in the last few years of crisis.',\n",
       "   'The fact is that the challenges inherent in completing the TTIP are no more severe than those that EU leaders have faced in the last few years of crisis.',\n",
       "   'The fact is that the challenges inherent in completing the TTIP are no more pressing than those that EU leaders have faced in the last few years of crisis.']},\n",
       " {'en': 'When the TTIP negotiations resume next month, EU leaders must push for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       "  'mutants': ['When the TTIP negotiations resume next month, EU leaders must look for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       "   'When the TTIP negotiations resume next month, EU leaders must hope for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       "   'When the TTIP negotiations resume next month, EU leaders must strive for genuine progress, with the goal of completing a deal by the end of the year.']},\n",
       " {'en': 'The good news is that the recent midterm elections in the US might have improved their chances.',\n",
       "  'mutants': ['The good news is that the recent midterm elections in the US might have improved their chances.',\n",
       "   'The good news is that the recent midterm elections in the US might have ruined their chances.',\n",
       "   'The good news is that the recent midterm elections in the US might have reduced their chances.']},\n",
       " {'en': 'President Barack Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       "  'mutants': ['President barack Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       "   'President joe Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       "   'President president Obama now might get so-called fast-track negotiating authority from Congress.']},\n",
       " {'en': 'If he does, Congress would simply approve or reject any negotiated agreement, rather than picking it apart.',\n",
       "  'mutants': ['If he does, Congress would simply approve or reject any negotiated agreement, rather than tear it apart.',\n",
       "   'If he does, Congress would simply approve or reject any negotiated agreement, rather than tearing it apart.',\n",
       "   'If he does, Congress would simply approve or reject any negotiated agreement, rather than take it apart.']},\n",
       " {'en': 'The US presidential election season is starting, and other issues in the new year could easily take over the EU agenda.',\n",
       "  'mutants': ['The US presidential election season is starting, and other issues in the new year could easily take over the EU agenda.',\n",
       "   'The US presidential election season is starting, and other issues in the new year could possibly take over the EU agenda.',\n",
       "   'The US presidential election season is starting, and other issues in the new year could soon take over the EU agenda.']},\n",
       " {'en': 'That is why Europe’s leaders have no time to waste.',\n",
       "  'mutants': ['That is why Europe’s leaders have no time to waste.',\n",
       "   'That is why Europe’s governments have no time to waste.',\n",
       "   'That is why Europe’s nations have no time to waste.']},\n",
       " {'en': 'They must seize economic opportunity – and avert strategic disaster.',\n",
       "  'mutants': ['They can seize economic opportunity – and avert strategic disaster.',\n",
       "   'They must seize economic opportunity – and avert strategic disaster.',\n",
       "   'They will seize economic opportunity – and avert strategic disaster.']},\n",
       " {'en': 'The Year That Ended an Epoch?',\n",
       "  'mutants': ['The Year That was an Epoch?',\n",
       "   'The Year That became an Epoch?',\n",
       "   'The Year That is an Epoch?']},\n",
       " {'en': 'MADRID – As 2016 comes to an end, the outlook for 2017 is shrouded in uncertainty.',\n",
       "  'mutants': ['MADRID – As 2016 comes to an end, the outlook for 2017 is still in uncertainty.',\n",
       "   'MADRID – As 2016 comes to an end, the outlook for 2017 is already in uncertainty.',\n",
       "   'MADRID – As 2016 comes to an end, the outlook for 2017 is also in uncertainty.']},\n",
       " {'en': 'Tensions in the Middle East are rising, and populist movements have appeared in Europe and the United States.',\n",
       "  'mutants': ['Tensions in the Middle East are rising and populist movements have appeared in Europe and the United States.',\n",
       "   'Tensions in the Middle East are high and populist movements have appeared in Europe and the United States.',\n",
       "   'Tensions in the Middle East are increasing and populist movements have appeared in Europe and the United States.']},\n",
       " {'en': 'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at rapprochement, which were marred by the fundamental disagreement about Syrian President Bashar al-Assad’s future role in any peace process or political transition.',\n",
       "  'mutants': ['In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at reconciliation which were marred by the fundamental disagreement about Syrian President Bashar al-Assad’s future role in any peace process or political transition.',\n",
       "   'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at resolution which were marred by the fundamental disagreement about Syrian President Bashar al-Assad’s future role in any peace process or political transition.',\n",
       "   'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at peace which were marred by the fundamental disagreement about Syrian President Bashar al-Assad’s future role in any peace process or political transition.']},\n",
       " {'en': 'Meanwhile, over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo – once Syria’s largest city, now utterly devastated by the war.',\n",
       "  'mutants': ['but over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo – once Syria’s largest city, now utterly devastated by the war.',\n",
       "   'and over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo – once Syria’s largest city, now utterly devastated by the war.',\n",
       "   'also over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo – once Syria’s largest city, now utterly devastated by the war.']}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutant_pairs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sys = wn.synsets(\"book\", pos = wn.VERB)\n",
    "\n",
    "#[str(lemma.name()) for lemma in wn.synset(sys[0].name()).lemmas()]\n",
    "sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['however',\n",
       " 'nevertheless',\n",
       " 'withal',\n",
       " 'still',\n",
       " 'yet',\n",
       " 'all_the_same',\n",
       " 'even_so',\n",
       " 'nonetheless',\n",
       " 'notwithstanding']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('however', wn.ADV)[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrase ('brother', ['buddy', 'blood_brother', 'chum', 'sidekick', 'pal', 'comrade', 'crony'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def tag(sentence):\n",
    " words = word_tokenize(sentence)\n",
    " words = pos_tag(words)\n",
    " return words\n",
    "\n",
    "def paraphraseable(tag):\n",
    " return tag.startswith('N') or tag.startswith('V') or tag.startswith('J') or tag.startswith('R')\n",
    "\n",
    "def pos(tag):\n",
    " if tag.startswith('N'):\n",
    "  return wn.NOUN\n",
    " elif tag.startswith('V'):\n",
    "  return wn.VERB\n",
    " elif tag.startswith('J'):\n",
    "  return wn.ADJ\n",
    " elif pos.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "\n",
    "def synonyms(word, tag):\n",
    "    lemma_lists = [ss.lemmas() for ss in wn.synsets(word, pos(tag))]\n",
    "    lemmas = [lemma.name() for lemma in sum(lemma_lists, [])]\n",
    "    return set(lemmas)\n",
    "\n",
    "def synonymIfExists(sentence, index):\n",
    "   tags = tag(sentence)\n",
    "   word, t = tags[index]\n",
    "   if paraphraseable(t):\n",
    "    syns = synonyms(word, t)\n",
    "    if syns:\n",
    "      syns_list = [s for s in syns if s.lower() != word.lower()]\n",
    "      return word, syns_list\n",
    "    return word, []\n",
    "\n",
    "def paraphrase(sentence):\n",
    " return [x for x in synonymIfExists(sentence)]\n",
    "get=[]\n",
    "get=synonymIfExists(\"I am his old brother\", 4)\n",
    "print(\"paraphrase\",get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我是他的老兄弟']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer([\"I am his old blood brother\"], return_tensors=\"pt\")\n",
    "outputs = model.generate(input.input_ids, output_scores=True, return_dict_in_generate=True, output_hidden_states=True)\n",
    "tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeamSearchEncoderDecoderOutput(sequences=tensor([[65000,  2672,  2320,  2554,  7370,     0]]), sequences_scores=tensor([-0.6191]), scores=(tensor([[-8.0706e+00, -1.3176e+01, -1.8279e+01,  ..., -2.0530e+01,\n",
       "         -2.0005e+01,        -inf],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09,        -inf],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09,        -inf],\n",
       "        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09,        -inf]]), tensor([[ -9.0575,  -9.1186,  -9.8088,  ..., -19.7308, -25.6526,     -inf],\n",
       "        [-12.3644, -10.8325,  -6.5922,  ..., -20.7825, -25.4118,     -inf],\n",
       "        [-12.9211, -14.8706, -18.3882,  ..., -26.8826, -28.8784,     -inf],\n",
       "        [-10.2162,  -7.9428,  -7.1700,  ..., -21.1276, -23.4055,     -inf]]), tensor([[ -9.6594, -10.5615, -10.2936,  ..., -21.6635, -25.1355,     -inf],\n",
       "        [ -9.1250,  -9.4463,  -8.5660,  ..., -21.3557, -23.9969,     -inf],\n",
       "        [ -8.5816,  -9.3940, -10.3983,  ..., -21.2435, -24.6133,     -inf],\n",
       "        [-12.1354, -14.8067, -16.3441,  ..., -26.1704, -29.6275,     -inf]]), tensor([[ -6.6145,  -8.1378, -10.1964,  ..., -20.3942, -22.8144,     -inf],\n",
       "        [ -7.7982,  -8.6290, -10.3179,  ..., -21.1071, -23.7560,     -inf],\n",
       "        [-12.4997, -12.6737, -13.5503,  ..., -23.9351, -26.8833,     -inf],\n",
       "        [ -9.7994, -13.1096, -13.0252,  ..., -25.3591, -27.4971,     -inf]]), tensor([[ -3.0957, -13.0173,  -6.6229,  ..., -22.6768, -26.0252,     -inf],\n",
       "        [ -3.4096, -13.8650,  -7.8142,  ..., -23.7624, -26.8911,     -inf],\n",
       "        [ -3.5840, -13.2102,  -7.5418,  ..., -23.0925, -26.4841,     -inf],\n",
       "        [ -3.7401, -12.9096,  -6.9442,  ..., -22.5029, -26.3529,     -inf]]), tensor([[ -5.6081, -17.7817, -18.3308,  ..., -26.5614, -25.6657,     -inf],\n",
       "        [ -6.1124, -18.6245, -19.7648,  ..., -27.0204, -25.8851,     -inf],\n",
       "        [ -6.3872, -18.3671, -19.2258,  ..., -26.9236, -25.9835,     -inf],\n",
       "        [ -6.5506, -18.2036, -19.2827,  ..., -26.8354, -25.9512,     -inf]])), encoder_attentions=None, encoder_hidden_states=(tensor([[[ 0.3468, -0.0593,  0.3446,  ..., -0.3040, -0.1900,  0.4216],\n",
       "         [ 1.8861,  0.8580,  0.6001,  ..., -0.2460,  0.3780,  0.0996],\n",
       "         [ 0.9865,  1.0367,  0.8521,  ...,  0.8540, -0.8031,  0.8809],\n",
       "         ...,\n",
       "         [-1.1959, -0.8270,  0.0870,  ..., -0.8830, -0.9582,  1.8733],\n",
       "         [-1.7496, -0.6921, -0.7966,  ...,  1.2136,  0.2179,  0.6713],\n",
       "         [-0.0220,  0.3182, -0.9137,  ...,  0.6230,  0.6750,  0.2616]]]), tensor([[[ 0.4042, -0.2027, -0.6349,  ..., -0.1734, -0.7449, -0.6875],\n",
       "         [ 1.0035,  0.9140,  0.4521,  ..., -0.8240, -0.1325, -0.3888],\n",
       "         [ 0.3238,  1.4224,  0.3470,  ...,  0.5625, -0.8859,  1.0611],\n",
       "         ...,\n",
       "         [-0.3398, -1.1085,  0.0534,  ..., -1.0491, -0.2823,  1.3319],\n",
       "         [-0.9108, -0.2572, -0.9972,  ...,  1.1047,  0.0889,  0.8161],\n",
       "         [-0.0602, -0.0428, -0.1500,  ..., -0.1825,  1.9735, -0.0350]]]), tensor([[[ 0.5426,  0.5221, -0.4740,  ..., -0.0795, -0.9348, -0.4579],\n",
       "         [ 0.0755,  1.0037,  0.8955,  ..., -0.7115,  0.4295, -0.0625],\n",
       "         [ 0.3175,  0.6033,  1.1177,  ..., -0.1558, -1.3818,  0.7759],\n",
       "         ...,\n",
       "         [ 0.0419, -0.5635, -0.2700,  ..., -1.2772, -0.1498,  1.4286],\n",
       "         [-0.1115, -0.6901, -0.6778,  ...,  0.8612,  0.1510,  0.7598],\n",
       "         [ 0.0522, -0.1320,  0.0451,  ..., -0.1582,  0.4066,  0.0842]]]), tensor([[[ 0.5946,  0.6063, -0.5754,  ..., -0.1866, -0.8190, -0.6433],\n",
       "         [ 0.2618,  0.5792,  0.2477,  ..., -0.5050,  0.0084,  0.0324],\n",
       "         [ 0.2178, -0.3474,  0.0389,  ...,  0.1920, -1.3821,  0.8041],\n",
       "         ...,\n",
       "         [-0.1788, -0.9146, -0.0559,  ..., -1.6132,  0.2544,  0.7030],\n",
       "         [ 0.7344, -0.2794,  0.1558,  ...,  0.4995, -0.0865,  0.1123],\n",
       "         [-0.0957, -0.1167, -0.1632,  ..., -0.2206,  0.0261,  0.1475]]]), tensor([[[ 0.3016,  0.1708, -0.1303,  ...,  0.3287, -0.3307,  0.3492],\n",
       "         [-0.2168,  0.0154,  0.6645,  ...,  0.1963,  0.0291,  0.0807],\n",
       "         [ 0.6471, -0.7146, -0.3265,  ...,  0.1714, -1.2405,  0.7126],\n",
       "         ...,\n",
       "         [ 0.2570, -0.3550, -0.7931,  ..., -1.5357,  0.4861,  0.1961],\n",
       "         [ 1.1568,  0.1955, -0.6755,  ...,  0.5843,  0.1092,  0.3390],\n",
       "         [-0.1551, -0.2164, -0.0707,  ..., -0.0876, -0.1131,  0.4097]]]), tensor([[[-0.0600,  0.5779, -0.6818,  ...,  0.7986, -0.9508,  0.4424],\n",
       "         [ 0.3136, -0.1603,  0.1079,  ...,  0.5084, -0.0677,  0.0628],\n",
       "         [ 0.9403, -0.8329,  0.4227,  ..., -0.3357, -0.9872,  0.1242],\n",
       "         ...,\n",
       "         [ 0.1932, -0.2600, -0.5308,  ..., -1.7123,  0.6601, -0.3139],\n",
       "         [ 0.7223, -0.0659, -0.1924,  ...,  0.1370,  0.0269,  0.3414],\n",
       "         [-0.1491, -0.1682, -0.1788,  ..., -0.0258, -0.2394,  0.2459]]]), tensor([[[ 0.1719, -0.0067, -0.4274,  ...,  0.7945, -0.4561,  0.5338],\n",
       "         [ 0.1281, -0.5929,  0.2467,  ...,  0.3022,  0.2956,  0.1809],\n",
       "         [ 0.2407, -0.4569,  0.5994,  ..., -0.5339, -0.3741,  0.0461],\n",
       "         ...,\n",
       "         [ 0.2606, -0.2287, -0.3382,  ..., -1.1979,  0.5374, -0.2935],\n",
       "         [ 0.3392, -0.0884, -0.1897,  ..., -0.2339, -0.1555, -0.1211],\n",
       "         [-0.0370, -0.0594, -0.0266,  ..., -0.1394, -0.2151,  0.0423]]])), decoder_attentions=None, cross_attentions=None, decoder_hidden_states=((tensor([[[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]],\n",
       "\n",
       "        [[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]],\n",
       "\n",
       "        [[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]],\n",
       "\n",
       "        [[1.2311, 1.7610, 1.8055,  ..., 1.9265, 2.0761, 1.9213]]]), tensor([[[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]],\n",
       "\n",
       "        [[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]],\n",
       "\n",
       "        [[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]],\n",
       "\n",
       "        [[ 0.1585,  0.0087,  0.0095,  ..., -0.0497, -0.1141, -0.1549]]]), tensor([[[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]],\n",
       "\n",
       "        [[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]],\n",
       "\n",
       "        [[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]],\n",
       "\n",
       "        [[-0.3569,  0.1139,  0.1827,  ..., -0.0453, -0.0340,  0.0386]]]), tensor([[[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]],\n",
       "\n",
       "        [[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]],\n",
       "\n",
       "        [[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]],\n",
       "\n",
       "        [[-0.1119,  0.0493,  0.0623,  ...,  0.0062,  0.0206, -0.0714]]]), tensor([[[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]],\n",
       "\n",
       "        [[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]],\n",
       "\n",
       "        [[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]],\n",
       "\n",
       "        [[-0.0365, -0.0064,  0.0248,  ..., -0.0705, -0.1024,  0.0496]]]), tensor([[[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]],\n",
       "\n",
       "        [[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]],\n",
       "\n",
       "        [[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]],\n",
       "\n",
       "        [[-0.1040, -0.3599,  0.3417,  ..., -0.6031,  0.0473, -0.1218]]]), tensor([[[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]],\n",
       "\n",
       "        [[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]],\n",
       "\n",
       "        [[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]],\n",
       "\n",
       "        [[ 1.9405, -0.7533,  4.2732,  ..., -1.5543,  3.5870,  2.2154]]])), (tensor([[[ 1.3752,  0.7252,  1.7303,  ...,  0.1608,  0.7895,  0.4669]],\n",
       "\n",
       "        [[ 0.8758,  0.6905,  1.7954,  ...,  0.1587,  0.0176,  0.1273]],\n",
       "\n",
       "        [[ 0.9412,  0.9064,  1.8858,  ...,  0.6706,  0.8403,  0.5027]],\n",
       "\n",
       "        [[ 0.9093,  0.7428,  0.7795,  ..., -0.1225, -0.4390,  0.1416]]]), tensor([[[-0.6338,  0.1900, -0.0081,  ..., -0.8440,  0.6493, -0.0964]],\n",
       "\n",
       "        [[-0.0084, -0.3259, -0.8432,  ..., -0.0562, -0.1005, -0.3610]],\n",
       "\n",
       "        [[-0.7341, -0.0110,  0.1218,  ...,  0.0981,  1.4101, -0.1092]],\n",
       "\n",
       "        [[ 0.8696,  0.7684, -0.4003,  ...,  0.3899,  0.1323, -0.2550]]]), tensor([[[-0.1881,  0.9668,  0.2671,  ..., -1.4565,  0.5286,  0.1009]],\n",
       "\n",
       "        [[ 0.0839, -0.2131, -0.8653,  ..., -0.3846,  0.4270, -0.2407]],\n",
       "\n",
       "        [[-0.4180, -1.4671, -0.0554,  ...,  0.6997,  1.7747, -0.4086]],\n",
       "\n",
       "        [[ 0.6240, -0.0718, -0.1535,  ...,  0.5612,  0.4912, -0.6020]]]), tensor([[[-0.7800,  1.1222,  0.3322,  ..., -1.2844,  0.4637,  0.3102]],\n",
       "\n",
       "        [[-0.0977, -0.0200, -0.7190,  ..., -0.5539,  0.7813,  0.2757]],\n",
       "\n",
       "        [[-1.2829, -1.2407,  0.3688,  ...,  0.8264,  2.3903,  0.0200]],\n",
       "\n",
       "        [[ 0.1942, -0.1899,  0.3262,  ...,  0.2960,  0.4458, -0.2559]]]), tensor([[[-1.0633,  0.9297,  0.4906,  ..., -0.3730,  0.9208,  0.5365]],\n",
       "\n",
       "        [[-0.1933,  0.1830, -0.2552,  ..., -0.0853,  0.8656,  0.2406]],\n",
       "\n",
       "        [[-0.4999, -1.3624,  0.6060,  ...,  0.3297,  0.6816, -0.1871]],\n",
       "\n",
       "        [[ 0.3026, -0.1067,  1.1289,  ..., -0.0716,  0.0493,  0.4905]]]), tensor([[[-0.8745,  0.0561, -0.9117,  ..., -0.1073,  1.6905,  0.1678]],\n",
       "\n",
       "        [[ 0.1465,  0.6778, -0.5368,  ..., -0.0250,  1.0447,  0.3079]],\n",
       "\n",
       "        [[-0.1163, -2.0070,  0.3803,  ..., -0.8711,  2.0000, -0.3430]],\n",
       "\n",
       "        [[-0.6518,  0.1179,  1.7705,  ..., -0.6410, -0.5425,  0.3525]]]), tensor([[[ 2.3627, -0.5658, -4.2257,  ...,  0.3660,  5.0970, -1.1923]],\n",
       "\n",
       "        [[ 8.7663,  6.7017, -0.5519,  ...,  6.4116,  6.8683,  2.3494]],\n",
       "\n",
       "        [[ 3.7553, -5.1326,  3.2195,  ..., -1.2554, 10.1023, -1.0150]],\n",
       "\n",
       "        [[-0.7372,  0.9929,  5.2282,  ..., -2.4213,  0.5672,  1.8538]]])), (tensor([[[ 1.3077,  1.6365,  1.2665,  ...,  0.2984,  1.1858,  0.9258]],\n",
       "\n",
       "        [[ 1.3910,  1.2257,  1.0497,  ...,  0.6650,  1.0174,  0.3155]],\n",
       "\n",
       "        [[ 1.9161,  1.4189,  1.4074,  ...,  0.5770, -0.2562,  0.6835]],\n",
       "\n",
       "        [[ 0.9015,  1.2208,  1.1346,  ...,  0.2146,  0.9037,  0.0937]]]), tensor([[[ 0.3431, -0.5637,  0.3787,  ..., -0.3443,  0.8691,  0.0596]],\n",
       "\n",
       "        [[-0.2141, -0.4640, -0.1602,  ...,  0.3578,  0.3771, -0.3066]],\n",
       "\n",
       "        [[ 1.8768,  0.0594,  0.1546,  ..., -0.1605, -0.1369,  0.6260]],\n",
       "\n",
       "        [[-1.0154, -0.0353, -0.3643,  ..., -0.5874,  0.8222, -0.3101]]]), tensor([[[ 0.1165, -0.7664,  0.6281,  ..., -0.4668,  1.1748, -0.6576]],\n",
       "\n",
       "        [[-0.0373, -0.4419,  0.2965,  ...,  0.3401,  0.7440, -0.9097]],\n",
       "\n",
       "        [[ 1.2355, -0.3778, -0.1583,  ..., -0.3043,  0.3302, -0.0568]],\n",
       "\n",
       "        [[-0.2951,  0.6815,  0.0999,  ..., -1.1500,  0.4647, -0.2123]]]), tensor([[[-0.2989, -0.6664,  0.9231,  ..., -0.3308,  0.3051, -0.3776]],\n",
       "\n",
       "        [[-0.1281, -0.2125,  0.3366,  ...,  0.2509,  0.6282, -0.3133]],\n",
       "\n",
       "        [[ 1.0821,  0.0740, -0.4274,  ..., -0.4342,  0.3453, -0.5712]],\n",
       "\n",
       "        [[-1.1076,  1.0402,  0.2219,  ..., -1.1132,  0.4350,  0.0209]]]), tensor([[[-0.0963, -0.5139,  1.1370,  ..., -0.2954, -0.2420,  0.3309]],\n",
       "\n",
       "        [[-0.0392, -0.1226,  0.9448,  ...,  0.0526,  0.1083,  0.1754]],\n",
       "\n",
       "        [[ 0.2952, -0.4682, -0.3861,  ..., -0.3740,  0.2446,  0.0950]],\n",
       "\n",
       "        [[-1.1077,  0.9542,  0.1811,  ..., -0.4618,  0.7419,  0.3958]]]), tensor([[[ 0.5223, -0.3105,  1.4012,  ...,  0.0818, -0.5787,  0.4374]],\n",
       "\n",
       "        [[ 0.0909, -0.1317,  1.0713,  ...,  0.6061,  0.5060, -0.0129]],\n",
       "\n",
       "        [[ 0.3697, -0.1355, -0.5999,  ..., -0.0891, -0.5396, -0.1718]],\n",
       "\n",
       "        [[-0.9060,  0.3342, -1.0973,  ..., -0.5997,  1.6889, -0.0865]]]), tensor([[[ 6.8671,  0.3144,  1.9647,  ...,  5.1155, -5.2701, -0.2905]],\n",
       "\n",
       "        [[ 8.4827, -0.0141,  2.9547,  ..., 10.1506,  1.4818,  0.7651]],\n",
       "\n",
       "        [[ 5.9604,  1.7759, -3.3866,  ...,  6.2326, -5.6534, -2.4670]],\n",
       "\n",
       "        [[ 4.3836, -0.2963, -3.9889,  ..., -1.4357,  5.2408, -2.0975]]])), (tensor([[[ 1.1479,  0.7276,  0.7920,  ...,  0.5770, -0.2562,  0.6835]],\n",
       "\n",
       "        [[ 1.1479,  0.7276,  0.7920,  ...,  0.5770, -0.2562,  0.6835]],\n",
       "\n",
       "        [[ 0.6448,  0.4286,  0.7745,  ...,  0.0927,  0.5194, -0.2204]],\n",
       "\n",
       "        [[ 0.1630,  0.9444,  1.0975,  ...,  0.5192,  0.4252,  0.7792]]]), tensor([[[ 2.0141, -0.2072,  0.1000,  ..., -0.2684, -0.0357,  0.6847]],\n",
       "\n",
       "        [[ 1.9596,  0.1509, -0.0545,  ..., -0.1658, -0.0846,  0.7015]],\n",
       "\n",
       "        [[ 0.5406,  0.4224, -0.1765,  ..., -0.6808, -0.2171, -1.2616]],\n",
       "\n",
       "        [[ 0.8790, -0.1194, -0.1433,  ..., -0.7188,  0.4968,  0.7401]]]), tensor([[[ 1.7136, -0.8766, -0.0958,  ..., -0.6825,  0.4487,  0.2855]],\n",
       "\n",
       "        [[ 1.5217, -0.4128, -0.2345,  ..., -0.4097,  0.3929,  0.1266]],\n",
       "\n",
       "        [[ 0.5875, -0.7104,  0.0309,  ..., -1.1978,  0.3864, -1.7069]],\n",
       "\n",
       "        [[ 1.0982, -1.0355, -0.1570,  ..., -0.4095,  0.4333,  0.4515]]]), tensor([[[ 1.8342, -0.6643, -0.3671,  ..., -0.8203,  0.5229, -0.3958]],\n",
       "\n",
       "        [[ 1.5945, -0.1929, -0.6108,  ..., -0.4931,  0.2455, -0.3553]],\n",
       "\n",
       "        [[ 0.7768, -0.4572,  0.9122,  ..., -1.0015,  0.1091, -1.1313]],\n",
       "\n",
       "        [[ 1.2674, -0.6651, -0.2644,  ..., -0.5892,  0.1734,  0.6201]]]), tensor([[[ 0.8612, -1.1590, -0.3666,  ..., -0.7846,  0.2569,  0.3278]],\n",
       "\n",
       "        [[ 0.8045, -0.8630, -0.3976,  ..., -0.5012,  0.1785,  0.3768]],\n",
       "\n",
       "        [[ 0.1488, -0.1971,  1.1647,  ..., -0.8721,  0.0747, -0.7533]],\n",
       "\n",
       "        [[ 1.3568,  0.0945,  0.1329,  ..., -0.8566,  0.6048,  0.4781]]]), tensor([[[ 0.7099, -0.6942, -0.0769,  ..., -0.0860, -0.6823,  0.1207]],\n",
       "\n",
       "        [[ 0.8126, -0.3899, -0.2184,  ...,  0.1780, -0.6579, -0.1683]],\n",
       "\n",
       "        [[ 0.9170, -0.1253,  1.7311,  ...,  0.0699,  0.5918, -0.2794]],\n",
       "\n",
       "        [[ 1.5542,  0.2263,  0.1688,  ...,  0.0418,  0.2775,  0.1577]]]), tensor([[[ 5.3168,  0.6143, -2.9551,  ...,  5.6563, -7.5836, -2.1464]],\n",
       "\n",
       "        [[ 6.5826,  0.9498, -2.2169,  ...,  8.3863, -7.0443, -2.2352]],\n",
       "\n",
       "        [[ 8.3926, -0.5893,  6.6866,  ...,  4.8648,  1.4328,  0.2441]],\n",
       "\n",
       "        [[13.2284,  3.8744, -0.7293,  ..., 10.9088, -1.1516,  2.2559]]])), (tensor([[[-0.4513, -0.6551, -0.1825,  ...,  0.3269,  0.3788,  0.0785]],\n",
       "\n",
       "        [[-0.8269,  0.1611, -0.4391,  ...,  0.2206,  1.0618,  0.4410]],\n",
       "\n",
       "        [[-0.6121, -0.0013, -0.3834,  ...,  0.9918,  0.0318,  0.7557]],\n",
       "\n",
       "        [[-0.6121, -0.0013, -0.3834,  ...,  0.9918,  0.0318,  0.7557]]]), tensor([[[ 1.2487,  0.2139, -0.9652,  ...,  0.1044, -0.1009, -0.0617]],\n",
       "\n",
       "        [[ 0.9065,  0.5728, -0.1066,  ..., -0.2096, -0.1985, -0.0552]],\n",
       "\n",
       "        [[ 0.9858,  0.2660, -0.9309,  ..., -0.1074, -0.3745,  0.3543]],\n",
       "\n",
       "        [[ 0.9762,  0.2837, -0.9455,  ..., -0.1105, -0.3612,  0.3532]]]), tensor([[[ 1.1193,  0.2529, -0.9176,  ...,  0.1810,  0.2754, -0.4644]],\n",
       "\n",
       "        [[ 1.0675, -0.1174, -0.5790,  ..., -0.0618,  0.2600, -0.7228]],\n",
       "\n",
       "        [[ 1.1154, -0.3488, -1.1426,  ...,  0.0722,  0.2476, -0.0287]],\n",
       "\n",
       "        [[ 0.9424, -0.2078, -1.1723,  ...,  0.1864,  0.2657, -0.1628]]]), tensor([[[ 0.7463,  0.6394, -0.3111,  ...,  0.4897, -0.4000, -0.4116]],\n",
       "\n",
       "        [[ 0.3099,  0.4253, -0.3272,  ..., -0.0155, -0.3719, -1.1136]],\n",
       "\n",
       "        [[ 0.5298, -0.1925, -0.5699,  ...,  0.2893, -0.1962, -0.3401]],\n",
       "\n",
       "        [[ 0.3801, -0.0242, -0.6691,  ...,  0.6287, -0.1248, -0.1957]]]), tensor([[[ 0.8432,  0.1680,  0.3303,  ...,  0.1875, -0.2814, -0.3347]],\n",
       "\n",
       "        [[ 0.3424,  0.3089, -0.0638,  ...,  0.1112, -0.1223, -0.8203]],\n",
       "\n",
       "        [[ 0.5029, -0.2868, -0.3744,  ...,  0.2949, -0.0760, -0.3560]],\n",
       "\n",
       "        [[ 0.3802,  0.0486, -0.2333,  ...,  0.5928, -0.1190, -0.3305]]]), tensor([[[ 0.7273,  0.0840,  1.1742,  ..., -0.0510, -1.2408, -0.7058]],\n",
       "\n",
       "        [[ 0.0014,  0.1600,  0.8180,  ..., -0.1004, -0.7970, -1.0187]],\n",
       "\n",
       "        [[ 0.7785, -0.4086, -0.1639,  ..., -0.1127, -1.1221, -0.7342]],\n",
       "\n",
       "        [[ 0.7466, -0.0103,  0.0592,  ...,  0.1794, -1.1372, -0.7856]]]), tensor([[[ 3.5050,  3.3481,  2.3852,  ...,  2.6766, -4.5272, -3.9199]],\n",
       "\n",
       "        [[-0.4249,  2.1750,  1.6835,  ...,  1.3572, -2.4010, -4.3397]],\n",
       "\n",
       "        [[ 4.2093,  2.4347, -2.2539,  ...,  3.2859, -4.9463, -1.8884]],\n",
       "\n",
       "        [[ 4.0194,  3.1173, -0.6455,  ...,  5.0434, -3.8894, -2.3015]]])), (tensor([[[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]],\n",
       "\n",
       "        [[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]],\n",
       "\n",
       "        [[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]],\n",
       "\n",
       "        [[-0.5281, -0.1531, -0.6374,  ..., -0.0910,  0.1581,  0.2913]]]), tensor([[[ 0.5770,  0.4768, -0.4539,  ...,  0.1900,  0.0651,  0.2025]],\n",
       "\n",
       "        [[ 0.7021,  0.4195, -0.3997,  ...,  0.1820,  0.0366,  0.2762]],\n",
       "\n",
       "        [[ 0.6558,  0.4452, -0.4901,  ...,  0.2179, -0.0025,  0.1606]],\n",
       "\n",
       "        [[ 0.6521,  0.4601, -0.4922,  ...,  0.2187,  0.0051,  0.1552]]]), tensor([[[ 0.4750,  0.1106, -0.0414,  ...,  0.1398,  0.2365,  0.0154]],\n",
       "\n",
       "        [[ 0.4723,  0.0440,  0.0048,  ...,  0.1891,  0.1455,  0.1075]],\n",
       "\n",
       "        [[ 0.4302,  0.0800, -0.1065,  ...,  0.1249,  0.2341, -0.0224]],\n",
       "\n",
       "        [[ 0.4415,  0.0529, -0.1155,  ...,  0.1859,  0.2077, -0.0267]]]), tensor([[[ 0.4703,  0.3315,  0.0085,  ...,  0.0076, -0.1374, -0.4071]],\n",
       "\n",
       "        [[ 0.3847,  0.2337, -0.0981,  ...,  0.1733, -0.1773, -0.3822]],\n",
       "\n",
       "        [[ 0.3872,  0.2036, -0.1225,  ...,  0.0332, -0.1402, -0.4343]],\n",
       "\n",
       "        [[ 0.3451,  0.2346, -0.1439,  ...,  0.1459, -0.1980, -0.3148]]]), tensor([[[ 0.4618,  0.0932, -0.0572,  ...,  0.3011, -1.3014, -0.1619]],\n",
       "\n",
       "        [[ 0.3835,  0.0755, -0.1499,  ...,  0.4640, -1.3007, -0.1690]],\n",
       "\n",
       "        [[ 0.4286, -0.0084, -0.2452,  ...,  0.3620, -1.2974, -0.1465]],\n",
       "\n",
       "        [[ 0.4482, -0.0089, -0.3100,  ...,  0.4459, -1.3416, -0.1144]]]), tensor([[[ 0.0903, -0.5004,  0.7909,  ..., -0.5611, -1.6997, -0.8448]],\n",
       "\n",
       "        [[ 0.0453, -0.4721,  0.6763,  ..., -0.5324, -1.5529, -0.8912]],\n",
       "\n",
       "        [[ 0.1036, -0.5550,  0.6311,  ..., -0.5081, -1.6995, -0.8432]],\n",
       "\n",
       "        [[ 0.1602, -0.5452,  0.5648,  ..., -0.5043, -1.7187, -0.7966]]]), tensor([[[-1.1703, -0.2199,  3.7070,  ..., -2.5994, -3.1775, -2.2700]],\n",
       "\n",
       "        [[-1.3739,  0.0971,  3.1880,  ..., -3.1332, -2.7319, -2.6753]],\n",
       "\n",
       "        [[-1.1799, -0.2546,  3.1743,  ..., -2.2641, -2.9037, -2.2557]],\n",
       "\n",
       "        [[-0.9040, -0.1390,  3.0676,  ..., -2.3530, -2.6203, -2.1989]]]))))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 14:01:50.942 | DEBUG    | text2vec.sentence_model:__init__:74 - Use device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7635359  -0.39156067  0.63999367 ...  0.2583298  -0.5223585\n",
      "  -0.21127473]\n",
      " [ 1.2544321  -0.17131853  1.1561146  ...  0.10490549 -0.32768762\n",
      "  -0.14699247]]\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel\n",
    "sentences = ['我是他的老兄弟', '我不是他的老弟']\n",
    "\n",
    "model2 = SentenceModel('shibing624/text2vec-base-chinese')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003824949264526"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "cos = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0153, -0.0026,  0.0152,  ..., -0.0576, -0.0526, -0.0256],\n",
       "         [ 0.0462,  0.0016, -0.0089,  ..., -0.0551, -0.0275, -0.0398],\n",
       "         [ 0.0034,  0.0044, -0.0047,  ..., -0.0065, -0.0797, -0.0053],\n",
       "         [-0.0086,  0.0047,  0.0133,  ..., -0.0568,  0.0043, -0.0624],\n",
       "         [-0.0349,  0.0133,  0.0089,  ...,  0.0094, -0.0346, -0.0145],\n",
       "         [ 0.0114,  0.0351, -0.0119,  ..., -0.0167, -0.0144, -0.0326]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我喜欢狗。']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=tokenizer('I like dog.',return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input, output_scores=True, return_dict_in_generate=True, output_hidden_states=True)\n",
    "tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0308, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get your batch data: token_id, mask and labels\n",
    "token_ids = tokenizer('Meanwhile, over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo – once Syria’s largest city, now utterly devastated by the war.', return_tensors=\"pt\").input_ids\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer('此外，过去一周，叙利亚政府军在俄罗斯和伊朗的支持下，已经夺回了几乎整个阿勒颇——阿勒颇曾是叙利亚最大的城市，现在已经彻底毁灭在战火之中。', return_tensors=\"pt\").input_ids\n",
    "# get your token embeddings\n",
    "token_embeds=model.get_input_embeddings().weight[token_ids].clone()\n",
    "token_embeds.retain_grad() \n",
    "# get model output that contains loss value\n",
    "outs = model(inputs_embeds=token_embeds,labels=labels, output_hidden_states=True, output_attentions=True)\n",
    "loss=outs.loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 512])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "grad=token_embeds.grad\n",
    "grad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "idx = np.argsort(torch.norm(grad, dim=2).squeeze(dim=0)).tolist()[::-1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['over',\n",
       " ',',\n",
       " 'the',\n",
       " 'Meanwhile',\n",
       " 'past',\n",
       " 'week',\n",
       " 'Syrian',\n",
       " ',',\n",
       " 'Iran',\n",
       " 'government',\n",
       " 'troops',\n",
       " 'have',\n",
       " ',',\n",
       " 'and',\n",
       " 'backed']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "input_tokens = [token_ids[0][i] for i in idx ]\n",
    "[tokenizer.decode(t) for t in input_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': [{'en': '1929 or 1989?', 'zh': '1929年还是1989年?'},\n",
       "  {'en': 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       "   'zh': '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。'},\n",
       "  {'en': 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       "   'zh': '一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。'},\n",
       "  {'en': 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       "   'zh': '如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。'},\n",
       "  {'en': 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       "   'zh': '目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。'},\n",
       "  {'en': 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       "   'zh': '欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时机来实行急需的结构性改革。'},\n",
       "  {'en': 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       "   'zh': '然而，作为地域战略学家，无论是从政治意义还是从经济意义上，让我自然想到的年份是1989年。'},\n",
       "  {'en': 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       "   'zh': '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。'},\n",
       "  {'en': 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       "   'zh': '事实上，从表面上看，两者似乎是完全是相反的：一个是象征着压抑和人为分裂的柏林墙的倒塌，而另一个是看似坚不可摧的并令人安心的金融资本主义机构的倒塌。'},\n",
       "  {'en': 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.',\n",
       "   'zh': '然而，和1989年一样，2008-2009年很可能也能被视为一个划时代的改变，其带来的发人深省的后果将在几十年后仍能让我们感受得到。'}]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1224)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[0][1]\n",
    "torch.norm(grad, dim=2).squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/as/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "def is_stopword(token_id, tokenizer):\n",
    "    word = tokenizer.decode(token_id)\n",
    "    if word in string.punctuation:\n",
    "        return True\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    stopword = stopwords.words('english') + [\"</s>\", \"<unk>\", \">>cmn_Hans<<\", \"<pad>\"]\n",
    "    if word in stopword or not word:\n",
    "        return True\n",
    "    return False \n",
    "def gradient_search(en_sentence, zh_sentence, k=5):\n",
    "    token_ids = tokenizer(en_sentence, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(zh_sentence, return_tensors=\"pt\").input_ids\n",
    "    # get your token embeddings\n",
    "    token_embeds=model.get_input_embeddings().weight[token_ids].clone()\n",
    "    token_embeds.retain_grad() \n",
    "    # get model output that contains loss value\n",
    "    outs = model(inputs_embeds=token_embeds,labels=labels, output_hidden_states=True, output_attentions=True)\n",
    "    loss=outs.loss\n",
    "    loss.backward(retain_graph=True)\n",
    "    grad=token_embeds.grad\n",
    "    grad_norm = torch.norm(grad, dim=2).squeeze(dim=0)\n",
    "    \n",
    "    for i in range(len(token_ids[0])):\n",
    "        current_token = token_ids[0][i]\n",
    "        if is_stopword(current_token, tokenizer):\n",
    "            grad_norm[i] = 0\n",
    "    idx = np.argsort(grad_norm).tolist()[::-1][:k]\n",
    "    \n",
    "    input_tokens = [token_ids[0][i] for i in idx ]\n",
    "    return idx, [tokenizer.decode(t) for t in input_tokens if tokenizer.decode(t) not in string.punctuation]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_data(dataset):\n",
    "    for pair in dataset['translation']:\n",
    "        pair[\"token_index\"], pair[\"top_tokens\"] = gradient_search(pair[\"en\"], pair[\"zh\"])\n",
    "    return dataset['translation']\n",
    "result = loop_data(raw_datasets[\"test\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': '28-Year-Old Chef Found Dead at San Francisco Mall',\n",
       "  'zh': '28岁厨师被发现死于旧金山一家商场',\n",
       "  'token_index': [8, 10, 7, 6, 11],\n",
       "  'top_tokens': ['Dead', 'San', 'Found', 'Chef', 'Francisco']},\n",
       " {'en': 'A 28-year-old chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "  'zh': '近日刚搬至旧金山的一位28岁厨师本周被发现死于当地一家商场的楼梯间。',\n",
       "  'token_index': [1, 3, 6, 5, 21],\n",
       "  'top_tokens': ['28', 'year', 'chef', 'old', 'air']},\n",
       " {'en': 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "  'zh': '但受害人的哥哥表示想不出有谁会想要加害于他，并称“一切终于好起来了。”',\n",
       "  'token_index': [2, 5, 6, 11, 21],\n",
       "  'top_tokens': ['victim', 'brother', 'says', 'think', 'saying']},\n",
       " {'en': \"The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "  'zh': '旧金山验尸官办公室表示，周三早上于西田购物中心发现的尸体确认为28岁旧金山居民 Frank Galicia。',\n",
       "  'token_index': [1, 2, 14, 5, 6],\n",
       "  'top_tokens': ['body', 'found', '28', 'West', 'field']},\n",
       " {'en': 'The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.',\n",
       "  'zh': '旧金山警察局称该起死亡案件被裁定为他杀，并正在进行调查。',\n",
       "  'token_index': [9, 11, 2, 7, 1],\n",
       "  'top_tokens': ['ruled', 'homicide', 'Francisco', 'death', 'San']},\n",
       " {'en': \"The victim's brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "  'zh': '受害人的哥哥 Louis Galicia 对美国广播公司位于旧金山的电台 KGO 表示，之前在波士顿做流水线厨师的Frank 于六个月前在旧金山的 Sons & Daughters 餐馆找到一份流水线厨师的理想工作。',\n",
       "  'token_index': [1, 4, 7, 8, 6],\n",
       "  'top_tokens': ['victim', 'brother', 'Gali', 'cia', 'Louis']},\n",
       " {'en': 'A spokesperson for Sons & Daughters said they were \"shocked and devastated\" by his death.',\n",
       "  'zh': 'Sons & Daughters 餐馆的一位发言人表示，他们对于 Frank 的死感到“非常震惊”。',\n",
       "  'token_index': [13, 14, 1, 3, 16],\n",
       "  'top_tokens': ['shock', 'ed', 'spokesperson', 'Son', 'devastated']},\n",
       " {'en': '\"We are a small team that operates like a close knit family and he will be dearly missed,\" the spokesperson said.',\n",
       "  'zh': '该发言人称，“我们是相处得像亲密无间的一家人的一个小团队，我们将深深怀念他。”',\n",
       "  'token_index': [18, 4, 5, 10, 11],\n",
       "  'top_tokens': ['dear', 'small', 'team', 'close', 'k']},\n",
       " {'en': \"Our thoughts and condolences are with Frank's family and friends at this difficult time.\",\n",
       "  'zh': '在这个悲痛的时刻，我们向 Frank 的家人及朋友表达我们深切的同情与哀悼。',\n",
       "  'token_index': [1, 11, 9, 3, 6],\n",
       "  'top_tokens': ['thoughts', 'friends', 'family', 'condolences', 'Frank']},\n",
       " {'en': 'Louis Galicia said Frank initially stayed in hostels, but recently, \"Things were finally going well for him.\"',\n",
       "  'zh': 'Louis Galicia 称 Frank 起初住在招待所里，但是最近“一切终于好起来了。”',\n",
       "  'token_index': [15, 2, 16, 1, 0],\n",
       "  'top_tokens': ['Th', 'cia', 'ing', 'Gali', 'Louis']},\n",
       " {'en': '\"He found an apartment, he was dating a girl,\" Louis Galicia told KGO.',\n",
       "  'zh': 'Louis Galicia 告诉 KGO：“Frank找到一间公寓，同时在跟一个女孩交往。”',\n",
       "  'token_index': [10, 12, 13, 2, 8],\n",
       "  'top_tokens': ['girl', 'Louis', 'Gali', 'found', 'dating']},\n",
       " {'en': 'Louis Galicia said he could not think of anyone who would want to hurt his younger brother.',\n",
       "  'zh': 'Louis Galicia 表示，他想不出有谁会想要加害他的弟弟。',\n",
       "  'token_index': [11, 0, 9, 1, 12],\n",
       "  'top_tokens': ['would', 'Louis', 'anyone', 'Gali', 'want']},\n",
       " {'en': 'He was a kind spirit with a big heart.',\n",
       "  'zh': 'Frank 拥有善良的心地及宽阔的胸怀。',\n",
       "  'token_index': [7, 8, 4, 3, 10],\n",
       "  'top_tokens': ['big', 'heart', 'spirit', 'kind', '</s>']},\n",
       " {'en': 'His way of connecting with the family was always making us a dish, making us dinner,\" Louis Galicia said.',\n",
       "  'zh': 'Louis Galicia 称：“Frank 经常为家人烹饪晚餐。”',\n",
       "  'token_index': [14, 1, 15, 12, 16],\n",
       "  'top_tokens': ['making', 'way', 'us', 'dish', 'dinner']},\n",
       " {'en': 'He never wanted to be in any kind of altercation.',\n",
       "  'zh': '他从不愿意与家人争吵。',\n",
       "  'token_index': [7, 9, 10, 2, 1],\n",
       "  'top_tokens': ['kind', 'alter', 'c', 'wanted', 'never']},\n",
       " {'en': 'He was the brother that went with the flow.',\n",
       "  'zh': '他是一个随从大家意见的人。',\n",
       "  'token_index': [8, 5, 3, 10, 9],\n",
       "  'top_tokens': ['flow', 'went', 'brother', '</s>']},\n",
       " {'en': '\"With everything else that\\'s going wrong with the world, he was that diamond in the rough that was shining bright every day,\" he said.',\n",
       "  'zh': 'Louis Galicia 说：“世上的所有事物都有可能出错，但Frank 称得上是一个每天都发光的外粗内秀之人。”',\n",
       "  'token_index': [2, 3, 16, 8, 7],\n",
       "  'top_tokens': ['everything', 'else', 'diamond', 'wrong', 'going']},\n",
       " {'en': 'Anyone with information is asked to call the SFPD Tip Line at 415-575-4444.',\n",
       "  'zh': '有线索人士请拨打旧金山警察局举报电话415-575-4444。',\n",
       "  'token_index': [14, 2, 0, 12, 15],\n",
       "  'top_tokens': ['415', 'information', 'Anyone', 'Line', '-5']},\n",
       " {'en': 'Junior doctors strike: Calls for fresh industrial action',\n",
       "  'zh': '初级医生罢工：呼吁新的劳工行动',\n",
       "  'token_index': [7, 6, 8, 4, 0],\n",
       "  'top_tokens': ['industrial', 'fresh', 'action', 'Calls', 'Junior']},\n",
       " {'en': 'Representatives of junior doctors have called on their union to authorise fresh industrial action in their dispute about a new contract.',\n",
       "  'zh': '初级医生代表号召联盟批准其针对新合同纠纷采取新的劳工行动。',\n",
       "  'token_index': [14, 13, 0, 2, 12],\n",
       "  'top_tokens': ['action',\n",
       "   'industrial',\n",
       "   'Representatives',\n",
       "   'junior',\n",
       "   'fresh']},\n",
       " {'en': 'The Junior Doctors Committee (JDC) of the British Medical Association (BMA) is to ask its full council to back more industrial action from early September.',\n",
       "  'zh': '英国医学协会初级医生委员会将要求其全体成员自九月初开始支持更多的劳工行动。',\n",
       "  'token_index': [1, 2, 20, 4, 22],\n",
       "  'top_tokens': ['Junior', 'Doctor', 'ask', 'Committee', 'full']},\n",
       " {'en': 'The JDC says ministers have failed to address concerns about the contract.',\n",
       "  'zh': '初级医生委员会表示，部长们未解决对于合同的忧虑。',\n",
       "  'token_index': [9, 8, 1, 2, 6],\n",
       "  'top_tokens': ['concerns', 'address', 'J', 'DC', 'failed']},\n",
       " {'en': 'Junior doctors and medical students voted in July to reject a contract deal agreed with the BMA.',\n",
       "  'zh': '初级医生与医科学生于7月份投票抵制与英国医学协会达成的合同交易。',\n",
       "  'token_index': [11, 12, 0, 1, 9],\n",
       "  'top_tokens': ['contract', 'deal', 'Junior', 'doctors', 'reject']},\n",
       " {'en': 'It was rejected by 58% of its members who voted in the ballot.',\n",
       "  'zh': '参与投票的成员中，58%反对该合同交易。',\n",
       "  'token_index': [10, 2, 8, 4, 13],\n",
       "  'top_tokens': ['voted', 'rejected', 'members', '58', 'ballot']},\n",
       " {'en': 'In a letter to members released on Twitter on Thursday night, the JDC\\'s chair Ellen McCourt said the government had remained \"persistently silent\" on issues which, she said, had resulted in the contract being rejected.',\n",
       "  'zh': '初级医生委员会主席 Ellen McCourt 在周四晚间于推特上发表的一封致成员信中表示，对于导致合同遭到抵制的问题，政府“一贯保持沉默”。',\n",
       "  'token_index': [2, 4, 7, 5, 9],\n",
       "  'top_tokens': ['letter', 'members', 'Twitter', 'released', 'Thursday']},\n",
       " {'en': 'She said: \"In light of this, the JDC Executive has voted to reject the proposed new contract in full and to call for formal re-negotiations on all of your concerns.\"',\n",
       "  'zh': '她表示：“鉴于此，初级医生委员会执行委员会已投票全盘驳回提议的新合同并呼吁就所有担忧重新进行正式协商。”',\n",
       "  'token_index': [1, 25, 14, 27, 5],\n",
       "  'top_tokens': ['said', 'call', 'voted', 'formal', 'light']},\n",
       " {'en': \"In response to the government's silence, JDC exec has today made a formal request for a special meeting of BMA Council to authorise a rolling programme of escalated industrial action beginning in early September.\",\n",
       "  'zh': '针对政府的沉默态度，初级医生委员会执行委员会已于今日正式要求英国医学协会理事会召开特别会议批准旨在从九月初开始升级劳工行动的一项长期计划。',\n",
       "  'token_index': [1, 7, 9, 4, 10],\n",
       "  'top_tokens': ['response', 'silence', 'J', 'government', 'DC']},\n",
       " {'en': 'The dispute has led to junior doctors taking part in six strikes this year, including the first all-out stoppages in the history of the NHS.',\n",
       "  'zh': '该纠纷已导致初级医生今年共参与六次罢工，包括英国国家医疗服务体系历史上的首次全面罢工。',\n",
       "  'token_index': [1, 3, 5, 6, 22],\n",
       "  'top_tokens': ['dispute', 'led', 'junior', 'doctors', 'page']},\n",
       " {'en': \"The BMA's junior doctor leader, Dr Johann Malawana, resigned following the vote to reject the negotiated terms of the contract, which the BMA had recommended.\",\n",
       "  'zh': '英国医学协会初级医生领导者 Johann Malawana 博士在英国医学协会所推荐的合同议定条款遭到投票抵制之后辞职。',\n",
       "  'token_index': [1, 23, 5, 12, 6],\n",
       "  'top_tokens': ['B', 'negotiated', 'junior', 'Mala', 'doctor']},\n",
       " {'en': 'He had told BMA members the deal was a good one that should be accepted, during meetings ahead of the poll of 54,000 junior doctors and medical students.',\n",
       "  'zh': '在54,000名初级医生与医科学生投票之前的会议中，他曾对英国医学协会成员表示，该交易不错，因此应该予以接受。',\n",
       "  'token_index': [2, 22, 3, 19, 5],\n",
       "  'top_tokens': ['told', 'poll', 'B', 'ahead', 'members']},\n",
       " {'en': 'Following the vote, Health Secretary Jeremy Hunt said the contract would be imposed on medics in England.',\n",
       "  'zh': '投票后，卫生部长 Jeremy Hunt 表示，该合同将在英格兰医护人员中强制实施。',\n",
       "  'token_index': [0, 2, 13, 4, 6],\n",
       "  'top_tokens': ['Following', 'vote', 'imposed', 'Health', 'Jeremy']},\n",
       " {'en': \"Russia and Turkey: An 'alliance of misfits'?\",\n",
       "  'zh': '俄罗斯与土耳其：“格格不入联盟”？',\n",
       "  'token_index': [8, 10, 0, 2, 11],\n",
       "  'top_tokens': ['ance', 'mis', 'Russia', 'Turkey', 'fits']},\n",
       " {'en': 'It was a gesture that ended a crisis.',\n",
       "  'zh': '一个手势结束一场危机。',\n",
       "  'token_index': [7, 5, 3, 9, 8],\n",
       "  'top_tokens': ['crisis', 'ended', 'gesture', '</s>']},\n",
       " {'en': 'The leaders of Russia and Turkey met on Tuesday to shake hands and declare a formal end to an eight-month long war of words and economic sanctions.',\n",
       "  'zh': '俄罗斯与土耳其领导人周二进行会见，双方握手并宣布正式结束长达八个月的口水战与经济制裁。',\n",
       "  'token_index': [1, 3, 5, 6, 11],\n",
       "  'top_tokens': ['leaders', 'Russia', 'Turkey', 'met', 'hands']},\n",
       " {'en': 'But, as Vladimir Putin greeted his Turkish counterpart in the gilded hall of a St. Petersburg palace, I got the distinct impression that Ankara wants this reconciliation the most.',\n",
       "  'zh': '然而，当弗拉基米尔·普京在一座圣彼德斯堡宫殿金碧辉煌的大厅中迎接土耳其总统时，我有种强烈的感觉：此次和解是安卡拉最希望看到的。',\n",
       "  'token_index': [3, 4, 16, 25, 15],\n",
       "  'top_tokens': ['Vladimir', 'Putin', 'hall', 'got', 'ed']},\n",
       " {'en': 'There was the handshake, yes.',\n",
       "  'zh': '是的，双方确实握手了。',\n",
       "  'token_index': [7, 4, 5, 3, 9],\n",
       "  'top_tokens': ['yes', 'hak', 'e', 'hands', '</s>']},\n",
       " {'en': \"But Mr Putin's smile looked thin and he was hardly oozing warmth even by his own restrained standards.\",\n",
       "  'zh': '然而，即使是按照普京先生一向内敛的标准来看，普京也只是浅浅微笑，几乎未流露出热情之意。',\n",
       "  'token_index': [1, 2, 15, 16, 17],\n",
       "  'top_tokens': ['Mr', 'Putin', 'warm', 'th', 'even']},\n",
       " {'en': 'Recep Tayyip Erdogan by contrast talked repeatedly of his \"dear friend\" Mr Putin - five times, according to one report.',\n",
       "  'zh': '据报道，雷杰普·塔伊普·埃尔多安则五次提到“好朋友”普京。',\n",
       "  'token_index': [2, 1, 0, 4, 19],\n",
       "  'top_tokens': ['Tay', 'ep', 'Rec', 'ip', 'Mr']},\n",
       " {'en': 'I lost count.',\n",
       "  'zh': '我记不清了。',\n",
       "  'token_index': [2, 1, 4, 3, 0],\n",
       "  'top_tokens': ['count', 'lost', '</s>', 'I']},\n",
       " {'en': 'He also pledged that relations with Russia would return not just to their pre-crisis level, but even higher.',\n",
       "  'zh': '同时，他承诺与俄罗斯的关系将回归至危机发生前的水平，甚至达到更高水平。',\n",
       "  'token_index': [15, 2, 13, 16, 1],\n",
       "  'top_tokens': ['crisis', 'pledged', 'pre', 'level', 'also']},\n",
       " {'en': 'The next day one newspaper here described Mr Erdogan as acting as if nothing bad had ever happened.',\n",
       "  'zh': '次日，此间一家报纸写到，埃尔多安先生则表现的好像什么都没发生一样。',\n",
       "  'token_index': [2, 1, 12, 3, 4],\n",
       "  'top_tokens': ['day', 'next', 'acting', 'one', 'newspaper']},\n",
       " {'en': 'To me, his enthusiasm implied the opposite.',\n",
       "  'zh': '但是，我认为他的热情暗示了某些相反的东西。',\n",
       "  'token_index': [7, 4, 5, 9, 8],\n",
       "  'top_tokens': ['opposite', 'enthusiasm', 'implied', '</s>']},\n",
       " {'en': \"But a lingering coolness emanating from Mr Putin showed that Russia's leader has forgotten nothing.\",\n",
       "  'zh': '然而，普京先生持续冷淡的态度说明他什么都没有忘记。',\n",
       "  'token_index': [12, 2, 10, 15, 3],\n",
       "  'top_tokens': ['Russia', 'linger', 'showed', 'leader', 'ing']},\n",
       " {'en': \"In fact, the cause of the crisis was the first thing he mentioned in his opening comments: Turkey's shooting down of a Russian fighter plane on the Syrian border.\",\n",
       "  'zh': '实际上，他在开场白中提到的第一件事便是此次危机的原因：土耳其于叙利亚边境击落一架俄罗斯战机。',\n",
       "  'token_index': [1, 4, 11, 10, 7],\n",
       "  'top_tokens': ['fact', 'cause', 'thing', 'first', 'crisis']},\n",
       " {'en': \"Moscow's reaction at the time was furious.\",\n",
       "  'zh': '莫斯科当时的反应很激烈。',\n",
       "  'token_index': [8, 6, 0, 3, 10],\n",
       "  'top_tokens': ['furious', 'time', 'Moscow', 'reaction', '</s>']},\n",
       " {'en': 'Mr Putin lashed out, accusing Ankara of stabbing Moscow in the back.',\n",
       "  'zh': '普京先生针对该事件进行猛烈抨击，指责安卡拉在背后暗算莫斯科。',\n",
       "  'token_index': [10, 11, 12, 13, 8],\n",
       "  'top_tokens': ['stab', 'b', 'ing', 'Moscow', 'Ankara']},\n",
       " {'en': 'The offence was even greater, coming from a supposed friend.',\n",
       "  'zh': '来自所谓朋友的攻击更让人难以接受。',\n",
       "  'token_index': [6, 1, 4, 9, 3],\n",
       "  'top_tokens': ['coming', 'offence', 'greater', 'supposed', 'even']},\n",
       " {'en': 'Rebuilding real trust will be hard, perhaps impossible.',\n",
       "  'zh': '双方很难，甚至不可能重新建立真正的信任。',\n",
       "  'token_index': [8, 6, 1, 3, 2],\n",
       "  'top_tokens': ['perhaps', 'hard', 'building', 'trust', 'real']},\n",
       " {'en': 'Russian public opinion has also turned since November.',\n",
       "  'zh': '自11月份开始，俄罗斯民意也有所扭转。',\n",
       "  'token_index': [7, 6, 1, 2, 5],\n",
       "  'top_tokens': ['November', 'since', 'public', 'opinion', 'turned']},\n",
       " {'en': 'For months, state-controlled media conducted a staggering, all-out offensive against Ankara.',\n",
       "  'zh': '数月来，被政府控制的媒体对安卡拉进行了令人震惊的全力进攻。',\n",
       "  'token_index': [9, 1, 3, 7, 14],\n",
       "  'top_tokens': ['staggering', 'months', 'state', 'conducted', 'offensive']},\n",
       " {'en': 'All of a sudden, it seemed like Turks were to blame for everything.',\n",
       "  'zh': '突然之间，土耳其人似乎要为所有事情负责。',\n",
       "  'token_index': [8, 11, 3, 7, 6],\n",
       "  'top_tokens': ['Turks', 'blame', 'sudden', 'like', 'seemed']},\n",
       " {'en': \"Most serious were accusations from top officials that Mr Erdogan's own family has profited from an illegal trade in oil from areas of Syria controlled by the so-called Islamic State.\",\n",
       "  'zh': '其中，最严重的是来自高级官员的指责：埃尔多安先生的家人从所谓伊斯兰国控制的叙利亚地区的非法石油交易中获利。',\n",
       "  'token_index': [1, 3, 5, 6, 9],\n",
       "  'top_tokens': ['serious', 'accusations', 'top', 'officials', 'Er']},\n",
       " {'en': \"He's denied that emphatically.\",\n",
       "  'zh': '他已断然否认该种说法。',\n",
       "  'token_index': [6, 5, 3, 8, 7],\n",
       "  'top_tokens': ['ally', 'emphatic', 'denied', '</s>']},\n",
       " {'en': \"But in St. Petersburg came the official message that it's time to move on.\",\n",
       "  'zh': '然而，圣彼德斯堡传来的消息显示，是时候该向前看了。',\n",
       "  'token_index': [2, 4, 13, 8, 7],\n",
       "  'top_tokens': ['St', 'Petersburg', 'time', 'message', 'official']},\n",
       " {'en': 'After all, this meeting only happened because Mr Putin got the apology he demanded from President Erdogan.',\n",
       "  'zh': '毕竟此次会面的前提是普京收到土耳其总统埃尔多安的道歉。',\n",
       "  'token_index': [12, 14, 4, 9, 16],\n",
       "  'top_tokens': ['apology', 'demanded', 'meeting', 'Putin', 'President']},\n",
       " {'en': 'Russia could claim a victory of sorts.',\n",
       "  'zh': '俄罗斯可以宣布胜利了。',\n",
       "  'token_index': [6, 1, 4, 0, 2],\n",
       "  'top_tokens': ['sorts', 'could', 'victory', 'Russia', 'claim']},\n",
       " {'en': 'For Ankara the benefits of calling a truce are clear.',\n",
       "  'zh': '对于安卡拉来说，休战的好处显而易见。',\n",
       "  'token_index': [7, 9, 1, 5, 3],\n",
       "  'top_tokens': ['truce', 'clear', 'Ankara', 'calling', 'benefits']},\n",
       " {'en': 'First and foremost, Erdogan needs all the friends he can get after he was nearly ousted from power last month in a failed coup.',\n",
       "  'zh': '首先，埃尔多安在上个月政变失败后险些被赶下台，因此他需要所有朋友的支持。',\n",
       "  'token_index': [2, 0, 4, 18, 17],\n",
       "  'top_tokens': ['foremost', 'First', 'Er', 'oust', 'nearly']},\n",
       " {'en': 'Repeat terror attacks on Turkey have clearly shaken him too.',\n",
       "  'zh': '不断的恐怖袭击显然已对他造成很大打击。',\n",
       "  'token_index': [7, 6, 4, 0, 1],\n",
       "  'top_tokens': ['shaken', 'clearly', 'Turkey', 'Repeat', 'terror']},\n",
       " {'en': 'There is also an economic motive.',\n",
       "  'zh': '同时也有经济动机。',\n",
       "  'token_index': [5, 4, 2, 7, 6],\n",
       "  'top_tokens': ['motive', 'economic', 'also', '</s>']},\n",
       " {'en': 'Russian sanctions have hit hard - particularly the ban on charter flights, which usually carry several million Russian tourists to the Turkish coast each year.',\n",
       "  'zh': '俄罗斯制裁对土耳其打击很大，尤其是禁止每年可输送几百万俄罗斯游客至土耳其海岸的包机服务。',\n",
       "  'token_index': [0, 1, 17, 16, 18],\n",
       "  'top_tokens': ['Russian', 'sanctions', 'million', 'several', 'Russian']},\n",
       " {'en': 'The number has slumped by almost 90%.',\n",
       "  'zh': '该数字已大幅下滑近90%。',\n",
       "  'token_index': [7, 6, 1, 4, 3],\n",
       "  'top_tokens': ['90', 'almost', 'number', 'ped', 'slum']},\n",
       " {'en': 'As for Russia, tour operators and charter companies here will certainly be relieved when flights eventually resume.',\n",
       "  'zh': '随着航班恢复，俄罗斯的旅行社与包机公司必将松一口气。',\n",
       "  'token_index': [11, 13, 2, 15, 4],\n",
       "  'top_tokens': ['certainly', 'relieved', 'Russia', 'flights', 'tour']},\n",
       " {'en': \"They're banking on a late-season rush to the Mediterranean.\",\n",
       "  'zh': '他们将迎来季末地中海旅游潮。',\n",
       "  'token_index': [9, 8, 10, 6, 13],\n",
       "  'top_tokens': ['son', 'sea', 'rush', 'late', 'Mediterranean']},\n",
       " {'en': 'And even this week, state TV has been predicting cheaper fruit and vegetables once Turkish agricultural imports are permitted again.',\n",
       "  'zh': '国家电视台甚至在本周预测，一旦土耳其农产品进口重新放开，水果及蔬菜的价格将下降。',\n",
       "  'token_index': [1, 14, 16, 3, 17],\n",
       "  'top_tokens': ['even', 'vegetables', 'Turkish', 'week', 'agricultural']},\n",
       " {'en': '\"Tourists that way, tomatoes back here,\" as a report in Vedemosti newspaper phrased it.',\n",
       "  'zh': '莫斯科报纸 Vedemosti 的一篇报道称：“游客去了，番茄回来了。”',\n",
       "  'token_index': [16, 3, 13, 19, 5],\n",
       "  'top_tokens': ['de', 'ists', 'report', 'newspaper', 'way']},\n",
       " {'en': 'But the visit also had additional political value for Moscow.',\n",
       "  'zh': '对于莫斯科来说，此次访问也具有额外的政治价值。',\n",
       "  'token_index': [9, 7, 6, 2, 3],\n",
       "  'top_tokens': ['Moscow', 'value', 'political', 'visit', 'also']},\n",
       " {'en': 'Ankara is angry with the West for what it considers a weak response to the attempted takeover.',\n",
       "  'zh': '安卡拉对于西方世界对接管意图的微弱反应感到愤怒。',\n",
       "  'token_index': [11, 12, 9, 0, 2],\n",
       "  'top_tokens': ['weak', 'response', 'considers', 'Ankara', 'angry']},\n",
       " {'en': \"Add to that its long-standing grudge at the snail's pace of talks to join the EU and step in Mr Putin - who is keen to capitalise on the chill and chip away at Turkey's ties with the West.\",\n",
       "  'zh': '此外，安卡拉对于加入欧盟谈判的缓慢进展及普京的插手长期感到不满，普京热衷于利用政治寒意以及削弱土耳其与西方世界的关系。',\n",
       "  'token_index': [0, 10, 4, 7, 6],\n",
       "  'top_tokens': ['Add', 'snail', 'long', 'grudge', 'standing']},\n",
       " {'en': 'The Russian leader certainly won bonus points with Ankara for calling in support of the elected authorities after the attempted coup.',\n",
       "  'zh': '由于在政变失败后拥护当选当局，俄罗斯领导人必将获得安卡拉的加分。',\n",
       "  'token_index': [15, 1, 2, 16, 12],\n",
       "  'top_tokens': ['elected', 'Russian', 'leader', 'authorities', 'support']},\n",
       " {'en': \"Mind you, that's a given for Moscow which has its own deep-seated fear of regime change.\",\n",
       "  'zh': '注意，这对于一直对政权更迭怀抱根深蒂固恐惧的莫斯科来说是一种馈赠。',\n",
       "  'token_index': [0, 14, 16, 17, 18],\n",
       "  'top_tokens': ['Mind', 'deep', 'se', 'ated', 'fear']},\n",
       " {'en': 'So the summit at this glitzy, seaside palace allowed Russia and Turkey to present what one analyst described to me as an \"alliance of misfits\": two countries that feel rejected and mistreated by the West, joining forces.',\n",
       "  'zh': '因此，在这个金碧辉煌的海边宫殿所举行的会面使俄罗斯与土耳其两个被西方世界拒绝与虐待的国家结成盟友，一位分析师将其描述为“格格不入联盟”。',\n",
       "  'token_index': [2, 17, 15, 7, 9],\n",
       "  'top_tokens': ['summit', 'present', 'Turkey', 'zy', 'sea']},\n",
       " {'en': 'Still, despite the public display of reconciliation, the two still have major differences.',\n",
       "  'zh': '然而，尽管公开和解，但双方仍存在重大分歧。',\n",
       "  'token_index': [0, 2, 10, 11, 7],\n",
       "  'top_tokens': ['Still', 'despite', 'two', 'still', 'reconciliation']},\n",
       " {'en': 'The key one is Syria, where Moscow has recently been casting itself as peacemaker but where Russia and Turkey back opposite sides.',\n",
       "  'zh': '叙利亚是关键因素之一。莫斯科近日在叙利亚扮演和事佬的角色，而俄罗斯与土耳其却支持相反派别。',\n",
       "  'token_index': [1, 2, 4, 15, 18],\n",
       "  'top_tokens': ['key', 'one', 'Syria', 'maker', 'Russia']},\n",
       " {'en': \"It could be telling that after almost three hours of initial talks, the two presidents told a press conference that they hadn't even touched on the topic.\",\n",
       "  'zh': '可以预见到的是，在经过近三个小时的初步谈话后，两位总统在发布会上表示，尚未谈及那个话题。',\n",
       "  'token_index': [1, 3, 6, 18, 19],\n",
       "  'top_tokens': ['could', 'telling', 'almost', 'press', 'conference']},\n",
       " {'en': \"Turkey's president deliberately avoided answering a question on their differences, while Mr Putin chose to underline them.\",\n",
       "  'zh': '土耳其总统刻意回避关于双方分歧的问题，而普京则予以强调。',\n",
       "  'token_index': [0, 14, 11, 3, 15],\n",
       "  'top_tokens': ['Turkey', 'Mr', 'differences', 'president', 'Putin']},\n",
       " {'en': 'There is no clear consensus on where they can seek common ground on Syria.',\n",
       "  'zh': '双方就如何在叙利亚问题上求同存异未达成明确共识。',\n",
       "  'token_index': [9, 10, 11, 4, 3],\n",
       "  'top_tokens': ['seek', 'common', 'ground', 'consensus', 'clear']},\n",
       " {'en': 'But after months of open hostility - and given the potential for utter disaster when Nato member Turkey shot down that Russian fighter jet - it is surely better that the two leaders are at least talking again.',\n",
       "  'zh': '在北大西洋公约组织成员国土耳其击落俄战机所带来的数月公开敌对及引发大型灾难的可能下，两国领导人再次重启对话肯定是件好事。',\n",
       "  'token_index': [2, 4, 5, 12, 10],\n",
       "  'top_tokens': ['months', 'open', 'hostility', 'utter', 'potential']},\n",
       " {'en': 'Royal Bank of Scotland to disappear for customers outside Scotland',\n",
       "  'zh': '苏格兰皇家银行将不再为苏格兰以外客户服务',\n",
       "  'token_index': [7, 8, 1, 3, 5],\n",
       "  'top_tokens': ['customers', 'outside', 'Bank', 'Scotland', 'disappear']},\n",
       " {'en': \"The brand RBS is to be reduced to a back office role, according to the bank's chief executive.\",\n",
       "  'zh': '据苏格兰皇家银行首席执行官透露，银行将简化为一个后勤部门。',\n",
       "  'token_index': [1, 14, 2, 12, 17],\n",
       "  'top_tokens': ['brand', 'according', 'RB', 'role', 'bank']},\n",
       " {'en': 'Royal Bank of Scotland will disappear for customers outside Scotland.',\n",
       "  'zh': '苏格兰皇家银行将不再为苏格兰以外的客户服务。',\n",
       "  'token_index': [8, 7, 9, 0, 5],\n",
       "  'top_tokens': ['outside', 'customers', 'Scotland', 'Royal', 'disappear']},\n",
       " {'en': \"Ross McEwan told BBC Scotland that the RBS brand was associated with the bank's global ambitions.\",\n",
       "  'zh': '罗斯·麦克尤恩告诉英国广播公司苏格兰分部，苏格兰皇家银行品牌曾经致力于其全球雄心壮志。',\n",
       "  'token_index': [1, 13, 0, 2, 16],\n",
       "  'top_tokens': ['Mc', 'associated', 'Ross', 'E', 'bank']},\n",
       " {'en': 'It has retreated from them since it nearly collapsed eight years ago and had to be bailed out.',\n",
       "  'zh': '但八年前濒临倒闭，不得不接受救助从那时开始便放弃了那样的追求。',\n",
       "  'token_index': [2, 3, 12, 11, 9],\n",
       "  'top_tokens': ['retreat', 'ed', 'ago', 'years', 'collapsed']},\n",
       " {'en': \"During that time, brand strategists have used 'RBS' to protect other consumer finance brands.\",\n",
       "  'zh': '那段时间，品牌策略师使用‘RBS’来保护其它消费金融品牌。',\n",
       "  'token_index': [2, 17, 4, 13, 19],\n",
       "  'top_tokens': ['time', 'protect', 'brand', 'RB', 'consumer']},\n",
       " {'en': 'It was backed with millions of pounds in sponsorship of international sport, from Six Nations rugby to Wimbledon champion Andy Murray.',\n",
       "  'zh': '集团曾耗费数百万英镑以赞助橄榄球六国赛和温布尔顿网球锦标赛冠军安迪·穆雷等方式赞助国际体育赛事。',\n",
       "  'token_index': [2, 4, 8, 16, 14],\n",
       "  'top_tokens': ['backed', 'millions', 'sponsorship', 'rug', 'Six']},\n",
       " {'en': 'But now, it has been judged right to let more national brands come to the fore.',\n",
       "  'zh': '但是现在，正确的做法是让更多的国家品牌崭露头角。',\n",
       "  'token_index': [12, 11, 14, 9, 7],\n",
       "  'top_tokens': ['brand', 'national', 'come', 'let', 'right']},\n",
       " {'en': 'Royal Bank of Scotland will be used with Scottish customers, but will not be initialised.',\n",
       "  'zh': '苏格兰皇家银行将为苏格兰客户所用，但不会回复原样。',\n",
       "  'token_index': [9, 0, 1, 8, 3],\n",
       "  'top_tokens': ['customers', 'Royal', 'Bank', 'Scottish', 'Scotland']},\n",
       " {'en': 'In England and Wales, all RBS references, outside head office and the stock exchange listing, will be changed to NatWest.',\n",
       "  'zh': '在英格兰和威尔士，所有在总部及证券交易所以外的提法将更改为国民西敏寺银行。',\n",
       "  'token_index': [1, 3, 17, 16, 15],\n",
       "  'top_tokens': ['England', 'Wales', 'listing', 'exchange', 'stock']},\n",
       " {'en': 'The Ulster Bank brand is already used for customers in the Republic of Ireland and Northern Ireland.',\n",
       "  'zh': '阿尔斯特银行品牌已用于爱尔兰共和国与北爱尔兰自治区的客户。',\n",
       "  'token_index': [12, 1, 9, 14, 2],\n",
       "  'top_tokens': ['Republic', 'Ul', 'customers', 'Ireland', 'ster']},\n",
       " {'en': \"There are other, smaller brands for private banking, which will get more prominence - Coutts, Adam & Co, Drummond, and Holt's Military Bank.\",\n",
       "  'zh': \"还有一些其它更突出的较小私人银行品牌，例如顾资银行、Adam & Co、 Drummond 与 Holt's Military Bank。\",\n",
       "  'token_index': [8, 9, 4, 5, 13],\n",
       "  'top_tokens': ['private', 'banking', 'smaller', 'brand', 'get']},\n",
       " {'en': 'Mr McEwan was interviewed during a tour of customers and staff in Inverness-shire.',\n",
       "  'zh': '麦克尤恩先生曾在因弗内斯郡客户与员工的参观之旅中接受采访。',\n",
       "  'token_index': [0, 1, 2, 12, 3],\n",
       "  'top_tokens': ['Mr', 'Mc', 'E', 'staff', 'wan']},\n",
       " {'en': 'He told BBC Scotland: \"The RBS brand will end up becoming our investor brand and the one that our staff are employed with, because we are now becoming much more a bank of brands.\"',\n",
       "  'zh': '他告诉英国广播公司苏格兰分部说：“苏格兰皇家银行品牌将最终成为我们的投资者品牌，同时也是员工受雇品牌之一，因为目前更大意义上我们已成为一个拥有众多品牌的银行。”',\n",
       "  'token_index': [2, 3, 7, 24, 1],\n",
       "  'top_tokens': ['BBC', 'Scotland', 'RB', 'employed', 'told']},\n",
       " {'en': 'As the bank itself became a global brand, RBS became the global brand.',\n",
       "  'zh': '由于该银行本身成为一个国际品牌，RBS也成为全球品牌。',\n",
       "  'token_index': [9, 11, 2, 7, 4],\n",
       "  'top_tokens': ['RB', 'became', 'bank', 'brand', 'became']},\n",
       " {'en': \"I'm now saying we no longer have global aspirations, we have local aspirations.\",\n",
       "  'zh': '目前，我要说的是，我们不再重点关注全球，我们已将目光转向本土。',\n",
       "  'token_index': [10, 9, 7, 4, 14],\n",
       "  'top_tokens': ['aspirations', 'global', 'longer', 'saying', 'local']},\n",
       " {'en': 'Each one of those brands will stand for something quite different in their own communities, and our staff will work with customers under those brands.',\n",
       "  'zh': '每一个品牌代表着各自所在行业的不同，而我们的员工通过不同品牌为客户服务。',\n",
       "  'token_index': [1, 4, 19, 15, 7],\n",
       "  'top_tokens': ['one', 'brand', 'staff', 'communities', 'stand']},\n",
       " {'en': 'RBS had already stated that it would not to continue its Six Nations sponsorship, and it has been raising the profile of different brands in its sports sponsorship.',\n",
       "  'zh': '苏格兰皇家银行已表明不会继续赞助六国锦标赛；同时，已努力提升不同品牌在体育赛事赞助的形象。',\n",
       "  'token_index': [0, 20, 22, 3, 4],\n",
       "  'top_tokens': ['RB', 'raising', 'profile', 'already', 'stated']},\n",
       " {'en': '\"The time is right for us to move to the bank of brands, because underneath (we\\'ve been asking) how do we focus on making this a better bank for customers?\" said the chief executive.',\n",
       "  'zh': '首席执行官表示：“现在到了成为一家多品牌银行的时候，因为私下我们一直在探求如何成为一家更好的服务于客户的银行。”',\n",
       "  'token_index': [2, 4, 8, 6, 11],\n",
       "  'top_tokens': ['time', 'right', 'move', 'us', 'bank']},\n",
       " {'en': \"It would have been very cynical three years ago if we'd said we're going to be a great bank for customers and put those brands out there.\",\n",
       "  'zh': '如果三年前我们立志成为一家拥有众多品牌的大银行将备受嘲讽。',\n",
       "  'token_index': [1, 5, 22, 21, 6],\n",
       "  'top_tokens': ['would', 'cynical', 'bank', 'great', 'three']},\n",
       " {'en': \"But with the work we've been doing, focussing on the customers needs, not our own, I think you're seeing a lot of change.\",\n",
       "  'zh': '目前我们重点关注客户需求，并且我相信您已注意到了这些变化。',\n",
       "  'token_index': [3, 10, 23, 12, 15],\n",
       "  'top_tokens': ['work', 'focus', 'think', 'ing', 'customers']},\n",
       " {'en': 'We can bring those brands back up again, so I think the time is right.',\n",
       "  'zh': '我认为我们可以重新启动这些品牌，而且现在时间正合适。',\n",
       "  'token_index': [12, 2, 14, 4, 6],\n",
       "  'top_tokens': ['think', 'bring', 'time', 'brand', 'back']}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = count_pos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "def find_tokenidx(token, sentence_lst):\n",
    "  for i in range(len(sentence_lst)):\n",
    "    if token.lower() in sentence_lst[i].lower():\n",
    "      return i\n",
    "def token_vec_sum(text, word):\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = bert_tokenizer.tokenize(marked_text)\n",
    "  index = find_tokenidx(word, tokenized_text)\n",
    "  print(index)\n",
    "  print(tokenized_text)\n",
    "  indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                    )\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      outputs = model(tokens_tensor, segments_tensors)\n",
    "      hidden_states = outputs[2]\n",
    "      token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "      token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "      token_embeddings = token_embeddings.permute(1,0,2)\n",
    "      token_vecs_sum = []\n",
    "      for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "  return token_vecs_sum[index]\n",
    "def similarity(origin, mutant, word, mutant_word):\n",
    "  \n",
    "  origin_sum, mutant_sum =  token_vec_sum(origin, word), token_vec_sum(mutant, mutant_word)\n",
    "  return 1 - cosine(origin_sum, mutant_sum)\n",
    "      \n",
    "  \n",
    "def mutant_pairs(examples):\n",
    "  pairs=[]\n",
    "  inputs = [ex[\"en\"] for ex in examples]\n",
    "  for i, input in enumerate(inputs):\n",
    "    input_list = input.split()\n",
    "    tokens = examples[i]['top_tokens']\n",
    "    #print(tokens)\n",
    "    token_idx = find_tokenidx(tokens[0], input_list)\n",
    "    #print(input_list[token_idx])\n",
    "    word = input_list[token_idx]\n",
    "\n",
    "    input_list[token_idx] = \"[MASK]\"\n",
    "    candidate = unmasker(\" \".join(input_list))\n",
    "\n",
    "    mutants = []\n",
    "    for c in candidate[:3]:\n",
    "      mutant = input_list.copy()\n",
    "      mutant[token_idx] = c[\"token_str\"]\n",
    "      \n",
    "      mutant_sent = \" \".join(mutant)\n",
    "      # if similarity(input, mutant_sent, word, c[\"token_str\"]) >=0.9:\n",
    "      mutants.append(\" \".join(mutant))\n",
    "    pairs.append({'en': input, 'mutants': mutants})\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': '28-Year-Old Chef Found Dead at San Francisco Mall',\n",
       "  'mutants': ['28-Year-Old Chef Found work at San Francisco Mall',\n",
       "   '28-Year-Old Chef Found himself at San Francisco Mall',\n",
       "   '28-Year-Old Chef Found food at San Francisco Mall']},\n",
       " {'en': 'A 28-year-old chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "  'mutants': ['A young chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "   'A french chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.',\n",
       "   'A local chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.']},\n",
       " {'en': 'But the victim\\'s brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "  'mutants': ['But the older brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "   'But the younger brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"',\n",
       "   'But the other brother says he can\\'t think of anyone who would want to hurt him, saying, \"Things were finally going well for him.\"']},\n",
       " {'en': \"The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "  'mutants': [\"The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "   \"The man found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\",\n",
       "   \"The person found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner's Office said.\"]},\n",
       " {'en': 'The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.',\n",
       "  'mutants': ['The San Francisco Police Department said the death was ruled a suicide and an investigation is ongoing.',\n",
       "   'The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.',\n",
       "   'The San Francisco Police Department said the death was ruled a murder and an investigation is ongoing.']},\n",
       " {'en': \"The victim's brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "  'mutants': [\"The youngest brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "   \"The younger brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\",\n",
       "   \"The older brother, Louis Galicia, told ABC station KGO in San Francisco that Frank, previously a line cook in Boston, had landed his dream job as line chef at San Francisco's Sons & Daughters restaurant six months ago.\"]},\n",
       " {'en': 'A spokesperson for Sons & Daughters said they were \"shocked and devastated\" by his death.',\n",
       "  'mutants': ['A spokesperson for Sons & Daughters said they were \" and devastated\" by his death.',\n",
       "   'A spokesperson for Sons & Daughters said they were shocked and devastated\" by his death.',\n",
       "   'A spokesperson for Sons & Daughters said they were surprised and devastated\" by his death.']},\n",
       " {'en': '\"We are a small team that operates like a close knit family and he will be dearly missed,\" the spokesperson said.',\n",
       "  'mutants': ['\"We are a small team that operates like a close knit family and he will be easily missed,\" the spokesperson said.',\n",
       "   '\"We are a small team that operates like a close knit family and he will be greatly missed,\" the spokesperson said.',\n",
       "   '\"We are a small team that operates like a close knit family and he will be much missed,\" the spokesperson said.']},\n",
       " {'en': \"Our thoughts and condolences are with Frank's family and friends at this difficult time.\",\n",
       "  'mutants': [\"Our apologies and condolences are with Frank's family and friends at this difficult time.\",\n",
       "   \"Our prayers and condolences are with Frank's family and friends at this difficult time.\",\n",
       "   \"Our thoughts and condolences are with Frank's family and friends at this difficult time.\"]},\n",
       " {'en': 'Louis Galicia said Frank initially stayed in hostels, but recently, \"Things were finally going well for him.\"',\n",
       "  'mutants': ['Louis Galicia said Frank initially stayed in hostels, but recently, things were finally going well for him.\"',\n",
       "   'Louis Galicia said Frank initially stayed in hostels, but recently, they were finally going well for him.\"',\n",
       "   'Louis Galicia said Frank initially stayed in hostels, but recently, \" were finally going well for him.\"']}]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutant_pairs(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "def sentence_sim(text1, text2):\n",
    "    origin = model.encode(text1)\n",
    "    mutant = model.encode(text2)\n",
    "    return cosine_similarity(\n",
    "    [origin],\n",
    "    [mutant])[0][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c818903a972b15f948093850adeb30c84eba2d54a31a843f1ceb959d2841b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
